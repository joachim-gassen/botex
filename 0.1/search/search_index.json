{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"botex: Using LLMs as Experimental Participants in oTree","text":""},{"location":"index.html#overview","title":"Overview","text":"<p>Welcome to botex, a new Python package that leverages the power of large language models (LLMs) as participants in oTree experiments.</p> <p>botex takes a novel approach to integrating LLMs into behavioral experiments. Rather than relying on predefined prompts,<sup>1</sup> botex bots dynamically interact with their experimental environment by scraping their respective oTree participant pages. This approach allows them to infer the experimental flow solely from the webpage's textual content. By aligning bot behavior directly with the experimental interface, botex eliminates potential discrepancies between human and bot designs. This not only opens up exciting opportunities to explore LLM behavior but also positions LLMs as a powerful tool for developing and pre-testing experiments intended for human participants.</p> <p> </p> <p>For interfacing with LLMs, it offers two options</p> <ul> <li>litellm: Allows the use of OpenAI's Chat-GPT AI and various other LLMs. </li> <li>llama.cpp: Allows the use of local (open source) LLMs  </li> </ul> <p>While both approaches have been tested and found to work, currently, we have only used OpenAI's Chat GPT-4 model for our own research work.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>If you want to use botex to create LLM participants for your own oTree experiments, you need the following:</p> <ul> <li>A working python environment &gt;= 3.10 and preferable a virtual environment.</li> <li>Google Chrome and ChromeDriver for scraping the oTree participant pages</li> <li>If you plan to use Chat-GPT 4 as your LLM (recommended for beginners), an OpenAI API key. If you want to go local, take a look at the next section.</li> <li>Access to an oTree server that you can start sessions on or at least an URL of an oTree participant link. The server can be local or remote.</li> </ul> <p>Then install the botex package: <code>pip install botex==0.1.0</code>. </p>"},{"location":"index.html#running-a-bot-on-a-single-participant-link","title":"Running a bot on a single participant link","text":"<p>After that, you should be able to start botex on an existing oTree participant link by running the following code snippet</p> <pre><code># Enabling logging is a good idea if you want to see what is going on\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\nimport botex\n\n# Running a botex bot on a specific oTree participant link\nbotex.run_single_bot(\n    botex_db = \"path to a sqlite3 file that will store the bot data (does not need to exist)\", \n    session_id = \"The session ID of your oTree experiment (will be stored with the botex data)\", \n    url = \"the URL of the participant link\", \n    openai_api_key = \"your OpenAI api key\"\n)\n</code></pre>"},{"location":"index.html#initalizing-an-otree-session-and-starting-bots","title":"Initalizing an oTree session and starting bots","text":"<p>Alternatively, you can use botex to initialize a session on your oTree server and to start all required bots for the session in one go. This session can also contain human participants. However, in that case, you would be responsible to get the humans going to complete the session ;-)</p> <pre><code>import logging\nlogging.basicConfig(level=logging.INFO)\n\nimport botex\n\n# Initialize an oTree session\nsdict = botex.init_otree_session(\n    config_name = \"config name of your oTree experiment\", \n    npart = 6 # number of participants in the session, including bots and humans\n    nhumans = 0, # set to non-zero if you want humans to play along\n    botex_db = \"path to a sqlite3 file (does not need to exist)\",\n    otree_server_url = \"url of your server, e.g., http://localhost:8000]\",\n    otree_rest_key = \"your oTree API secret key\"\n)\n\n# The returned dict will contain the oTree session ID, all participant codes, \n# human indicators, and the URLs separately for the LLM and human participants.\n# You can now start all LLM participants of the session in one go.  \nbotex.run_bots_on_session(\n    session_id = sdict['session_id'],  \n    botex_db = \"same path that you used for initializing the session\", \n    openai_api_key = \"your OpenAI api key\"\n)\n</code></pre> <p>After the bots have completed their runs, you should have their response data stored in your oTree database just as it is the case for human participants. If you are interested in exploring the botex data itself, which is stored in the sqlite3 file that you provided, we recommend that you take a look at our botex case study.</p>"},{"location":"index.html#use-of-local-llms","title":"Use of local LLMs","text":"<p>If you want to use a local LLM instead of commercial APIs via the litellm interface you need, in addition to the above:</p> <ul> <li>llama.cpp. Clone it from here and follow the instructions to build it.</li> <li>A local LLM model. You can use different models (e.g., from Hugging Face) but for starters download a GGUF-format model of Mistral-7B-Instruct-v0.3.Q4_K_M.gguf. At the moment the following Q4_K_M version is tested and working.</li> </ul> <p>Then all that you need to do is to adjust the botex calls from above by specifying the model and its configuration. You do this by providing a <code>LocalLLM</code> object to the botex calls that start bots. For example, for <code>botex.run_bots_on_session()</code>, your call would look something like this</p> <pre><code>botex.run_bots_on_session(\n    session_id = sdict['session_id'],  \n    botex_db = \"same path that you used for initializing the session\", \n    model = \"local\",\n    local_model_cfg={\n        \"path_to_llama_server\": \"the path to the llama.cpp server (called llama-server or server on older versions)\",\n        \"local_llm_path\": \"the path to your LLM model GGUF file\"\n    }\n)\n</code></pre> <p>Everything else from above remains the same. When starting local LLMs as bots take a good look at the log files to see how they do.</p>"},{"location":"index.html#more-information","title":"More information","text":"<p>If you want to learn more about botex you can read our paper.</p>"},{"location":"index.html#get-in-touch","title":"Get in touch!","text":"<p>If you are interested in this project or even have already tried it, we would love to hear from you. Simply shoot an email, comment on our linkedin post, or open an issue on GitHub!</p> <ol> <li> <p>See, for example, Grossmann, Engel and Ockenfels (paper, repo)\u00a0\u21a9</p> </li> </ol>"},{"location":"reference.html","title":"API Reference","text":"<p>This section details the botex API. It consists of a set of Python functions to set up and run oTree-based experiments using LLMs as bots.</p> <p>The Python API can be divided into two main parts:</p> <ol> <li>oTree Interface: Functions to interact with the oTree server, such as reading session config data from it, initializing sessions, and running bots on sessions.</li> <li>Export data: Functions to export data from the botex database.</li> </ol>"},{"location":"reference.html#otree-interface","title":"oTree Interface","text":"<p>Running experiments with botex requires an oTree server with an active session to be accessible. The following functions allow the user to interact with the oTree server. Once a session is initialized, the core functions <code>run_bots_on_session()</code> and <code>run_single_bot()</code> can be used to run bots on the session.</p>"},{"location":"reference.html#init_otree_session","title":"<code>init_otree_session</code>","text":"<p>Initialize an oTree session with a given number of participants.</p> <p>Parameters:</p> Name Type Description Default <code>config_name</code> <code>str</code> <p>The name of the oTree session configuration.</p> required <code>npart</code> <code>int</code> <p>The total number of participants.</p> required <code>nhumans</code> <code>int</code> <p>The number of human participants (defaults to zero).  Provide either nhumans or is_human, but not both.</p> <code>0</code> <code>is_human</code> <code>list</code> <p>A list of booleans indicating whether each participant  is human. Needs to be the same length as npart. If None  (the default), humans (if present) will be randomly assigned.</p> <code>None</code> <code>room_name</code> <code>str</code> <p>The name of the oTree room for the session.  If None (the default),no room will be used.</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file to store BotEx  data. If None (the default), it will be obtained from the  environment variable BOTEX_DB. If the database does not exist, it  will be created.</p> <code>None</code> <code>otree_server_url</code> <code>str</code> <p>The URL of the oTree server. If None  (the default), it will be obtained from the environment variable  OTREE_SERVER_URL.</p> <code>None</code> <code>otree_rest_key</code> <code>str</code> <p>The API key for the oTree server. If None (the default), it will be obtained from the environment  variable OTREE_REST_KEY.</p> <code>None</code> <code>modified_session_config_fields</code> <code>dict</code> <p>A dictionary of fields to modify  in the the oTree session config. Default is None. </p> <code>None</code> <p>Returns:</p> Type Description <p>dict with the keys 'session_id', 'participant_code', 'is_human',  'bot_urls', and 'human_urls' containing the session ID, participant  codes, human indicators, and the URLs for the human and bot  participants.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def init_otree_session(\n        config_name, npart, nhumans = 0, \n        is_human = None,\n        room_name = None,\n        botex_db = None,\n        otree_server_url = None,\n        otree_rest_key = None,\n        modified_session_config_fields = None,\n    ):\n    \"\"\"\n    Initialize an oTree session with a given number of participants.\n\n    Args:\n        config_name (str): The name of the oTree session configuration.\n        npart (int): The total number of participants.\n        nhumans (int): The number of human participants (defaults to zero). \n            Provide either nhumans or is_human, but not both.\n        is_human (list): A list of booleans indicating whether each participant \n            is human. Needs to be the same length as npart. If None \n            (the default), humans (if present) will be randomly assigned.\n        room_name (str): The name of the oTree room for the session. \n            If None (the default),no room will be used.\n        botex_db (str): The name of the SQLite database file to store BotEx \n            data. If None (the default), it will be obtained from the \n            environment variable BOTEX_DB. If the database does not exist, it \n            will be created.\n        otree_server_url (str): The URL of the oTree server. If None \n            (the default), it will be obtained from the environment variable \n            OTREE_SERVER_URL.\n        otree_rest_key (str): The API key for the oTree server.\n            If None (the default), it will be obtained from the environment \n            variable OTREE_REST_KEY.\n        modified_session_config_fields (dict): A dictionary of fields to modify \n            in the the oTree session config. Default is None. \n\n    Returns:\n        dict with the keys 'session_id', 'participant_code', 'is_human', \n            'bot_urls', and 'human_urls' containing the session ID, participant \n            codes, human indicators, and the URLs for the human and bot \n            participants.\n    \"\"\"\n\n    def call_api(method, *path_parts, **params) -&gt; dict:\n        path_parts = '/'.join(path_parts)\n        url = f'{otree_server_url}/api/{path_parts}'\n        resp = method(url, json=params, headers={'otree-rest-key': otree_rest_key})\n        if not resp.ok:\n            msg = (\n                f'Request to \"{url}\" failed '\n                f'with status code {resp.status_code}: {resp.text}'\n            )\n            raise Exception(msg)\n        return resp.json()\n\n    if nhumans &gt; 0 and is_human is not None: raise(Exception(\n        \"Provide either nhumans or is_human, but not both.\"\n    ))\n\n    if is_human is not None:\n        if len(is_human) != npart: raise(Exception(\n            \"Length of is_human must be the same as npart.\"\n        ))\n\n    if is_human is None and nhumans &gt; 0:\n        is_human = [True]*nhumans + [False]*(npart - nhumans)\n        shuffle(is_human)\n\n    if is_human is None and nhumans == 0: is_human = [False]*npart\n\n    if botex_db is None: botex_db = environ.get('BOTEX_DB')\n    if otree_server_url is None:\n        otree_server_url = environ.get('OTREE_SERVER_URL')\n    if otree_rest_key is None:\n        otree_rest_key = environ.get('OTREE_REST_KEY')\n\n    session_id = call_api(\n        requests.post, 'sessions', session_config_name=config_name, \n        num_participants=npart, room_name=room_name,\n        modified_session_config_fields=modified_session_config_fields\n    )['code']\n    part_data = sorted(\n        call_api(requests.get, 'sessions', session_id)['participants'],\n        key=lambda d: d['id_in_session']\n    )\n    part_codes = [pd['code'] for pd in part_data]\n\n    base_url = otree_server_url + '/InitializeParticipant/'\n    urls = [base_url + pc for pc in part_codes]\n\n    rows = zip(\n        [config_name]*npart, [session_id]*npart, \n        part_codes, is_human, urls\n    )\n\n    conn = setup_botex_db(botex_db)\n    cursor = conn.cursor()\n    cursor.executemany(\n        \"\"\"\n        INSERT INTO participants (\n            session_name, session_id, participant_id, is_human, url) \n            VALUES (?, ?, ?, ?, ?) \n        \"\"\", rows\n    )\n    conn.commit()\n    cursor.close()\n    return {\n        'session_id': session_id, \n        'participant_code': part_codes,\n        'is_human': is_human,\n        'bot_urls': list(compress(urls, [not x for x in is_human])), \n        'human_urls': list(compress(urls, is_human))\n    }\n</code></pre>"},{"location":"reference.html#get_bot_urls","title":"<code>get_bot_urls</code>","text":"<p>Get the URLs for the bot participants in an oTree session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The ID of the oTree session.</p> required <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file to store BotEx  data. If None (the default), it will be obtained from the  environment variable BOTEX_DB.</p> <code>None</code> <p>Returns:</p> Type Description <p>List of URLs for the bot participants.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def get_bot_urls(session_id, botex_db = None, already_started = False):\n    \"\"\"\n    Get the URLs for the bot participants in an oTree session.\n\n    Args:\n        session_id (str): The ID of the oTree session.\n        botex_db (str): The name of the SQLite database file to store BotEx \n            data. If None (the default), it will be obtained from the \n            environment variable BOTEX_DB.\n\n    Returns:\n        List of URLs for the bot participants.\n    \"\"\"\n\n    if botex_db is None: botex_db = environ.get('BOTEX_DB')\n    conn = sqlite3.connect(botex_db)\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT url,time_in,time_out FROM participants \n        WHERE session_id = ? AND is_human = 0\n        \"\"\", (session_id,)\n    )\n    if already_started:\n        urls = [row[0] for row in cursor.fetchall() if row[2] is None]\n    else:\n        urls = [row[0] for row in cursor.fetchall() if row[1] is None]\n    cursor.close()\n    conn.close()\n    return urls\n</code></pre>"},{"location":"reference.html#run_bots_on_session","title":"<code>run_bots_on_session</code>","text":"<p>Run BotEx bots on an oTree session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The ID of the oTree session.</p> required <code>bot_urls</code> <code>list</code> <p>A list of URLs for the bot participants. Will be retrieved from the database if None (the default).</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file for BotEx data. If None (the default), it will be obtained from the environment  variable BOTEX_DB.</p> <code>None</code> <code>full_conv_history</code> <code>bool</code> <p>Whether to keep the full conversation history. This will increase token use and only work with very short  experiments. Default is False.</p> <code>False</code> <code>model</code> <code>str</code> <p>The model to use for the bot. Default is \"gpt-4o\" from OpenAI. You will need an OpenAI key and be prepared to pay to  use this model. If set to \"local\", you need to provide a  configuration for the local model in local_model_cfg.</p> <code>'gpt-4o'</code> <code>openai_api_key</code> <code>str</code> <p>The API key for the OpenAI service. If None  (the default), it will be obtained from the environment variable  OPENAI_API_KEY.</p> <code>None</code> <code>already_started</code> <code>bool</code> <p>If True, the function will also run bots that  have already started but not yet finished. This is useful if bots  did not startup properly because of network issues. Default is  False.</p> <code>False</code> <code>wait</code> <code>bool</code> <p>If True (the default), the function will wait for the bots  to finish.</p> <code>True</code> <code>local_model_cfg</code> <code>dict</code> <p>Configuration for the local model. If model is  \"local\", as a bare minimum it should contain,  the \"path_to_llama_server\", and \"local_llm_path\" keys.  If these ar not present, the function will try to get them from the environment variables PATH_TO_LLAMA_SERVER and LOCAL_LLM_PATH.</p> <code>{}</code> <code>user_prompts</code> <code>dict</code> <p>A dictionary of user prompts to override the  default prompts that the bot uses. The keys should be one or more of  the following: ['start', 'analyze_first_page_no_q',  'analyze_first_page_q', 'analyze_page_no_q', 'analyze_page_q',  'analyze_page_no_q_full_hist', 'analyze_page_q_full_hist',  'page_not_changed', 'system', 'resp_too_long', 'json_error', 'end'.]  If a key is not present in the dictionary, the default prompt will  be used. If a key that is not in the default prompts is present in  the dictionary, then the bot will exit with a warning and not  running to make sure that the user is aware of the issue.</p> <code>None</code> <p>Returns:</p> Type Description <p>None (bot conversation logs are stored in database)</p> Source code in <code>src/botex/otree.py</code> <pre><code>def run_bots_on_session(\n        session_id, bot_urls = None, \n        botex_db = None, \n        model = \"gpt-4o\",\n        full_conv_history = False,\n        openai_api_key = None,\n        already_started = False,\n        wait = True,\n        local_model_cfg={},\n        user_prompts: dict | None = None\n    ):\n    \"\"\"\n    Run BotEx bots on an oTree session.\n\n    Args:\n        session_id (str): The ID of the oTree session.\n        bot_urls (list): A list of URLs for the bot participants.\n            Will be retrieved from the database if None (the default).\n        botex_db (str): The name of the SQLite database file for BotEx data.\n            If None (the default), it will be obtained from the environment \n            variable BOTEX_DB.\n        full_conv_history (bool): Whether to keep the full conversation history.\n            This will increase token use and only work with very short \n            experiments. Default is False.\n        model (str): The model to use for the bot. Default is \"gpt-4o\"\n            from OpenAI. You will need an OpenAI key and be prepared to pay to \n            use this model. If set to \"local\", you need to provide a \n            configuration for the local model in local_model_cfg.\n        openai_api_key (str): The API key for the OpenAI service. If None \n            (the default), it will be obtained from the environment variable \n            OPENAI_API_KEY.\n        already_started (bool): If True, the function will also run bots that \n            have already started but not yet finished. This is useful if bots \n            did not startup properly because of network issues. Default is \n            False.\n        wait (bool): If True (the default), the function will wait for the bots \n            to finish.\n        local_model_cfg (dict): Configuration for the local model. If model is \n            \"local\", as a bare minimum it should contain, \n            the \"path_to_llama_server\", and \"local_llm_path\" keys. \n            If these ar not present, the function will\n            try to get them from the environment variables PATH_TO_LLAMA_SERVER\n            and LOCAL_LLM_PATH.\n        user_prompts (dict): A dictionary of user prompts to override the \n            default prompts that the bot uses. The keys should be one or more of \n            the following: ['start', 'analyze_first_page_no_q', \n            'analyze_first_page_q', 'analyze_page_no_q', 'analyze_page_q', \n            'analyze_page_no_q_full_hist', 'analyze_page_q_full_hist', \n            'page_not_changed', 'system', 'resp_too_long', 'json_error', 'end'.] \n            If a key is not present in the dictionary, the default prompt will \n            be used. If a key that is not in the default prompts is present in \n            the dictionary, then the bot will exit with a warning and not \n            running to make sure that the user is aware of the issue.\n\n    Returns: \n        None (bot conversation logs are stored in database)\n    \"\"\"\n\n    if botex_db is None: botex_db = environ.get('BOTEX_DB')\n    if openai_api_key is None: openai_api_key = environ.get('OPENAI_API_KEY')\n    if bot_urls is None: \n        bot_urls = get_bot_urls(session_id, botex_db, already_started)\n    if model == \"local\":\n        if not \"path_to_llama_server\" in local_model_cfg:\n            local_model_cfg[\"path_to_llama_server\"] = environ.get('PATH_TO_LLAMA_SERVER')\n        if not \"local_llm_path\" in local_model_cfg:\n            local_model_cfg[\"local_llm_path\"] = environ.get('LOCAL_LLM_PATH')    \n        local_llm = LocalLLM(**local_model_cfg)\n        llm_server = local_llm.start_server()\n    else:\n        local_llm = None \n    threads = [\n        Thread(\n            target = run_bot, \n            kwargs = {\n                'botex_db': botex_db, 'session_id': session_id, \n                'url': url, 'full_conv_history': full_conv_history, \n                'model': model, 'openai_api_key': openai_api_key, \n                'local_llm': local_llm, 'user_prompts': user_prompts\n            }\n        ) for url in bot_urls \n    ]\n    for t in threads: t.start()\n    if wait: \n        for t in threads: t.join()\n\n    if local_llm:\n        assert llm_server, \"Local LLM server not started, but should have been.\"\n        local_llm.stop_server(llm_server)\n</code></pre>"},{"location":"reference.html#run_single_bot","title":"<code>run_single_bot</code>","text":"<p>Runs a single botex bot manually.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The participant URL to start the bot on.</p> required <code>session_name</code> <code>str</code> <p>The name of the oTree session. Defaults to \"unknown\"</p> <code>'unknown'</code> <code>session_id</code> <code>str</code> <p>The oTree ID of the oTree session.  Defaults to \"unknown\".</p> <code>'unknown'</code> <code>participant_id</code> <code>str</code> <p>The oTree ID of the participant.  Defaults to \"unknown\".</p> <code>'unknown'</code> <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file to store botex  data.</p> <code>None</code> <code>full_conv_history</code> <code>bool</code> <p>Whether to keep the full conversation history. This will increase token use and only work with very short  experiments. Default is False.</p> <code>False</code> <code>model</code> <code>str</code> <p>The model to use for the bot. Default is \"gpt-4o\" from OpenAI. You will need an OpenAI key and be prepared to pay to  use this model.</p> <code>'gpt-4o'</code> <code>openai_api_key</code> <code>str</code> <p>The API key for the OpenAI service.</p> <code>None</code> <code>local_llm</code> <code>LocalLLM</code> <p>A LocalLLM object to use for the bot. If this is  not None, the bot will use the local model instead of the OpenAI model.</p> <code>None</code> <p>Returns:</p> Type Description <p>None (conversation is stored in the botex database)</p> Source code in <code>src/botex/otree.py</code> <pre><code>def run_single_bot(\n    url, session_name = \"unknown\", session_id = \"unknown\", \n    participant_id = \"unknown\",\n    botex_db = None, full_conv_history = False,\n    model = \"gpt-4o\", openai_api_key = None,\n    local_llm: LocalLLM | None = None, user_prompts: dict | None = None\n):\n    \"\"\"\n    Runs a single botex bot manually.\n\n    Args:\n        url (str): The participant URL to start the bot on.\n        session_name (str): The name of the oTree session. Defaults to \"unknown\"\n        session_id (str): The oTree ID of the oTree session. \n            Defaults to \"unknown\".\n        participant_id (str): The oTree ID of the participant. \n            Defaults to \"unknown\".\n        botex_db (str): The name of the SQLite database file to store botex \n            data.\n        full_conv_history (bool): Whether to keep the full conversation history.\n            This will increase token use and only work with very short \n            experiments. Default is False.\n        model (str): The model to use for the bot. Default is \"gpt-4o\"\n            from OpenAI. You will need an OpenAI key and be prepared to pay to \n            use this model.\n        openai_api_key (str): The API key for the OpenAI service.\n        local_llm (LocalLLM): A LocalLLM object to use for the bot. If this is \n            not None, the bot will use the local model instead of the OpenAI model.\n\n    Returns: \n        None (conversation is stored in the botex database)\n    \"\"\"\n    if botex_db is None: botex_db = environ.get('BOTEX_DB')\n    if openai_api_key is None: openai_api_key = environ.get('OPENAI_API_KEY')\n    if model == \"local\":\n        if not \"path_to_llama_server\" in local_model_cfg:\n            local_model_cfg[\"path_to_llama_server\"] = environ.get('PATH_TO_LLAMA_SERVER')\n        if not \"local_llm_path\" in local_model_cfg:\n            local_model_cfg[\"local_llm_path\"] = environ.get('LOCAL_LLM_PATH')    \n        local_llm = LocalLLM(**local_model_cfg)\n        llm_server = local_llm.start_server()\n    else:\n        local_llm = None \n\n    if local_llm:\n        assert llm_server, \"Local LLM server not started, but should have been.\"\n        local_llm.stop_server(llm_server)\n\n    is_human = 0\n\n    conn = setup_botex_db(botex_db)\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        INSERT INTO participants (\n            session_name, session_id, participant_id, is_human, url) \n            VALUES (?, ?, ?, ?, ?) \n        \"\"\", (session_name, session_id, participant_id, is_human, url,)\n    )\n    conn.commit()\n    cursor.close()\n\n    run_bot(\n        botex_db, session_id, url, full_conv_history,\n        model, openai_api_key, local_llm, user_prompts\n    )\n</code></pre>"},{"location":"reference.html#export-botex-data","title":"Export botex data","text":"<p>Running oTree experiments with botex generates two databases:</p> <ol> <li>The 'normal' experiment data that oTree collects.</li> <li>Additional data that botex collects, such as the prompting sequence between botex and the bots, as well as the answers and the reasoning behind the answers that the LLM bots provide.</li> </ol> <p>botex provides functions to export botex data from the botex database. To obtain oTree data, you need to use the oTree web interface.</p>"},{"location":"reference.html#read_participants_from_botex_db","title":"<code>read_participants_from_botex_db</code>","text":"<p>Read the participants table from the botex database.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of a SQLite database file. If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <p>Returns:</p> Type Description <p>A list of dictionaries with participant data.</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def read_participants_from_botex_db(session_id = None, botex_db = None):\n    \"\"\"\n    Read the participants table from the botex database.\n\n    Args:\n        session_id (str, optional): A session ID to filter the results.\n        botex_db (str, optional): The name of a SQLite database file.\n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n\n    Returns:\n        A list of dictionaries with participant data.\n    \"\"\"\n\n    if botex_db is None: botex_db = environ.get('BOTEX_DB')\n    conn = sqlite3.connect(botex_db)\n    conn.row_factory = sqlite3.Row \n    cursor = conn.cursor()\n    if session_id:\n        cursor.execute(\n            \"SELECT * FROM participants WHERE session_id = ?\", (session_id,)\n        )\n    else:\n        cursor.execute(\"SELECT * FROM participants\")\n    sessions = [dict(row) for row in cursor.fetchall()]\n    cursor.close()\n    conn.close()\n    return sessions\n</code></pre>"},{"location":"reference.html#read_conversations_from_botex_db","title":"<code>read_conversations_from_botex_db</code>","text":"<p>Reads the conversations table from the botex database.  The conversation table contains the messages exchanged  with the LLM underlying the bot.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of a SQLite database file. If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <p>Returns:</p> Type Description <p>A list of dictionaries with the conversation data.</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def read_conversations_from_botex_db(\n        session_id = None, botex_db = None\n    ):\n    \"\"\"\n    Reads the conversations table from the botex database. \n    The conversation table contains the messages exchanged \n    with the LLM underlying the bot.\n\n    Args:\n        session_id (str, optional): A session ID to filter the results.\n        botex_db (str, optional): The name of a SQLite database file.\n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n\n    Returns:\n        A list of dictionaries with the conversation data.\n    \"\"\"\n    if botex_db is None: botex_db = environ.get('BOTEX_DB')\n    conn = sqlite3.connect(botex_db)\n    conn.row_factory = sqlite3.Row \n    cursor = conn.cursor()\n    if session_id:\n        cursor.execute(\n            \"SELECT * FROM conversations WHERE session_id = ?\", (session_id,)\n        )\n    else:\n        cursor.execute(\"SELECT * FROM conversations\")\n    conversations = [dict(row) for row in cursor.fetchall()]\n    cursor.close()\n    conn.close()\n    return conversations\n</code></pre>"},{"location":"reference.html#read_responses_from_botex_db","title":"<code>read_responses_from_botex_db</code>","text":"<p>Extracts the responses and their rationales from the botex conversation data. </p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of a SQLite database file. If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <p>Returns:</p> Type Description <p>A list of dictionaries with the rationale data.</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def read_responses_from_botex_db(session_id = None, botex_db = None):\n    \"\"\"\n    Extracts the responses and their rationales from the botex conversation data. \n\n    Args:\n        session_id (str, optional): A session ID to filter the results.\n        botex_db (str, optional): The name of a SQLite database file.\n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n\n    Returns:\n        A list of dictionaries with the rationale data.\n    \"\"\"\n\n    cs = read_conversations_from_botex_db(botex_db = botex_db)\n    resp = [parse_conversation(c) for c in cs]\n    rt = []\n    for r in resp:\n        for a in r['answers']:\n            rt.append({\n                'session_id': r['session_id'], \n                'participant_id': r['participant_id'], \n                'round': a['round'], \n                'question_id': a['id'], \n                'answer': a['answer'], \n                'reason': a['reason']\n            })\n    return rt\n</code></pre>"},{"location":"reference.html#export_participant_data","title":"<code>export_participant_data</code>","text":"<p>Export the participants table from the botex database to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>The file path to save the CSV file.</p> required <code>botex_db</code> <code>str</code> <p>The file path to the botex sqlite3 file.  If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <p>Returns:</p> Type Description <p>None (saves the CSV to the specified file path)</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def export_participant_data(csv_file, botex_db = None):\n    \"\"\"\n    Export the participants table from the botex database to a CSV file.\n\n    Args:\n        csv_file (str): The file path to save the CSV file.\n        botex_db (str, optional): The file path to the botex sqlite3 file. \n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n\n    Returns:\n        None (saves the CSV to the specified file path)\n    \"\"\"\n    p = read_participants_from_botex_db(botex_db = botex_db)\n    with open(csv_file, 'w') as f:\n        w = csv.DictWriter(f, p[0].keys())\n        w.writeheader()\n        w.writerows(p)\n</code></pre>"},{"location":"reference.html#export_response_data","title":"<code>export_response_data</code>","text":"<p>Export the responses parsed from the bot conversations in the botex database to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>The file path to save the CSV file.</p> required <code>botex_db</code> <code>str</code> <p>The file path to the botex sqlite3 file.  If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <p>Returns:</p> Type Description <p>None (saves the CSV to the specified file path)</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def export_response_data(csv_file, botex_db = None):\n    \"\"\"\n    Export the responses parsed from the bot conversations in the botex\n    database to a CSV file.\n\n    Args:\n        csv_file (str): The file path to save the CSV file.\n        botex_db (str, optional): The file path to the botex sqlite3 file. \n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n\n    Returns:\n        None (saves the CSV to the specified file path)\n    \"\"\"\n\n    r = read_responses_from_botex_db(botex_db = botex_db)\n    with open(csv_file, 'w') as f:\n        w = csv.DictWriter(f, r[0].keys())\n        w.writeheader()\n        w.writerows(r)\n</code></pre>"}]}