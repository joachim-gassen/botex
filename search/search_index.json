{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"botex: Using LLMs as Experimental Participants in oTree","text":"<p>This in-development Python package allows you to use large language models (LLMs) as bots in oTree experiments. It has been inspired by recent work of Grossmann, Engel and Ockenfels (paper, repo) but uses a different approach. Instead of using dedicated prompts, botex bots consecutively scrape their respective oTree participant's webpage and infer the experimental flow solely from the webpage text content. This avoids the risk of misalignment between human (webpage) and bot (LLM prompt) experimental designs and, besides facilitating the study of LLM \"behavior\", allows to use LLM participants to develop and pre-test oTree experiments that are designed (primarily) for human participants.</p> <p>The downside of this approach is that the scraping has to rely on some level of standardization. Luckily, the oTree framework is relatively rigid, unless the user adds customized HTML forms to their experimental designs. Currently, all standard form models used by oTree are tested and verified to work. In the future, we plan to implement also customized HTML forms but likely this will require some standardization by the user implementing the experimental design.</p> <p>For interfacing with LLMs, botex offers two options</p> <ul> <li>litellm: Allows the use of various commercial LLMs</li> <li>llama.cpp: Allows the use of local (open source) LLMs  </li> </ul> <p>While both approaches have been tested and found to work, currently, we have only used OpenAI's Chat GPT-4o model for our own research work. See further below for a list of commercial and open-source LLMs that we have verified to pass the package tests.</p>"},{"location":"getting_started.html","title":"Getting Started","text":""},{"location":"reference.html","title":"API Reference","text":""},{"location":"reference.html#setup","title":"Setup","text":""},{"location":"reference.html#load_botex_env","title":"<code>load_botex_env</code>","text":"<p>Load botex environment variables from a file.</p> <p>Parameters:</p> Name Type Description Default <code>env_file</code> <code>str</code> <p>The path to the .env file containing the botex configuration. Defaults to \"botex.env\".</p> <code>'botex.env'</code> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>True if at least one botex environment variable was set.</p> Source code in <code>src/botex/env.py</code> <pre><code>def load_botex_env(env_file = \"botex.env\") -&gt; bool:\n    \"\"\"\n    Load botex environment variables from a file.\n\n    Args:\n        env_file (str, optional): The path to the .env file containing\n            the botex configuration. Defaults to \"botex.env\".\n\n    Returns:\n        Bool: True if at least one botex environment variable was set.\n    \"\"\"\n    if not os.path.exists(env_file):\n        logger.warning(\n            f\"Could not read any botex environment variables from '{env_file}' \"\n            \"as the file does not exist. \"\n            \"Please make sure that the file is in the right location and that \"\n            \"it sets the botex environment variables that you need.\"\n        )\n        return False\n    success = load_dotenv(env_file)\n    if success:\n        logger.info(f\"Loaded botex environment variables from '{env_file}'\")\n    else:\n        logger.info(\n            f\"Botex environment variables parsed from '{env_file}'. \"\n            \"No new environment variables were set.\"\n        )\n    return success\n</code></pre>"},{"location":"reference.html#start_llamacpp_server","title":"<code>start_llamacpp_server</code>","text":"<p>Starts a llama.cpp server instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict | None</code> <p>A dict containing to the least the keys <code>llamacpp_server_path</code> (the path to the llama.cpp  executable) and <code>local_llm_path</code> (the path to the LLM model that you want llama.cpp to use). If None (the default), then these parameters are read from the environment variables <code>LLAMACPP_SERVER_PATH</code> and  <code>LLAMACPP_LOCAL_LLM_PATH</code>. See notes below for additional configuration parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>Popen</code> <p>The process of the running llama.cpp sever if start was successful.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the server is already running or if starting the server  fails.            </p> Additional details <p>You can provide other configuration parameters for the  local model in the model configuration dictionary. These include:</p> <pre><code>-   `server_url` (str): The base URL for the llama.cpp \n    server, defaults to `\"http://localhost:8080\"`.\n\n-   `context_length` (int): The context length for the model. \n    If `None`, BotEx will try to get the context length from the \n    local model metadata; if that is not possible, it defaults \n    to `4096`.\n\n-   `number_of_layers_to_offload_to_gpu` (int): The number of \n    layers to offload to the GPU, defaults to `0`.\n\n- ` temperature` (float): The temperature for the model, \n    defaults to `0.5`.\n\n-   `maximum_tokens_to_predict` (int): The maximum number of \n    tokens to predict, defaults to `10000`.\n\n-   `top_p` (float): The top-p value for the model, \n    defaults to `0.9`.\n\n-   `top_k` (int): The top-k value for the model, \n    defaults to `40`.\n\n-   `num_slots` (int): The number of slots for the model, \n    defaults to `1`.\n</code></pre> <p>For all these keys, if not provided in the configuration dictionary,  botex will try to get the value from environment variables (in all  capital letters, prefixed by LLAMACPP_); if that is not possible, it  will use the default value.</p> Example <pre><code>from botex.llamacpp import start_llamacpp_server\n\nconfig = {\n    \"server_path\": \"/path/to/llama.cpp\",\n    \"local_llm_path\": \"/path/to/local/model\",\n    \"server_url\": \"http://localhost:8080\",\n    \"context_length\": 4096,\n    \"number_of_layers_to_offload_to_gpu\": 0,\n    \"temperature\": 0.8,\n    \"maximum_tokens_to_predict\": 10000,\n    \"top_p\": 0.9,\n    \"top_k\": 40,\n    \"num_slots\": 1\n}\n</code></pre> Source code in <code>src/botex/llamacpp.py</code> <pre><code>def start_llamacpp_server(config: dict | None = None) -&gt; subprocess.Popen:\n    \"\"\"\n    Starts a llama.cpp server instance.\n\n    Args:\n        config (dict | None, optional): A dict containing to the least\n            the keys `llamacpp_server_path` (the path to the llama.cpp \n            executable) and `local_llm_path` (the path to the LLM model that you\n            want llama.cpp to use). If None (the default), then these parameters\n            are read from the environment variables `LLAMACPP_SERVER_PATH` and \n            `LLAMACPP_LOCAL_LLM_PATH`. See notes below for additional\n            configuration parameters\n\n    Returns:\n        The process of the running llama.cpp sever if start was\n            successful.\n\n    Raises:\n        Exception: If the server is already running or if starting the server \n            fails.            \n\n    ??? tip \"Additional details\"\n        You can provide other configuration parameters for the \n        local model in the model configuration dictionary. These include:\n\n            -   `server_url` (str): The base URL for the llama.cpp \n                server, defaults to `\"http://localhost:8080\"`.\n\n            -   `context_length` (int): The context length for the model. \n                If `None`, BotEx will try to get the context length from the \n                local model metadata; if that is not possible, it defaults \n                to `4096`.\n\n            -   `number_of_layers_to_offload_to_gpu` (int): The number of \n                layers to offload to the GPU, defaults to `0`.\n\n            - ` temperature` (float): The temperature for the model, \n                defaults to `0.5`.\n\n            -   `maximum_tokens_to_predict` (int): The maximum number of \n                tokens to predict, defaults to `10000`.\n\n            -   `top_p` (float): The top-p value for the model, \n                defaults to `0.9`.\n\n            -   `top_k` (int): The top-k value for the model, \n                defaults to `40`.\n\n            -   `num_slots` (int): The number of slots for the model, \n                defaults to `1`.\n\n\n        For all these keys, if not provided in the configuration dictionary, \n        botex will try to get the value from environment variables (in all \n        capital letters, prefixed by LLAMACPP_); if that is not possible, it \n        will use the default value.\n\n\n    ??? example \"Example\"\n        ```python\n        from botex.llamacpp import start_llamacpp_server\n\n        config = {\n            \"server_path\": \"/path/to/llama.cpp\",\n            \"local_llm_path\": \"/path/to/local/model\",\n            \"server_url\": \"http://localhost:8080\",\n            \"context_length\": 4096,\n            \"number_of_layers_to_offload_to_gpu\": 0,\n            \"temperature\": 0.8,\n            \"maximum_tokens_to_predict\": 10000,\n            \"top_p\": 0.9,\n            \"top_k\": 40,\n            \"num_slots\": 1\n        }\n        ```\n    \"\"\"\n    manager = LlamaCppServerManager(config)\n    return manager.start_server()\n</code></pre>"},{"location":"reference.html#stop_llamacpp_server","title":"<code>stop_llamacpp_server</code>","text":"<p>Stops a running llama.cpp server instance.</p> <p>Parameters:</p> Name Type Description Default <code>process</code> <code>Popen</code> <p>The process of the running llama.cpp server.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None (stops the running llama.cpp server)</p> Source code in <code>src/botex/llamacpp.py</code> <pre><code>def stop_llamacpp_server(process: subprocess.Popen) -&gt; None:\n    \"\"\"\n    Stops a running llama.cpp server instance.\n\n    Args:\n        process (subprocess.Popen): The process of the running llama.cpp server.\n\n    Returns:\n        None (stops the running llama.cpp server)\n    \"\"\"\n    LlamaCppServerManager.stop_server(process)\n</code></pre>"},{"location":"reference.html#otree-interface","title":"oTree Interface","text":""},{"location":"reference.html#start_otree_server","title":"<code>start_otree_server</code>","text":"<p>Start an oTree server in a subprocess.</p> <p>Parameters:</p> Name Type Description Default <code>project_path</code> <code>str</code> <p>Path to your oTree project folder. If None (the default), it will be obtained from the environment variable OTREE_PROJECT_PATH.</p> <code>None</code> <code>port</code> <code>int</code> <p>The port to run the server on. If None  (the default), it will first try to read from the  environment variable OTREE_PORT. It that is not set. it  will default to 8000.</p> <code>None</code> <code>log_file</code> <code>str</code> <p>Path to the log file. If None  (the default), it will first be tried to read from the environment variable OTREE_LOG_FILE. If that is not set, it will default to 'otree.log'.</p> <code>None</code> <code>auth_level</code> <code>str</code> <p>The authentication level for the oTree  server. It is set by environment variable OTREE_AUTH_LEVEL.  The default is None, which will leave this environment variable unchanged. if you use 'DEMO' or 'STUDY', the environment variable will be set accordingly and you need to provide a rest key in the argument 'rest_key' below.</p> <code>None</code> <code>rest_key</code> <code>str</code> <p>The API key for the oTree server. If None (the default), it will be obtained from the environment variable OTREE_REST_KEY.</p> <code>None</code> <code>admin_password</code> <code>str</code> <p>The admin password for the oTree server. For this to work, <code>settings.py</code> in the oTree project needs to read the password from the environment variable OTREE_ADMIN_PASSWORD (which is normally the case).</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds to wait for the  server. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>Popen</code> <p>A subprocess object.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the oTree server does not start within the timeout.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def start_otree_server(\n        project_path = None,\n        port = None, \n        log_file = None,\n        auth_level = None, \n        rest_key = None,\n        admin_password = None,\n        timeout = 5\n    ) -&gt; subprocess.Popen:\n    \"\"\"\n    Start an oTree server in a subprocess.\n\n    Args:\n        project_path (str, optional): Path to your oTree project folder.\n            If None (the default), it will be obtained from the environment\n            variable OTREE_PROJECT_PATH.\n        port (int, optional): The port to run the server on. If None \n            (the default), it will first try to read from the \n            environment variable OTREE_PORT. It that is not set. it \n            will default to 8000.\n        log_file (str, optional): Path to the log file. If None \n            (the default), it will first be tried to read from the\n            environment variable OTREE_LOG_FILE. If that is not set,\n            it will default to 'otree.log'.\n        auth_level (str, optional): The authentication level for the oTree \n            server. It is set by environment variable OTREE_AUTH_LEVEL. \n            The default is None, which will leave this environment variable\n            unchanged. if you use 'DEMO' or 'STUDY', the environment variable\n            will be set accordingly and you need to provide a rest key\n            in the argument 'rest_key' below.\n        rest_key (str, optional): The API key for the oTree server.\n            If None (the default), it will be obtained from the environment\n            variable OTREE_REST_KEY.\n        admin_password (str, optional): The admin password for the oTree server.\n            For this to work, `settings.py` in the oTree project needs to read\n            the password from the environment variable OTREE_ADMIN_PASSWORD\n            (which is normally the case).\n        timeout (int, optional): Timeout in seconds to wait for the \n            server. Defaults to 5.\n\n    Returns:\n        A subprocess object.\n\n    Raises:\n        Exception: If the oTree server does not start within the timeout.\n    \"\"\"\n    if project_path is None:\n        project_path = os.environ.get('OTREE_PROJECT_PATH')\n        if project_path is None:\n            raise Exception('No oTree project path provided.')\n    if port is None: port = os.environ.get('OTREE_PORT', 8000)\n    if log_file is None: log_file = os.environ.get('OTREE_LOG_FILE', 'otree.log')\n    otree_log = open(log_file, 'w')\n    if auth_level is not None: \n        os.environ['OTREE_AUTH_LEVEL'] = auth_level\n        if rest_key is not None:\n            os.environ['OTREE_REST_KEY'] = rest_key\n        if admin_password is not None:\n            os.environ['OTREE_ADMIN_PASSWORD'] = admin_password \n\n\n    if platform.system() == \"Windows\":\n        otree_server = subprocess.Popen(\n            [\"otree\", \"devserver\", str(port)], cwd=project_path,\n            stderr=otree_log, stdout=otree_log,\n            creationflags=subprocess.CREATE_NEW_PROCESS_GROUP\n        )\n    else:\n        otree_server = subprocess.Popen(\n            [\"otree\", \"devserver\", str(port)], cwd=project_path,\n            stderr=otree_log, stdout=otree_log\n        )\n    otree_server_url = f'http://localhost:{port}'\n    os.environ['OTREE_SERVER_URL'] = otree_server_url\n\n    # Access oTree API to check if server is running\n    time_out = time.time() + timeout\n    while True:\n        try: \n            data = call_otree_api(\n                requests.get, 'otree_version', otree_rest_key=rest_key\n            )\n        except:\n            data = {'error': \"No API response\"}\n\n        if 'version' in data:\n            logger.info(\n                \"oTree server started successfully \"\n                f\"with endpoint '{otree_server_url}'\"\n            )\n            break\n        else:\n            if time.time() &gt; time_out:\n                logger.error(\n                    f\"oTree endpoint '{otree_server_url}' did not respond \"\n                    f\"within {timeout} seconds. Exiting.\"\n                )\n                raise Exception('oTree server did not start.')\n            time.sleep(1)\n    return otree_server\n</code></pre>"},{"location":"reference.html#stop_otree_server","title":"<code>stop_otree_server</code>","text":"<p>Stop an oTree server subprocess.</p> <p>Parameters:</p> Name Type Description Default <code>otree_server</code> <code>subprocess</code> <p>The subprocess object to be terminated.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The return code of the oTree subprocess</p> Source code in <code>src/botex/otree.py</code> <pre><code>def stop_otree_server(otree_server: subprocess.Popen) -&gt; int:\n    \"\"\"\n    Stop an oTree server subprocess.\n\n    Args:\n        otree_server (subprocess): The subprocess object to be terminated.\n\n    Returns:\n        The return code of the oTree subprocess\n    \"\"\"\n    otree_running = otree_server.poll() is None\n    if otree_running: \n        proc = psutil.Process(otree_server.pid)\n        if platform.system() == \"Windows\":\n            proc.send_signal(signal.CTRL_BREAK_EVENT)\n            proc.wait()\n        else: \n            proc.children()[0].send_signal(signal.SIGKILL)\n            otree_server.kill()\n            otree_server.wait()\n    else:\n        logger.warning('oTree server already stopped.')\n    return otree_server.poll()\n</code></pre>"},{"location":"reference.html#get_session_configs","title":"<code>get_session_configs</code>","text":"<p>Get the session configurations from an oTree server.</p> <p>Parameters:</p> Name Type Description Default <code>otree_server_url</code> <code>str</code> <p>The URL of the oTree server. Read from  environment variable OTREE_SERVER_URL if None (the default).</p> <code>None</code> <code>otree_rest_key</code> <code>str</code> <p>The API key for the oTree server. Read from  environment variable OTREE_REST_KEY if None (the default).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The session configurations.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def get_session_configs(\n        otree_server_url: str | None = None,\n        otree_rest_key: str | None = None\n    ) -&gt; dict:\n    \"\"\"\n    Get the session configurations from an oTree server.\n\n    Args:\n        otree_server_url (str): The URL of the oTree server. Read from \n            environment variable OTREE_SERVER_URL if None (the default).\n        otree_rest_key (str): The API key for the oTree server. Read from \n            environment variable OTREE_REST_KEY if None (the default).\n\n    Returns:\n        dict: The session configurations.\n    \"\"\"\n\n    return call_otree_api(\n        requests.get, 'session_configs', \n        otree_server_url=otree_server_url, otree_rest_key=otree_rest_key\n    )\n</code></pre>"},{"location":"reference.html#init_otree_session","title":"<code>init_otree_session</code>","text":"<p>Initialize an oTree session with a given number of participants.</p> <p>Parameters:</p> Name Type Description Default <code>config_name</code> <code>str</code> <p>The name of the oTree session configuration.</p> required <code>npart</code> <code>int</code> <p>The total number of participants.</p> required <code>nhumans</code> <code>int</code> <p>The number of human participants (defaults to zero. Provide either nhumans or is_human, but not both.</p> <code>0</code> <code>is_human</code> <code>list</code> <p>A list of booleans indicating whether each participant  is human. Needs to be the same length as npart. If None (the  default), humans (if present) will be randomly assigned.</p> <code>None</code> <code>room_name</code> <code>str</code> <p>The name of the oTree room for the session. If None  (the default), no room will be used.</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file to store BotEx    data. If None (the default), it will be obtained from the  environment variable BOTEX_DB. If the database does not exist, it  will be created.</p> <code>None</code> <code>otree_server_url</code> <code>str</code> <p>The URL of the oTree server. If None (the  default), it will be obtained from the environment variable  OTREE_SERVER_URL.</p> <code>None</code> <code>otree_rest_key</code> <code>str</code> <p>The API key for the oTree server. If None (the  default), it will be obtained from the environment variable  OTREE_REST_KEY.</p> <code>None</code> <code>modified_session_config_fields</code> <code>dict</code> <p>A dictionary of fields to modify  in the the oTree session config. Default is None. </p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict with the keys 'session_id', 'participant_code', 'is_human',  'bot_urls', and 'human_urls' containing the session ID, participant  codes, human indicators, and the URLs for the human and bot  participants.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def init_otree_session(\n        config_name: str,\n        npart: int,\n        nhumans: int = 0, \n        is_human: List[bool] | None = None,\n        room_name: str | None = None,\n        botex_db: str | None = None,\n        otree_server_url: str | None = None,\n        otree_rest_key: str | None = None,\n        modified_session_config_fields: dict | None = None,\n    ) -&gt; dict:\n    \"\"\"\n    Initialize an oTree session with a given number of participants.\n\n    Args:\n        config_name (str): The name of the oTree session configuration.\n        npart (int): The total number of participants.\n        nhumans (int): The number of human participants (defaults to zero.\n            Provide either nhumans or is_human, but not both.\n        is_human (list): A list of booleans indicating whether each participant \n            is human. Needs to be the same length as npart. If None (the \n            default), humans (if present) will be randomly assigned.\n        room_name (str): The name of the oTree room for the session. If None \n            (the default), no room will be used.\n        botex_db (str): The name of the SQLite database file to store BotEx     \n            data. If None (the default), it will be obtained from the \n            environment variable BOTEX_DB. If the database does not exist, it \n            will be created.\n        otree_server_url (str): The URL of the oTree server. If None (the \n            default), it will be obtained from the environment variable \n            OTREE_SERVER_URL.\n        otree_rest_key (str): The API key for the oTree server. If None (the \n            default), it will be obtained from the environment variable \n            OTREE_REST_KEY.\n        modified_session_config_fields (dict): A dictionary of fields to modify \n            in the the oTree session config. Default is None. \n\n    Returns:\n        dict with the keys 'session_id', 'participant_code', 'is_human', \n            'bot_urls', and 'human_urls' containing the session ID, participant \n            codes, human indicators, and the URLs for the human and bot \n            participants.\n    \"\"\"\n\n    if nhumans &gt; 0 and is_human is not None: raise(Exception(\n        \"Provide either nhumans or is_human, but not both.\"\n    ))\n\n    if is_human is not None:\n        if len(is_human) != npart: raise(Exception(\n            \"Length of is_human must be the same as npart.\"\n        ))\n\n    if is_human is None and nhumans &gt; 0:\n        is_human = [True]*nhumans + [False]*(npart - nhumans)\n        shuffle(is_human)\n\n    if is_human is None and nhumans == 0: is_human = [False]*npart\n\n    if botex_db is None: botex_db = os.environ.get('BOTEX_DB')\n\n    if otree_server_url is None:\n        otree_server_url = os.environ.get('OTREE_SERVER_URL')\n\n    session_id = call_otree_api(\n        requests.post, 'sessions', \n        otree_server_url=otree_server_url, otree_rest_key=otree_rest_key, \n        session_config_name=config_name, \n        num_participants=npart, room_name=room_name,\n        modified_session_config_fields=modified_session_config_fields\n    )['code']\n    part_data = sorted(\n        call_otree_api(\n            requests.get, 'sessions', session_id,\n            otree_server_url=otree_server_url, otree_rest_key=otree_rest_key \n        )['participants'],\n        key=lambda d: d['id_in_session']\n    )\n    part_codes = [pd['code'] for pd in part_data]\n\n    base_url = otree_server_url + '/InitializeParticipant/'\n    urls = [base_url + pc for pc in part_codes]\n\n    rows = zip(\n        [config_name]*npart, [session_id]*npart, \n        part_codes, is_human, urls\n    )\n\n    conn = setup_botex_db(botex_db)\n    cursor = conn.cursor()\n    cursor.executemany(\n        \"\"\"\n        INSERT INTO participants (\n            session_name, session_id, participant_id, is_human, url) \n            VALUES (?, ?, ?, ?, ?) \n        \"\"\", rows\n    )\n    conn.commit()\n    cursor.close()\n    return {\n        'session_id': session_id, \n        'participant_code': part_codes,\n        'is_human': is_human,\n        'bot_urls': list(compress(urls, [not x for x in is_human])), \n        'human_urls': list(compress(urls, is_human))\n    }\n</code></pre>"},{"location":"reference.html#get_bot_urls","title":"<code>get_bot_urls</code>","text":"<p>Get the URLs for the bot participants in an oTree session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The ID of the oTree session.</p> required <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file to store BotEx  data. If None (the default), it will be obtained from the  environment variable BOTEX_DB.</p> <code>None</code> <code>already_started</code> <code>bool</code> <p>If True, the function will also run bots that  have already started but not yet finished. This is useful if bots  did not startup properly because of network issues. Default is  False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of URLs for the bot participants</p> Source code in <code>src/botex/otree.py</code> <pre><code>def get_bot_urls(\n        session_id: str,\n        botex_db: str | None = None,\n        already_started: bool = False\n    ) -&gt; List[str]:\n    \"\"\"\n    Get the URLs for the bot participants in an oTree session.\n\n    Args:\n        session_id (str): The ID of the oTree session.\n        botex_db (str): The name of the SQLite database file to store BotEx \n            data. If None (the default), it will be obtained from the \n            environment variable BOTEX_DB.\n        already_started (bool): If True, the function will also run bots that \n            have already started but not yet finished. This is useful if bots \n            did not startup properly because of network issues. Default is \n            False.\n\n    Returns:\n        List of URLs for the bot participants\n    \"\"\"\n\n    if botex_db is None: botex_db = os.environ.get('BOTEX_DB')\n    conn = sqlite3.connect(botex_db)\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT url,time_in,time_out FROM participants \n        WHERE session_id = ? AND is_human = 0\n        \"\"\", (session_id,)\n    )\n    if already_started:\n        urls = [row[0] for row in cursor.fetchall() if row[2] is None]\n    else:\n        urls = [row[0] for row in cursor.fetchall() if row[1] is None]\n    cursor.close()\n    conn.close()\n    return urls\n</code></pre>"},{"location":"reference.html#run_bots_on_session","title":"<code>run_bots_on_session</code>","text":"<p>Run botex bots on an oTree session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The ID of the oTree session.</p> required <code>bot_urls</code> <code>list</code> <p>A list of URLs for the bot participants. Will be retrieved from the database if None (the default).</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file for BotEx data. If None (the default), it will be obtained from the environment  variable BOTEX_DB.</p> <code>None</code> <code>model</code> <code>str</code> <p>The model to use for the bot. Default is  <code>gpt-4o-2024-08-06</code> from OpenAI vie LiteLLM. It needs to be a model  that supports structured outputs. For OpenAI, these are  gpt-4o-mini-2024-07-18 and later or gpt-4o-2024-08-06 and later. If  you use a commercial model, You need to provide an API key in the  parameter <code>api_key</code> and be prepared to pay to use this model.</p> <p>If you want to use local models, we suggest that you use llama.cpp,  In this case, set this string to <code>lamacpp</code> and set the URL of your  llama.cpp server in <code>api_base</code>. If you want botex to start the  llama.cpp server for you, run <code>start_llamacpp_sever()</code> prior to  running run_bots_on_session().</p> <code>'gpt-4o-2024-08-06'</code> <code>api_key</code> <code>str</code> <p>The API key for the model that you use. If None (the  default), it will be obtained from environment variables by liteLLM  (e.g., OPENAI_API_KEY or GEMINI_API_KEY). </p> <code>None</code> <code>api_base</code> <code>str</code> <p>The base URL for the llm server. Default is None not to interfere with the default LiteLLM behavior. If you want to use a  local model with llama.cpp and if you have not explicitly set this  parameter, it will default to <code>http://localhost:8080</code>, the default  url for the llama.cpp server.</p> <code>None</code> <code>throttle</code> <code>bool</code> <p>Whether to slow down the bot's requests. Slowing done  the requests can help to avoid rate limiting. Default is False. The  bot will switch to <code>throttle=True</code> when LiteLLM is used and  completion requests raise exceptions.</p> <code>False</code> <code>full_conv_history</code> <code>bool</code> <p>Whether to keep the full conversation history. This will increase token use and only work with very short  experiments. Default is False.</p> <code>False</code> <code>user_prompts</code> <code>dict</code> <p>A dictionary of user prompts to override the  default prompts that the bot uses. The keys should be one or more  of the following:</p> <p>[<code>start</code>, <code>analyze_first_page_no_q</code>, <code>analyze_first_page_q</code>,  <code>analyze_page_no_q</code>, <code>analyze_page_q</code>, <code>analyze_page_no_q_full_hist</code>, <code>analyze_page_q_full_hist</code>,  <code>page_not_changed</code>, <code>system</code>, <code>system_full_hist</code>, <code>resp_too_long</code>,  <code>json_error</code>, <code>end</code>, <code>end_full_hist</code>].</p> <p>If a key is not present in  the dictionary, the default prompt will  be used. If a key that is not in the default prompts is present in  the dictionary, then the bot will exit with a warning and not run  to make sure that the user is aware of the issue.</p> <code>None</code> <code>already_started</code> <code>bool</code> <p>If True, the function will also run bots that  have already started but not yet finished. This is useful if bots  did not startup properly because of network issues. Default is  False.</p> <code>False</code> <code>wait</code> <code>bool</code> <p>If True (the default), the function will wait for the bots  to finish.</p> <code>True</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass on to <code>litellm.completion()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None | List[Thread]</code> <p>None (bot conversation logs are stored in database) if wait is True. A list of Threads running the bots if wait is False.</p> Additional details <p>When running local models via llama.cpp, if you would like      botex to start the llama.cpp server for you,      run <code>start_llamacpp_server()</code> to start up the server prior to     running <code>run_bots_on_session()</code>.</p> Example Usage <ul> <li>Running botex with the default model (<code>gpt-4o-2024-08-06</code>)</li> </ul> <pre><code>run_bots_on_session(\n    session_id=\"your_session_id\",\n    botex_db=\"path/to/botex.db\",\n    api_key=\"your_openai_api_key\",\n    # Other parameters if and as needed\n)\n</code></pre> <ul> <li>Using a specific model supported by LiteLLM</li> </ul> <pre><code>run_bots_on_session(\n    session_id=\"your_session_id\",\n    botex_db=\"path/to/botex.db\",\n    model=\"gemini/gemini-1.5-flash\",\n    api_key=\"your_gemini_api_key\",\n    # Other parameters if and as needed\n)\n</code></pre> <ul> <li>Using a local model with BotEx starting the llama.cpp server</li> </ul> <pre><code>llamacpp_config = {\n    \"server_path\": \"/path/to/llama/server\",\n    \"local_llm_path\": \"/path/to/local/model\",\n    # Additional configuration parameters if and as needed\n}\nprocess_id = start_llamacpp_server(llamacpp_config)\nrun_bots_on_session(\n    session_id=\"your_session_id\",\n    botex_db=\"path/to/botex.db\",\n    model=\"llamacpp\",\n    # Other parameters if and as needed\n)\nstop_llamacpp_server(process_id)\n</code></pre> <ul> <li>Using a local model with an already running llama.cpp server that      uses a URL different from the default (if you are using the      default \"http://localhost:8080\", you can simply omit the <code>api_base</code>      parameter)</li> </ul> <pre><code>run_bots_on_session(\n    session_id=\"your_session_id\",\n    botex_db=\"path/to/botex.db\",\n    model = \"llamacpp\",\n    api_base = \"http://yourserver:port\"},\n    # Other parameters if and as needed\n)\n</code></pre> Source code in <code>src/botex/otree.py</code> <pre><code>def run_bots_on_session(\n        session_id: str,\n        bot_urls: List[str] | None = None, \n        botex_db: str | None = None, \n        model: str = \"gpt-4o-2024-08-06\",\n        api_key: str | None = None,\n        api_base: str | None = None,\n        throttle: bool = False,\n        full_conv_history: bool = False,\n        user_prompts: dict | None = None,\n        already_started: bool = False,\n        wait: bool = True,\n        **kwargs\n    ) -&gt; None | List[Thread]:\n    \"\"\"\n    Run botex bots on an oTree session.\n\n    Args:\n        session_id (str): The ID of the oTree session.\n        bot_urls (list): A list of URLs for the bot participants.\n            Will be retrieved from the database if None (the default).\n        botex_db (str): The name of the SQLite database file for BotEx data.\n            If None (the default), it will be obtained from the environment \n            variable BOTEX_DB.\n        model (str): The model to use for the bot. Default is \n            `gpt-4o-2024-08-06` from OpenAI vie LiteLLM. It needs to be a model \n            that supports structured outputs. For OpenAI, these are \n            gpt-4o-mini-2024-07-18 and later or gpt-4o-2024-08-06 and later. If \n            you use a commercial model, You need to provide an API key in the \n            parameter `api_key` and be prepared to pay to use this model.\n\n            If you want to use local models, we suggest that you use llama.cpp, \n            In this case, set this string to `lamacpp` and set the URL of your \n            llama.cpp server in `api_base`. If you want botex to start the  llama.cpp server for you, run `start_llamacpp_sever()` prior to \n            running run_bots_on_session().\n        api_key (str): The API key for the model that you use. If None (the \n            default), it will be obtained from environment variables by liteLLM \n            (e.g., OPENAI_API_KEY or GEMINI_API_KEY). \n        api_base (str): The base URL for the llm server. Default is None not to\n            interfere with the default LiteLLM behavior. If you want to use a \n            local model with llama.cpp and if you have not explicitly set this \n            parameter, it will default to `http://localhost:8080`, the default \n            url for the llama.cpp server.\n        throttle (bool): Whether to slow down the bot's requests. Slowing done \n            the requests can help to avoid rate limiting. Default is False. The \n            bot will switch to `throttle=True` when LiteLLM is used and \n            completion requests raise exceptions.\n        full_conv_history (bool): Whether to keep the full conversation history.\n            This will increase token use and only work with very short \n            experiments. Default is False.\n        user_prompts (dict): A dictionary of user prompts to override the \n            default prompts that the bot uses. The keys should be one or more \n            of the following:\n\n            [`start`, `analyze_first_page_no_q`, `analyze_first_page_q`, \n            `analyze_page_no_q`, `analyze_page_q`,\n            `analyze_page_no_q_full_hist`, `analyze_page_q_full_hist`, \n            `page_not_changed`, `system`, `system_full_hist`, `resp_too_long`, \n            `json_error`, `end`, `end_full_hist`].\n\n            If a key is not present in  the dictionary, the default prompt will \n            be used. If a key that is not in the default prompts is present in \n            the dictionary, then the bot will exit with a warning and not run \n            to make sure that the user is aware of the issue.\n        already_started (bool): If True, the function will also run bots that \n            have already started but not yet finished. This is useful if bots \n            did not startup properly because of network issues. Default is \n            False.\n        wait (bool): If True (the default), the function will wait for the bots \n            to finish.\n        kwargs (dict): Additional keyword arguments to pass on to\n            `litellm.completion()`.\n\n    Returns:\n        None (bot conversation logs are stored in database) if wait is True. A list of Threads running the bots if wait is False.\n\n    ??? tip \"Additional details\"\n\n        When running local models via llama.cpp, if you would like \n            botex to start the llama.cpp server for you, \n            run `start_llamacpp_server()` to start up the server prior to\n            running `run_bots_on_session()`.\n\n    ??? example \"Example Usage\"\n\n        - Running botex with the default model (`gpt-4o-2024-08-06`)\n\n        ```python\n        run_bots_on_session(\n            session_id=\"your_session_id\",\n            botex_db=\"path/to/botex.db\",\n            api_key=\"your_openai_api_key\",\n            # Other parameters if and as needed\n        )\n        ```\n\n        - Using a specific model supported by LiteLLM\n\n        ```python    \n        run_bots_on_session(\n            session_id=\"your_session_id\",\n            botex_db=\"path/to/botex.db\",\n            model=\"gemini/gemini-1.5-flash\",\n            api_key=\"your_gemini_api_key\",\n            # Other parameters if and as needed\n        )\n        ```\n\n        - Using a local model with BotEx starting the llama.cpp server\n\n        ```python\n        llamacpp_config = {\n            \"server_path\": \"/path/to/llama/server\",\n            \"local_llm_path\": \"/path/to/local/model\",\n            # Additional configuration parameters if and as needed\n        }\n        process_id = start_llamacpp_server(llamacpp_config)\n        run_bots_on_session(\n            session_id=\"your_session_id\",\n            botex_db=\"path/to/botex.db\",\n            model=\"llamacpp\",\n            # Other parameters if and as needed\n        )\n        stop_llamacpp_server(process_id)\n        ```\n\n        - Using a local model with an already running llama.cpp server that \n            uses a URL different from the default (if you are using the \n            default \"http://localhost:8080\", you can simply omit the `api_base` \n            parameter)\n\n        ```python\n        run_bots_on_session(\n            session_id=\"your_session_id\",\n            botex_db=\"path/to/botex.db\",\n            model = \"llamacpp\",\n            api_base = \"http://yourserver:port\"},\n            # Other parameters if and as needed\n        )\n        ```\n    \"\"\"\n    if botex_db is None: botex_db = os.environ.get('BOTEX_DB')\n    if api_key is None and 'openai_api_key' in kwargs: \n        api_key = kwargs.pop('openai_api_key')\n    if bot_urls is None: \n        bot_urls = get_bot_urls(session_id, botex_db, already_started)\n\n    thread_kwargs = {\n        'botex_db': botex_db, 'session_id': session_id, \n        'full_conv_history': full_conv_history, \n        'model': model, 'api_key': api_key,\n        'api_base': api_base,\n        'user_prompts': user_prompts,\n        'throttle': throttle\n    }\n    thread_kwargs.update(kwargs)\n    threads = [\n        Thread(\n            target = run_bot, \n            kwargs = dict(thread_kwargs, **{'url': url})\n        ) for url in bot_urls \n    ]\n    for t in threads: t.start()\n    if wait: \n        for t in threads: t.join()\n    else:\n        return threads\n</code></pre>"},{"location":"reference.html#run_single_bot","title":"<code>run_single_bot</code>","text":"<p>Runs a single botex bot manually.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The participant URL to start the bot on.</p> required <code>session_name</code> <code>str</code> <p>The name of the oTree session. Defaults to \"unknown\"</p> <code>'unknown'</code> <code>session_id</code> <code>str</code> <p>The oTree ID of the oTree session. Defaults to  \"unknown\".</p> <code>'unknown'</code> <code>participant_id</code> <code>str</code> <p>The oTree ID of the participant. Defaults to  \"unknown\".</p> <code>'unknown'</code> <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file to store botex  data.</p> <code>None</code> <code>full_conv_history</code> <code>bool</code> <p>Whether to keep the full conversation history. This will increase token use and only work with very short  experiments. Default is False.</p> <code>False</code> <code>model</code> <code>str</code> <p>The model to use for the bot. Default is  <code>gpt-4o-2024-08-06</code> from OpenAI vie LiteLLM. It needs to be a model  that supports structured outputs. For OpenAI, these are  gpt-4o-mini-2024-07-18 and later or gpt-4o-2024-08-06 and later. If  you use a commercial model, You need to provide an API key in the  parameter <code>api_key</code> and be prepared to pay to use this model.</p> <p>If you want to use local models, we suggest that you use llama.cpp,  In this case, set this string to <code>lamacpp</code> and set the URL of your  llama.cpp server in <code>api_base</code>. If you want botex to start the  llama.cpp server for you, run <code>start_llamacpp_sever()</code> prior to  running run_bots_on_session().</p> <code>'gpt-4o-2024-08-06'</code> <code>api_key</code> <code>str</code> <p>The API key for the model that you use. If None (the  default), it will be obtained from environment variables by liteLLM  (e.g., OPENAI_API_KEY or GEMINI_API_KEY). </p> <code>None</code> <code>api_base</code> <code>str</code> <p>The base URL for the llm server. Default is None not to interfere with the default LiteLLM behavior. If you want to use a  local model with llama.cpp and if you have not explicitly set this  parameter, it will default to <code>http://localhost:8080</code>, the default  url for the llama.cpp server.</p> <code>None</code> <code>throttle</code> <code>bool</code> <p>Whether to slow down the bot's requests. Slowing done  the requests can help to avoid rate limiting. Default is False. The  bot will switch to <code>throttle=True</code> when LiteLLM is used and  completion requests raise exceptions.</p> <code>False</code> <code>full_conv_history</code> <code>bool</code> <p>Whether to keep the full conversation history. This will increase token use and only work with very short  experiments. Default is False.</p> <code>False</code> <code>user_prompts</code> <code>dict</code> <p>A dictionary of user prompts to override the  default prompts that the bot uses. The keys should be one or more  of the following:</p> <p>[<code>start</code>, <code>analyze_first_page_no_q</code>, <code>analyze_first_page_q</code>,  <code>analyze_page_no_q</code>, <code>analyze_page_q</code>, <code>analyze_page_no_q_full_hist</code>, <code>analyze_page_q_full_hist</code>,  <code>page_not_changed</code>, <code>system</code>, <code>system_full_hist</code>, <code>resp_too_long</code>,  <code>json_error</code>, <code>end</code>, <code>end_full_hist</code>].</p> <p>If a key is not present in  the dictionary, the default prompt will  be used. If a key that is not in the default prompts is present in  the dictionary, then the bot will exit with a warning and not run  to make sure that the user is aware of the issue.</p> <code>None</code> <code>wait</code> <code>bool</code> <p>If True (the default), the function will wait for the bots  to finish.</p> <code>True</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass on to <code>litellm.completion()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None | Thread</code> <p>None (conversation is stored in the botex database) if wait is True. The Thread running the bot if wait is False.</p> <p>Notes:</p> <ul> <li>When running local models via llama.cpp, if you would like      botex to start the llama.cpp server for you,      run <code>start_llamacpp_server()</code> to start up the server prior to     running `run_bots_on_session().</li> </ul> Example Usage <ul> <li>Using a model via LiteLLM</li> </ul> <pre><code>run_single_bot(\n    url=\"your_participant_url\",\n    session_name=\"your_session_name\",\n    session_id=\"your_session_id\",\n    participant_id=\"your_participant_id\",\n    botex_db=\"path/to/botex.db\",\n    model=\"a LiteLLM model string, e.g. 'gemini/gemini-1.5-flash'\",\n    api_key=\"the API key for your model provide\",\n    # Other parameters if and as needed\n)\n</code></pre> <ul> <li>Using a local model with an already running llama.cpp server</li> </ul> <pre><code>run_single_bot(\n    url=\"your_participant_url\",\n    session_name=\"your_session_name\",\n    session_id=\"your_session_id\",\n    participant_id=\"your_participant_id\",\n    botex_db=\"path/to/botex.db\",\n    model=\"llamacpp\",\n    api_base=\"http://yourhost:port\" # defaults to http://localhost:8080\n    # Other parameters if and as needed\n)\n</code></pre> <ul> <li>Using a local model with BotEx starting the llama.cpp server</li> </ul> <pre><code>llamacpp_config = {\n    \"server_path\": \"/path/to/llama/server\",\n    \"local_llm_path\": \"/path/to/local/model\",\n    # Additional configuration parameters if and as needed\n}\nprocess_id = start_llamacpp_server(llamacpp_config)\nrun_single_bot(\n    url=\"your_participant_url\",\n    session_name=\"your_session_name\",\n    session_id=\"your_session_id\",\n    participant_id=\"your_participant_id\",\n    botex_db=\"path/to/botex.db\",\n    model=\"llamacpp\",\n    # Other parameters if and as needed\n)\nstop_llamacpp_server(process_id)\n</code></pre> Source code in <code>src/botex/otree.py</code> <pre><code>def run_single_bot(\n    url: str,\n    session_name: str = \"unknown\",\n    session_id: str = \"unknown\", \n    participant_id: str = \"unknown\",\n    botex_db: str | None = None,\n    model: str = \"gpt-4o-2024-08-06\",\n    api_key: str | None = None,\n    api_base: str | None = None,\n    throttle: bool = False, \n    full_conv_history: bool = False,\n    user_prompts: dict | None = None,\n    wait: bool = True,\n    **kwargs\n) -&gt; None | Thread:\n    \"\"\"\n    Runs a single botex bot manually.\n\n    Args:\n        url (str): The participant URL to start the bot on.\n        session_name (str): The name of the oTree session. Defaults to \"unknown\"\n        session_id (str): The oTree ID of the oTree session. Defaults to \n            \"unknown\".\n        participant_id (str): The oTree ID of the participant. Defaults to \n            \"unknown\".\n        botex_db (str): The name of the SQLite database file to store botex \n            data.\n        full_conv_history (bool): Whether to keep the full conversation history.\n            This will increase token use and only work with very short \n            experiments. Default is False.\n        model (str): The model to use for the bot. Default is \n            `gpt-4o-2024-08-06` from OpenAI vie LiteLLM. It needs to be a model \n            that supports structured outputs. For OpenAI, these are \n            gpt-4o-mini-2024-07-18 and later or gpt-4o-2024-08-06 and later. If \n            you use a commercial model, You need to provide an API key in the \n            parameter `api_key` and be prepared to pay to use this model.\n\n            If you want to use local models, we suggest that you use llama.cpp, \n            In this case, set this string to `lamacpp` and set the URL of your \n            llama.cpp server in `api_base`. If you want botex to start the  llama.cpp server for you, run `start_llamacpp_sever()` prior to \n            running run_bots_on_session().\n        api_key (str): The API key for the model that you use. If None (the \n            default), it will be obtained from environment variables by liteLLM \n            (e.g., OPENAI_API_KEY or GEMINI_API_KEY). \n        api_base (str): The base URL for the llm server. Default is None not to\n            interfere with the default LiteLLM behavior. If you want to use a \n            local model with llama.cpp and if you have not explicitly set this \n            parameter, it will default to `http://localhost:8080`, the default \n            url for the llama.cpp server.\n        throttle (bool): Whether to slow down the bot's requests. Slowing done \n            the requests can help to avoid rate limiting. Default is False. The \n            bot will switch to `throttle=True` when LiteLLM is used and \n            completion requests raise exceptions.\n        full_conv_history (bool): Whether to keep the full conversation history.\n            This will increase token use and only work with very short \n            experiments. Default is False.\n        user_prompts (dict): A dictionary of user prompts to override the \n            default prompts that the bot uses. The keys should be one or more \n            of the following:\n\n            [`start`, `analyze_first_page_no_q`, `analyze_first_page_q`, \n            `analyze_page_no_q`, `analyze_page_q`,\n            `analyze_page_no_q_full_hist`, `analyze_page_q_full_hist`, \n            `page_not_changed`, `system`, `system_full_hist`, `resp_too_long`, \n            `json_error`, `end`, `end_full_hist`].\n\n            If a key is not present in  the dictionary, the default prompt will \n            be used. If a key that is not in the default prompts is present in \n            the dictionary, then the bot will exit with a warning and not run \n            to make sure that the user is aware of the issue.\n        wait (bool): If True (the default), the function will wait for the bots \n            to finish.\n        kwargs (dict): Additional keyword arguments to pass on to\n            `litellm.completion()`.\n\n    Returns:\n        None (conversation is stored in the botex database) if wait is True.\n            The Thread running the bot if wait is False.\n\n    Notes:\n\n    -   When running local models via llama.cpp, if you would like \n        botex to start the llama.cpp server for you, \n        run `start_llamacpp_server()` to start up the server prior to\n        running `run_bots_on_session().\n\n    ??? example \"Example Usage\"\n\n        - Using a model via LiteLLM\n\n        ```python\n        run_single_bot(\n            url=\"your_participant_url\",\n            session_name=\"your_session_name\",\n            session_id=\"your_session_id\",\n            participant_id=\"your_participant_id\",\n            botex_db=\"path/to/botex.db\",\n            model=\"a LiteLLM model string, e.g. 'gemini/gemini-1.5-flash'\",\n            api_key=\"the API key for your model provide\",\n            # Other parameters if and as needed\n        )\n        ```\n\n\n        - Using a local model with an already running llama.cpp server\n\n        ```python\n        run_single_bot(\n            url=\"your_participant_url\",\n            session_name=\"your_session_name\",\n            session_id=\"your_session_id\",\n            participant_id=\"your_participant_id\",\n            botex_db=\"path/to/botex.db\",\n            model=\"llamacpp\",\n            api_base=\"http://yourhost:port\" # defaults to http://localhost:8080\n            # Other parameters if and as needed\n        )\n        ```\n\n        - Using a local model with BotEx starting the llama.cpp server\n\n        ```python\n        llamacpp_config = {\n            \"server_path\": \"/path/to/llama/server\",\n            \"local_llm_path\": \"/path/to/local/model\",\n            # Additional configuration parameters if and as needed\n        }\n        process_id = start_llamacpp_server(llamacpp_config)\n        run_single_bot(\n            url=\"your_participant_url\",\n            session_name=\"your_session_name\",\n            session_id=\"your_session_id\",\n            participant_id=\"your_participant_id\",\n            botex_db=\"path/to/botex.db\",\n            model=\"llamacpp\",\n            # Other parameters if and as needed\n        )\n        stop_llamacpp_server(process_id)\n        ```\n    \"\"\"\n    if api_base is not None:\n        kwargs['api_base'] = api_base\n\n    if botex_db is None: botex_db = os.environ.get('BOTEX_DB')\n    if api_key is None and 'openai_api_key' in kwargs: \n        api_key = kwargs.pop('openai_api_key')\n\n    kwargs['api_key'] = api_key\n    is_human = 0\n\n    conn = setup_botex_db(botex_db)\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        INSERT INTO participants (\n            session_name, session_id, participant_id, is_human, url) \n            VALUES (?, ?, ?, ?, ?) \n        \"\"\", (session_name, session_id, participant_id, is_human, url,)\n    )\n    conn.commit()\n    cursor.close()\n    if wait:\n        run_bot(\n            botex_db = botex_db, \n            session_id = session_id, \n            url = url, \n            model = model, \n            throttle = throttle, \n            full_conv_history = full_conv_history,\n            user_prompts = user_prompts,\n            **kwargs\n        )\n    else:\n        return Thread(\n            target = run_bot, \n            kwargs = dict(\n                botex_db = botex_db, \n                session_id = session_id, \n                url = url, \n                model = model, \n                throttle = throttle, \n                full_conv_history = full_conv_history,\n                user_prompts = user_prompts,\n                **kwargs\n            )\n        )\n</code></pre>"},{"location":"reference.html#export-data","title":"Export data","text":""},{"location":"reference.html#export_participant_data","title":"<code>export_participant_data</code>","text":"<p>Export the participants table from the botex database to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>The file path to save the CSV file.</p> required <code>botex_db</code> <code>str</code> <p>The file path to the botex sqlite3 file.  If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None (saves the CSV to the specified file path)</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def export_participant_data(csv_file, botex_db = None) -&gt; None:\n    \"\"\"\n    Export the participants table from the botex database to a CSV file.\n\n    Parameters:\n        csv_file (str): The file path to save the CSV file.\n        botex_db (str, optional): The file path to the botex sqlite3 file. \n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n\n    Returns:\n        None (saves the CSV to the specified file path)\n    \"\"\"\n    p = read_participants_from_botex_db(botex_db = botex_db)\n    with open(csv_file, 'w') as f:\n        w = csv.DictWriter(f, p[0].keys())\n        w.writeheader()\n        w.writerows(p)\n</code></pre>"},{"location":"reference.html#export_response_data","title":"<code>export_response_data</code>","text":"<p>Export the responses parsed from the bot conversations in the botex database to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>The file path to save the CSV file.</p> required <code>botex_db</code> <code>str</code> <p>The file path to the botex sqlite3 file.  If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None (saves the CSV to the specified file path)</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def export_response_data(csv_file, botex_db = None, session_id = None) -&gt; None:\n    \"\"\"\n    Export the responses parsed from the bot conversations in the botex\n    database to a CSV file.\n\n    Parameters:\n        csv_file (str): The file path to save the CSV file.\n        botex_db (str, optional): The file path to the botex sqlite3 file. \n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n        session_id (str, optional): A session ID to filter the results.\n\n    Returns:\n        None (saves the CSV to the specified file path)\n    \"\"\"\n\n    r = read_responses_from_botex_db(botex_db = botex_db, session_id = session_id)\n    with open(csv_file, 'w') as f:\n        w = csv.DictWriter(f, r[0].keys())\n        w.writeheader()\n        w.writerows(r)\n</code></pre>"},{"location":"reference.html#read_participants_from_botex_db","title":"<code>read_participants_from_botex_db</code>","text":"<p>Read the participants table from the botex database.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of a SQLite database file. If not  provided, it will try to read the file name from the environment  variable BOTEX_DB.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of dictionaries with participant data.</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def read_participants_from_botex_db(session_id = None, botex_db = None) -&gt; List[Dict]:\n    \"\"\"\n    Read the participants table from the botex database.\n\n    Args:\n        session_id (str, optional): A session ID to filter the results.\n        botex_db (str, optional): The name of a SQLite database file. If not \n            provided, it will try to read the file name from the environment \n            variable BOTEX_DB.\n\n    Returns:\n        A list of dictionaries with participant data.\n    \"\"\"\n\n    if botex_db is None: botex_db = environ.get('BOTEX_DB')\n    conn = sqlite3.connect(botex_db)\n    conn.row_factory = sqlite3.Row \n    cursor = conn.cursor()\n    if session_id:\n        cursor.execute(\n            \"SELECT * FROM participants WHERE session_id = ?\", (session_id,)\n        )\n    else:\n        cursor.execute(\"SELECT * FROM participants\")\n    sessions = [dict(row) for row in cursor.fetchall()]\n    cursor.close()\n    conn.close()\n    return sessions\n</code></pre>"},{"location":"reference.html#read_conversations_from_botex_db","title":"<code>read_conversations_from_botex_db</code>","text":"<p>Reads the conversations table from the botex database.  The conversation table contains the messages exchanged  with the LLM underlying the bot.</p> <p>Parameters:</p> Name Type Description Default <code>participant_id</code> <code>str</code> <p>A Participant ID to filter the results.</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of a SQLite database file. If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of dictionaries with the conversation data.</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def read_conversations_from_botex_db(\n        participant_id = None, botex_db = None, session_id = None\n    ) -&gt; List[Dict]:\n    \"\"\"\n    Reads the conversations table from the botex database. \n    The conversation table contains the messages exchanged \n    with the LLM underlying the bot.\n\n    Args:\n        participant_id (str, optional): A Participant ID to filter the results.\n        botex_db (str, optional): The name of a SQLite database file.\n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n        session_id (str, optional): A session ID to filter the results.\n\n    Returns:\n        A list of dictionaries with the conversation data.\n    \"\"\"\n    if botex_db is None: botex_db = environ.get('BOTEX_DB')\n    conn = sqlite3.connect(botex_db)\n    conn.row_factory = sqlite3.Row \n    cursor = conn.cursor()\n    if participant_id:\n        cursor.execute(\n            \"SELECT * FROM conversations WHERE id = ?\", (participant_id,)\n        )\n    else:\n        cursor.execute(\"SELECT * FROM conversations\")\n    conversations = [dict(row) for row in cursor.fetchall()]\n    if session_id:\n        conversations = [\n            c for c in conversations \n            if json.loads(c['bot_parms'])['session_id'] == session_id\n        ]\n    cursor.close()\n    conn.close()\n    return conversations\n</code></pre>"},{"location":"reference.html#read_responses_from_botex_db","title":"<code>read_responses_from_botex_db</code>","text":"<p>Extracts the responses and their rationales from the botex conversation data. </p> <p>Parameters:</p> Name Type Description Default <code>botex_db</code> <code>str</code> <p>The name of a SQLite database file. If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of dictionaries with the rationale data.</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def read_responses_from_botex_db(botex_db = None, session_id = None) -&gt; List[Dict]:\n    \"\"\"\n    Extracts the responses and their rationales from the botex conversation data. \n\n    Parameters:\n        botex_db (str, optional): The name of a SQLite database file.\n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n        session_id (str, optional): A session ID to filter the results.\n\n    Returns:\n        A list of dictionaries with the rationale data.\n    \"\"\"\n\n    cs = read_conversations_from_botex_db(botex_db = botex_db, session_id = session_id)\n    resp = [parse_conversation(c) for c in cs]\n    rt = []\n    for r in resp:\n        for answer_dict in r['answers']:\n            for id_, a in answer_dict.items():\n                if id_ == 'round': continue\n                rt.append({\n                    'session_id': r['session_id'], \n                    'participant_id': r['participant_id'], \n                    'round': answer_dict['round'], \n                    'question_id': id_, \n                    'answer': a['answer'], \n                    'reason': a['reason']\n                })\n    return rt\n</code></pre>"},{"location":"reference.html#export_otree_data","title":"<code>export_otree_data</code>","text":"<p>Export wide data from an oTree server.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>Path to the CSV file where the data should be stored.</p> required <code>server_url</code> <code>str</code> <p>URL of the oTree server. If None  (the default), the function will try to use the oTree server URL  from the environment variable OTREE_SERVER_URL.</p> <code>None</code> <code>admin_name</code> <code>str</code> <p>Admin username. Defaults to \"admin\".</p> <code>'admin'</code> <code>admin_password</code> <code>str</code> <p>Admin password. If None (the default), the function will try to use the oTree admin password from the  environment variable OTREE_ADMIN_PASSWORD.</p> <code>None</code> <code>time_out</code> <code>int</code> <p>Timeout in seconds to wait for the download.  Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the download does not succeed within the timeout.</p> <p>Returns     None (data is stored in the CSV file).</p> Detail <p>The function uses Selenium and a headless Chrome browser to download  the CSV file. Ideally, it would use an oTree API endpoint instead.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def export_otree_data(\n        csv_file: str,\n        server_url: str | None = None, \n        admin_name: str | None = \"admin\", \n        admin_password: str | None = None,\n        time_out: int | None = 10\n    ) -&gt; None:\n    \"\"\"\n    Export wide data from an oTree server.\n\n    Args:\n        csv_file (str): Path to the CSV file where the data should be stored.\n        server_url (str, optional): URL of the oTree server. If None \n            (the default), the function will try to use the oTree server URL \n            from the environment variable OTREE_SERVER_URL.\n        admin_name (str, optional): Admin username. Defaults to \"admin\".\n        admin_password (str, optional): Admin password. If None (the default),\n            the function will try to use the oTree admin password from the \n            environment variable OTREE_ADMIN_PASSWORD.\n        time_out (int, optional): Timeout in seconds to wait for the download. \n            Defaults to 10.\n\n    Raises:\n        Exception: If the download does not succeed within the timeout.\n\n    Returns\n        None (data is stored in the CSV file).\n\n    Detail:\n        The function uses Selenium and a headless Chrome browser to download \n        the CSV file. Ideally, it would use an oTree API endpoint instead.\n    \"\"\"\n\n    chrome_options = Options()\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--disable-gpu\")\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        prefs = {\"download.default_directory\": tmp_dir}\n        chrome_options.add_experimental_option(\"prefs\", prefs)\n        driver = webdriver.Chrome(options=chrome_options)\n        if server_url is None:\n            server_url = os.getenv(\"OTREE_SERVER_URL\")\n        if admin_password is None:\n            admin_password = os.getenv(\"OTREE_ADMIN_PASSWORD\")\n\n        export_url = f\"{server_url}/export\"\n        driver.get(export_url)\n        current_url = driver.current_url\n        if \"login\" in current_url:\n            driver.find_element(By.ID, \"id_username\").send_keys(admin_name)\n            driver.find_element(By.ID, \"id_password\").send_keys(admin_password)\n            submit_button = WebDriverWait(driver, 10).until(\n                EC.element_to_be_clickable((By.ID, \"btn-login\"))\n            )\n            submit_button.click()\n            WebDriverWait(driver, 10).until(EC.url_changes(current_url))\n            driver.get(export_url)\n\n        download_link = WebDriverWait(driver, 10).until(\n            EC.element_to_be_clickable((By.ID, \"wide-csv\"))\n        )\n        download_link.click()\n\n        time_out = time.time() + time_out\n        while True:            \n            time.sleep(1)\n            csv_files = [f for f in os.listdir(tmp_dir) if f.endswith(\".csv\")]\n            if len(csv_files) == 1:\n                os.rename(f\"{tmp_dir}/{csv_files[0]}\", csv_file)\n                logger.info(\"oTree CSV file downloaded.\")\n                break\n            else:\n                if time.time() &gt; time_out:\n                    logger.error(\"oTree CSV file download failed.\")\n                    break\n        driver.quit()\n</code></pre>"},{"location":"reference.html#normalize_otree_data","title":"<code>normalize_otree_data</code>","text":"<p>Normalize oTree data from wide to long format, then reshape it into a set  of list-of-dicts structures. Optionally save it to a set of CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>otree_csv_file</code> <code>str</code> <p>Path to a wide multi app oTree CSV file.</p> required <code>var_dict</code> <code>dict</code> <p>A dict to customize the exported data. See detail section.</p> <code>{'participant': {'code': 'participant_code', 'time_started_utc': 'time_started_utc', '_current_app_name': 'current_app', '_current_page_name': 'current_page'}, 'session': {'code': 'session_code'}}</code> <code>store_as_csv</code> <code>bool</code> <p>Whether to store the normalized data as CSV files. Defaults to False.</p> <code>False</code> <code>data_exp_path</code> <code>str</code> <p>Path to the folder where the normalized CSV files should be stored. Defaults to '.' (current folder).</p> <code>'.'</code> <code>exp_prefix</code> <code>str</code> <p>Prefix to be used for the CSV file names.  Defaults to '' (no prefix).</p> <code>''</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict whose keys are table names (e.g. 'session', 'participant',  'myapp_group', 'myapp_player', etc.) and whose values are lists of  dictionaries (i.e., row data).</p> Additional details <p>The var_dict parameter is a dictionary that allows to customize the exported data. The keys of the dictionary are the names of the oTree apps. The values are dictionaries that map the original column names to the desired column names. The keys of these inner dictionaries are the original column names and the values are the desired column names. All variables that are not included in the dict are omitted from the output. The 'participant' and 'session' keys are reserved for the participant  and session data, respectively.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def normalize_otree_data(\n    otree_csv_file: str, \n    var_dict: dict | None = {\n        'participant': {\n            'code': 'participant_code', \n            'time_started_utc': 'time_started_utc',\n            '_current_app_name': 'current_app', \n            '_current_page_name': 'current_page',\n        },\n        'session': {\n            'code': 'session_code'\n        }\n    },\n    store_as_csv: bool = False,\n    data_exp_path: str | None = '.', \n    exp_prefix: str | None = '',\n) -&gt; dict:\n    \"\"\"\n    Normalize oTree data from wide to long format, then reshape it into a set \n    of list-of-dicts structures. Optionally save it to a set of CSV files.\n\n    Args:\n        otree_csv_file (str): Path to a wide multi app oTree CSV file.\n        var_dict (dict, optional): A dict to customize the exported data. See\n            detail section.\n        store_as_csv (bool, optional): Whether to store the normalized data as\n            CSV files. Defaults to False.\n        data_exp_path (str, optional): Path to the folder where the normalized\n            CSV files should be stored. Defaults to '.' (current folder).\n        exp_prefix (str, optional): Prefix to be used for the CSV file names. \n            Defaults to '' (no prefix).\n\n    Returns:\n        A dict whose keys are table names (e.g. 'session', 'participant', \n            'myapp_group', 'myapp_player', etc.) and whose values are lists of \n            dictionaries (i.e., row data).\n\n    ??? tip \"Additional details\"\n        The var_dict parameter is a dictionary that allows to customize the\n        exported data. The keys of the dictionary are the names of the oTree\n        apps. The values are dictionaries that map the original column names to\n        the desired column names. The keys of these inner dictionaries are the\n        original column names and the values are the desired column names. All\n        variables that are not included in the dict are omitted from the output.\n        The 'participant' and 'session' keys are reserved for the participant \n        and session data, respectively.\n    \"\"\"\n\n    # The function is based on the naming conventions of oTree CSV files.\n    # oTree uses multi-level headers in the CSV file, where each level is\n    # separated by a dot. \n\n    # The general flow of the function is as follows:\n    # 1) Read the CSV file and extract the multi-level headers.\n    # 2) Pivot the data to long format by flattening wide columns into rows of   \n    #     [observation, level_1..4, value].\n    # 3) Extract participant and session data.\n    # 4) Separate the remaining long data by app and sub-level, pivoting each\n    #    resulting data set back into wide format and add the appropriate\n    #    keys from participant and session data.\n    #    - subsession (should be empty since subsessions seem to be equal to \n    #      rounds, tbc)\n    #    - group (merge with session data on participant_code to create key, \n    #      only keep if it contains data)\n    #    - player (rename id_in_group to player_id)  \n    # 5) Optionally store the data as CSV files.\n\n    # --------------------------------------------------------------------------\n    # Helper functions\n    # --------------------------------------------------------------------------\n\n    def extract_data(var, stacked_data, var_dict):\n        relevant = [\n            d for d in stacked_data\n            if d['level_1'] == var and d['level_2'] in var_dict[var].keys()\n        ]\n        # Remap level_2 column names\n        for item in relevant:\n            item['level_2'] = var_dict[var][item['level_2']]\n\n        # Build pivot result as dict {observation -&gt; one row dict}\n        pivoted = {}\n        for item in relevant:\n            obs = item['observation']\n            col = item['level_2']\n            val = item['value']\n            if obs not in pivoted:\n                pivoted[obs] = {'observation': obs}\n            # Only keep first occurrence if duplicates exist\n            if col not in pivoted[obs]:\n                pivoted[obs][col] = val\n\n        return list(pivoted.values())\n\n    def index_to_participant_code(data_list, obs_to_pcode):\n        out = []\n        for row in data_list:\n            obs = row['observation']\n            new_row = dict(row)\n            new_row['participant_code'] = obs_to_pcode.get(obs, None)\n            del new_row['observation']\n            out.append(new_row)\n        return out\n\n    def try_convert_number(x):\n        if x is None:\n            return None\n        # Already a number? (in case we call this multiple times)\n        if isinstance(x, (int, float)):\n            return x\n        # Attempt int -&gt; float -&gt; fallback str\n        try:\n            return int(x)\n        except (ValueError, TypeError):\n            pass\n        try:\n            return float(x)\n        except (ValueError, TypeError):\n            pass\n        return str(x)\n\n    def convert_columns(data_list):\n        for row in data_list:\n            for k, v in row.items():\n                if k not in (\"observation\",):  # do not convert observation index\n                    row[k] = try_convert_number(v)\n        return data_list\n\n    def remove_all_empty_columns(rows):\n        keys = rows[0].keys() if rows else []\n        has_nonempty = {col: False for col in keys}\n        for r in rows:\n            for col in has_nonempty:\n                if r.get(col, None) not in (None, ''):\n                    has_nonempty[col] = True\n        for r in rows:\n            for col, is_nonempty in has_nonempty.items():\n                if not is_nonempty:\n                    r.pop(col)\n        return rows\n\n    def unify_dict_keys(rows):\n        seen_keys = []\n        for d in rows:\n            for k in d.keys():\n                if k not in seen_keys:\n                    seen_keys.append(k)\n        new_rows = []\n        for d in rows:\n            new_d = {}\n            for k in seen_keys:\n                new_d[k] = d.get(k, '') \n            new_rows.append(new_d)\n        return new_rows\n\n    def reorder_columns(data_list, first_cols):\n        out = []\n        for row in data_list:\n            new_row = {}\n            # keep track of which keys got placed\n            placed = set()\n            # place the \"first_cols\" in order\n            for c in first_cols:\n                if c in row:\n                    new_row[c] = row[c]\n                    placed.add(c)\n            # place remaining\n            for c in row:\n                if c not in placed:\n                    new_row[c] = row[c]\n            out.append(new_row)\n        return out\n\n\n    def write_dicts_to_csv(dict_rows, file_path):\n        if not dict_rows:\n            # If empty, write just an empty file or possibly only headers\n            with open(file_path, 'w', newline='', encoding='utf-8') as f:\n                pass\n            return\n        fieldnames = list(dict_rows[0].keys())\n        with open(file_path, 'w', newline='', encoding='utf-8') as f:\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n            writer.writeheader()\n            writer.writerows(dict_rows)\n\n\n    # --------------------------------------------------------------------------\n    # Main code\n    # --------------------------------------------------------------------------\n\n    # --- 1) Read the CSV file and extract the multi-level headers -------------\n\n    with open(otree_csv_file, 'r', encoding='utf-8-sig') as f:\n        reader = csv.reader(f)\n        all_rows = list(reader)\n    if not all_rows:\n        raise ValueError(f\"CSV file {otree_csv_file} is empty or invalid.\")\n\n    headers = all_rows[0]\n    data_rows = all_rows[1:]\n    multi_headers = [tuple(h.split('.')) for h in headers]\n\n\n    # --- 2) Pivot the data to long format -------------------------------------\n\n    processed_rows = []\n    for row_idx, row in enumerate(data_rows):\n        row_dict = {}\n        for col_idx, val in enumerate(row):\n            key_tuple = list(multi_headers[col_idx])\n            while len(key_tuple) &lt; 4:\n                key_tuple.append(None)\n            row_dict[tuple(key_tuple)] = val\n        processed_rows.append(row_dict)\n\n    stacked_data = []\n    for obs_idx, row_dict in enumerate(processed_rows):\n        for (lvl1, lvl2, lvl3, lvl4), val in row_dict.items():\n            stacked_data.append({\n                'observation': obs_idx,\n                'level_1': lvl1,\n                'level_2': lvl2,\n                'level_3': lvl3,\n                'level_4': lvl4,\n                'value': val\n            })\n\n    all_level1 = set(d['level_1'] for d in stacked_data if d['level_1'])\n    all_level1 = all_level1 - {'participant', 'session'}\n    apps = sorted(list(all_level1))\n\n    # Ensure var_dict has entries for each discovered app (even if empty).\n    for app in apps:\n        if app not in var_dict:\n            var_dict[app] = {}\n\n\n    # --- 3) Extract participant and session data ------------------------------\n\n    participant_data = extract_data('participant', stacked_data, var_dict)\n    participant_data = convert_columns(participant_data)\n\n    # Build a map: obs -&gt; participant_code\n    obs_to_participant_code = {}\n    for row in participant_data:\n        # We expect 'participant_code' in these rows\n        if 'participant_code' in row:\n            obs_to_participant_code[row['observation']] = row['participant_code']\n\n    session_data = extract_data('session', stacked_data, var_dict)\n    session_data = convert_columns(session_data)\n    session_data = index_to_participant_code(session_data, obs_to_participant_code)\n\n    for row in participant_data: row.pop('observation')\n    participant_data = reorder_columns(participant_data, ['participant_code'])\n\n    final_tables = {\n        'participant': participant_data,\n        'session': session_data,\n    }\n\n    # --- 4) Separate the remaining long data by app and sub-level -------------\n\n    # For convenience, gather all unique level_3 for each (level_1 == app).\n    # Typically level_3 is 'subsession', 'group', 'player', etc.\n    for app in apps:\n        logger.info(f\"Normalize data for oTree app: '{app}'\")\n        app_level_3 = sorted(set(\n            d['level_3'] for d in stacked_data if d['level_1'] == app and d['level_3']\n        ))\n\n        for group_name in app_level_3:\n            # Filter data for this app &amp; group\n            relevant = [\n                d for d in stacked_data\n                if d['level_1'] == app and d['level_3'] == group_name\n            ]\n            if not relevant:\n                continue\n\n            pivoted = {}\n            for item in relevant:\n                obs = item['observation']\n                try:\n                    rnd = int(item['level_2'])  # round number\n                except (ValueError, TypeError):\n                    rnd = None\n                if item['value'] not in (None, ''):\n                    col = item['level_4']\n                    val = item['value']\n                    key = (obs, rnd)\n\n                    if key not in pivoted:\n                        pivoted[key] = {\n                            'observation': obs,\n                            'round': rnd\n                        }\n                    if col and col not in pivoted[key]:\n                        pivoted[key][col] = val\n\n            pivoted_list = unify_dict_keys(list(pivoted.values()))\n            out_df_rows = convert_columns(pivoted_list)\n            out_df_rows = index_to_participant_code(\n                out_df_rows, obs_to_participant_code\n            )\n            table_name = f\"{app}_{group_name}\"  \n\n            if group_name == 'subsession':\n                # This code assumes that subsesions are equal to rounds.\n                # This implies that round_number' and 'round' columns should \n                # match and that the resulting data should have 3 columns.\n                col_names = out_df_rows[0].keys() if out_df_rows else []\n                if len(col_names) != 3:\n                    logger.error(\n                        f\"Error {app} data extraction: app seems to contain \"\n                        \"more subsessions than rounds or subsession level data.\"\n                    )\n                    raise ValueError(\n                        f\"Error in {app} data extraction: app seems to contain \"\n                        \"more subsessions than rounds.\"\n                    )\n                for row in out_df_rows:\n                    if 'round_number' in row and row['round_number'] != row['round']:\n                        logger.error(\n                            f\"Error {app} data extraction: subsession round_number \"\n                            \"does not match inferred round.\"\n                        )\n                        raise ValueError(\n                            f\"Error in {app} data extraction: subsession round_number \"\n                            \"does not match inferred round.\"\n                        )\n                # No data in subsession, so skip storing\n                continue\n\n            elif group_name == 'group':\n                # group data might or might not have data\n                # (only if there's more than one group or if there are \n                # group-level variables)\n\n                sess_index = {}\n                for srow in session_data:\n                    pcode = srow.get('participant_code', None)\n                    sess_index[pcode] = srow\n\n                merged_group = []\n                for row in out_df_rows:\n                    pcode = row.get('participant_code', None)\n                    newrow = dict(row)\n                    if pcode in sess_index:\n                        # merge session data into newrow\n                        for k, v in sess_index[pcode].items():\n                            # do not overwrite if we already have something\n                            if k not in ('participant_code', 'observation', 'round'):\n                                newrow[k] = v\n                    merged_group.append(newrow)\n                out_df_rows = merged_group\n\n                # Check if 'id_in_subsession' is all 1 - meaning there is only \n                # one group per session\n                all_id1 = True\n                for row in out_df_rows:\n                    if row.get('id_in_subsession') != 1:\n                        all_id1 = False\n                        break\n\n                if all_id1:\n                    # drop ['id_in_subsession', 'participant_code']\n                    cleaned = []\n                    for row in out_df_rows:\n                        newrow = dict(row)\n                        newrow.pop('id_in_subsession', None)\n                        newrow.pop('participant_code', None)\n                        cleaned.append(newrow)\n                    out_df_rows = cleaned\n\n                    # If after dropping columns we are left with only 2 columns \n                    # (session_code, round), there is no group level data\n                    if out_df_rows:\n                        col_count = len(out_df_rows[0])\n                        if col_count == 2:\n                            # skip storing\n                            continue\n                else:\n                    # rename id_in_subsession -&gt; group_id\n                    # build a map of group_id to participant_code\n                    cleaned = []\n                    group_participant_map = []\n                    for row in out_df_rows:\n                        newrow = dict(row)\n                        newmap = dict()\n                        newrow['group_id'] = newrow.pop('id_in_subsession')\n                        newmap['participant_code'] = newrow.pop('participant_code')\n                        newmap['round'] = newrow['round']\n                        newmap['group_id'] = newrow['group_id']\n                        cleaned.append(newrow)\n                        group_participant_map.append(newmap)\n                    out_df_rows = cleaned\n\n                # reorder columns\n                col_order = ['session_code', 'round']\n                if not all_id1: col_order.append('group_id')\n                out_df_rows = reorder_columns(out_df_rows, col_order)\n\n                # Deduplicate\n                unique_rows = []\n                seen = set()\n                for row in out_df_rows:\n                    # convert to a tuple of items\n                    row_tup = tuple(sorted(row.items()))\n                    if row_tup not in seen:\n                        seen.add(row_tup)\n                        unique_rows.append(row)\n                out_df_rows = unique_rows\n\n                final_tables[table_name] = out_df_rows\n\n            elif group_name == 'player':\n                # rename id_in_group -&gt; player_id\n                # and merge with group_participant_map if it exists\n                cleaned = []\n                for row in out_df_rows:\n                    newrow = dict(row)\n                    newrow['player_id'] = newrow.pop('id_in_group')\n                    if not all_id1:\n                        for m in group_participant_map:\n                            if  m['round'] == newrow['round'] and \\\n                                m['participant_code'] == newrow['participant_code']:\n                                newrow['group_id'] = m['group_id']\n                                break\n                    cleaned.append(newrow)\n                out_df_rows = cleaned\n                # reorder\n                if not all_id1:\n                    col_order = ['participant_code', 'round', 'group_id', 'player_id']\n                else:\n                    col_order = ['participant_code', 'round', 'player_id']\n                out_df_rows = reorder_columns(\n                    out_df_rows, col_order\n                )\n                final_tables[table_name] = out_df_rows\n\n            else:\n                # This should not happen - but if it does, store the data as is\n                logger.warning(\n                    f\"Unrecognized level name '{group_name}' in app '{app}'.\"\n                )\n                final_tables[table_name] = out_df_rows\n\n    # delete app names from table names if there is only one app\n    if len(apps) == 1:\n        for table_name in final_tables.keys():\n                if table_name.startswith(apps[0] + '_'):\n                    final_tables[table_name.split('_')[-1]] = \\\n                        final_tables.pop(table_name)\n\n\n    # --- 5) Optionally store as CSV -------------------------------------------\n\n    if store_as_csv:\n        if exp_prefix: exp_prefix += '_'\n        for table_name, rows in final_tables.items():\n            out_csv = os.path.join(\n                data_exp_path, f\"{exp_prefix}{table_name}.csv\"\n            )\n            write_dicts_to_csv(rows, out_csv)\n\n    logger.info(f\"Normalizing data completed\")\n    return final_tables\n</code></pre>"},{"location":"tutorials.html","title":"Tutorials","text":""}]}