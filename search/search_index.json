{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"botex: Using LLMs as Experimental Participants in oTree","text":""},{"location":"index.html#overview","title":"Overview","text":"<p>Welcome to botex, a new Python package that leverages the power of large language models (LLMs) as participants in oTree experiments. </p> <p>Botex takes a novel approach to integrating LLMs into behavioral experiments. Rather than relying on predefined prompts,<sup>1</sup> botex bots dynamically interact with their experimental environment by scraping their respective oTree participant pages. This approach allows them to infer the experimental flow solely from the webpage's textual content. By aligning bot behavior directly with the experimental interface, botex eliminates potential discrepancies between human and bot designs. This not only opens up exciting opportunities to explore LLM behavior but also positions LLMs as a powerful tool for developing and pre-testing experiments intended for human participants.</p> <p> </p>"},{"location":"index.html#why-choose-botex","title":"Why Choose Botex?","text":"<p>botex's innovative approach offers several advantages:</p> <ul> <li>Alignment: By scraping oTree pages, LLMs respond to the same interface as human participants, ensuring consistency in experimental design.</li> <li>Pre-testing: LLMs can act as intelligent pre-testers, providing valuable feedback during the design phase of human-centric experiments.</li> <li>Behavioral Insights: Explore how LLMs interact and respond under experimental conditions designed for humans.</li> </ul>"},{"location":"index.html#current-capabilities-and-limitations","title":"Current Capabilities and Limitations","text":"<p>While botex provides robust functionality for standard oTree forms, its reliance on web scraping introduces certain constraints:</p> <ul> <li>Standardized oTree Designs: The oTree framework\u2019s rigidity ensures compatibility, but customized HTML forms may require adjustments.</li> <li>Future Enhancements: We aim to extend support to custom HTML forms. However, some degree of standardization by users will likely be necessary for seamless integration.</li> </ul> <p>See Getting Started for a quick start guide.</p>"},{"location":"index.html#usable-llms","title":"Usable LLMs","text":"<p>For interfacing with LLMs, botex offers two options</p> <ul> <li>litellm: Allows the use of various commercial LLMs</li> <li>llama.cpp: Allows the use of local (open source) LLMs  </li> </ul> <p>The model that you use for inference has to support structured outputs. We have tested botex with the following LLMs:</p> Vendor Model Link Status Notes OpenAI gpt-4o-2024-08-06 and later OpenAI API OK Requires at least paid user tier 1 OpenAI gpt-4o-mini-2024-07-18 and later OpenAI API OK Requires at least paid user tier 1 Google gemini/gemini-1.5-flash-8b Google AI Studio OK 1,500 requests per day are free Google gemini/gemini-1.5-flash Google AI Studio OK 1,500 requests per day are free Google gemini/gemini-1.5-pro Google AI Studio OK 50 requests per day are free (not usable for larger experiments in the free tier) Open Source llama-3.1-8b-instruct-Q8_0.gguf Hugging Face OK Run with llama.cpp Open Source Mistral-7B-Instruct-v0.3.Q4_K_M.gguf Hugging Face OK Run with llama.cpp Open Source qwen2.5-7b-instruct-q4_k_m.gguf Hugging Face OK Run with llama.cpp <p>If you have success running botex with other models, please let us know so that we can add them to the list.</p>"},{"location":"index.html#paper","title":"Paper","text":"<p>If you use botex in your research, please cite its accompanying paper:</p> <p>Fikir Worku Edossa, Joachim Gassen, and Victor S. Maas (2024): Using Large Language Models to Explore Contextualization Effects in Economics-Based Accounting Experiments. Working Paper. SSRN.</p> <ol> <li> <p>See, for example, Grossmann, Engel and Ockenfels (paper, repo)\u00a0\u21a9</p> </li> </ol>"},{"location":"getting_started.html","title":"Getting Started","text":""},{"location":"getting_started.html#requirements","title":"Requirements","text":"<p>If you want to use botex to create LLM participants for your own oTree experiments, you need the following:</p> <ul> <li>A working python environment &gt;= 3.10, pip and preferably a virtual environment.</li> <li>Google Chrome for scraping the oTree participant pages.</li> </ul>"},{"location":"getting_started.html#install-botex","title":"Install botex","text":"<p>Then, activate the virtual environment and install the current development version of the package in it:</p> <pre><code>pip install git+https://github.com/joachim-gassen/botex.git\n</code></pre>"},{"location":"getting_started.html#install-otree","title":"Install oTree","text":"<p>The easiest way to get botex up and running is to use its command line interface. It guides you through the process to start botex on a running oTree instance. To set oTree up, you can run the following in the same virtual environment that you set up botex in:</p> <pre><code>pip install otree #you might observe a pip dependency error, but you can ignore it\notree startproject otree # Say yes for sample games\ncd otree \notree devserver\n</code></pre>"},{"location":"getting_started.html#using-the-botex-command-line-interface","title":"Using the botex Command Line Interface","text":"<p>Now, you have a locally running oTree sever. Switch to another terminal with the same virtual environment activated. Then start the botex command line interface by running <code>botex</code>. You should see the following output:</p> <pre><code>(.venv) user@host:~/github/project$ botex\n\nBotex database file not provided. Defaulting to 'botex.sqlite3'\noTree server URL not provided. Defaulting to 'http://localhost:8000'\nNo LLM provided. Enter your model string here (\"llamacpp\" if you are using\nllama.cpp) or press enter to accept the default [gemini/gemini-1.5-flash]: \n</code></pre> <p>Assuming that you are fine with using the Google Gemini model, after pressing Enter you need to provide an API key. If you do not have one yet, you can get a free one from the Google AI Studio. After entering the key, you can select an oTree experiment from the examples offered by the fresh oTree installation:</p> <pre><code>Enter the API key for your LLM model (for the Gemini model,\nyou can get a free API key at https://aistudio.google.com/apikey): ***\n\nAvailable session configurations:\n1: guess_two_thirds\n2: survey\nSelect a configuration by number: \n</code></pre> <p>We suggest that you choose the Guess two Thirds game.</p> <pre><code>Select a configuration by number: 1\n\nSelected session configuration: guess_two_thirds\nEnter number of human participants [0]: \n</code></pre> <p>While this is up to you, we suggest that you play along, so select one human participant.</p> <pre><code>Selected session configuration: guess_two_thirds\nEnter number of human participants [0]: 1\nNumber of participants: 3\nNumber of human participants: 1\nSession 'd8nqycbk' initialized\nHuman URLs: ['http://localhost:8000/InitializeParticipant/z0z34nrg']\nYou can monitor its progress at http://localhost:8000/SessionMonitor/d8nqycbk\nStarting bots on session...\n</code></pre>"},{"location":"getting_started.html#running-the-experiment","title":"Running the Experiment","text":"<p>The botex bots will now start to scrape their oTree participant pages and play the game. You can monitor their progress by opening the session monitor URL in your browser. You should see something like this:</p> <p></p> <p>You see, the bots already made their first move and wait for you ('P2'). Time to act. Open the human URL in your browser and play the game. You will see the bots' moves in the session monitor as you progress. </p>"},{"location":"getting_started.html#exporting-botex-data","title":"Exporting botex Data","text":"<p>After you and the bots have completed the game, you can download the oTree data by using the oTree web frontend just like you would for a 'normal' experiment. In addition, the app asks whether you want to export the botex data. We suggest that you do, providing a file name to save the LLM response data in the current working directory.</p> <pre><code>Session complete.\nYou can view the oTree data at http://localhost:8000/SessionData/d8nqycbk\nDownload the oTree data at: http://localhost:8000/export\nEnter CSV file name to export botex data to or press Enter to skip: two_thirds.csv\n</code></pre> <p>With this, the botex app terminates. You have run your first oTree experiment with LLM participants. Congratulations!</p> <p>Take a look at the resulting CSV file. It contains the responses from the two bot instances and the reasoning for their answers. The data should look something like this:</p> session_id participant_id round question_id answer reason d8nqycbk tojkfg7i 1 id_guess 66 \"I will start by guessing 2/3 of 50, which is approximately 33. However, I expect others to do the same, so I'll choose a slightly higher number hoping to be closer to 2/3 of the average.\" d8nqycbk tojkfg7i 2 id_guess 30 \"I reasoned that many players will start with a guess of 0, assuming everyone is rational. But then many will increase their number, so I choose a number slightly above 0 to try and outsmart my competitors. The previous round's 2/3 average was 36. I'll try a much lower number this time.\" d8nqycbk tojkfg7i 3 id_guess 10 \"Given the previous rounds' results and the tendency for players to adjust their guesses, I expect most guesses to cluster around 0. Therefore, I'm choosing a low number to increase my chances of winning.\" d8nqycbk czkz2bbj 1 id_guess 66 \"I am choosing a number slightly below 67, since the other participants would probably consider this iterative reasoning and would bid even lower.\" d8nqycbk czkz2bbj 2 id_guess 20 \"In the first round, the average was close to 60. If everyone assumes everyone else is rational, then the average will continue to decrease towards 0. However, since the number must be between 0 and 100, there is a lower bound to the guess. This time I will guess 20.\" d8nqycbk czkz2bbj 3 id_guess 10 \"Given the previous rounds, it's likely that participants will continue to reduce their guesses. I am choosing a very low number to try and win, acknowledging that if everyone does this, the average will be very low.\" <p>You see that, in this run, our LLM bots were not particularly smart in the first round but then quickly adapted their strategies in the later rounds.</p>"},{"location":"getting_started.html#next-steps","title":"Next Steps","text":"<p>If you are interested in learning how to use the Python API of botex in your own code, we suggest that you continue with the tutorial Run an oTree experiment with botex. If you are interested in a more advanced use case and how different LLMs perform in the Guess Two Thirds game, you can thereafter continue with the tutorial Using botex to benchmark LLM performance in oTree experiments.</p>"},{"location":"reference.html","title":"API Reference","text":"<p>This section details the botex API. It consists of the command line interface  <code>botex</code> and a set of Python functions to set up and run oTree-based experiments using LLMs as bots.</p> <p>The Python API can be divided into three main parts:</p> <ol> <li>Setup: Functions to set up the botex configuration and to start/stop a local llama.cpp server if needed.</li> <li>oTree Interface: Functions to interact with the oTree server, such as starting/stopping the server, reading session config data from it, initializing sessions, and running bots on sessions.</li> <li>Export data: Functions to export data from the botex and oTree databases.</li> </ol>"},{"location":"reference.html#command-line-interface","title":"Command Line Interface","text":"<p>The <code>botex</code> command line interface provides the option to set up and run oTree experiments with bots from the command line. It also allows the user to export data from both, the botex and the oTree databases.</p>"},{"location":"reference.html#botex","title":"botex","text":"<p>Run an oTree session with botex. All necessary arguments can be provided either as command line arguments, in the environment file referenced by the <code>-c</code> argument, or as environment variables.</p> <p>Usage:</p> <pre><code>botex [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --config TEXT            Path to the environment file\n                               containing the botex configuration.\n                               Defaults to 'botex.env'.\n\n  -i, --ignore                 Ignore any environment variables and\n                               config files.\n\n  -b, --botex-db TEXT          Path to the botex SQLite database\n                               file (will be created if it does not\n                               exist). Read from environment\n                               variable BOTEX_DB if not provided.\n                               Defaults to 'botex.sqlite3'.\n\n  -u, --otree-server-url TEXT  oTree server URL. Read from\n                               environment variable OTREE_SERVER_URL\n                               if not provided. Defaults to\n                               'http://localhost:8000'.\n\n  -r, --otree-rest-key TEXT    oTree secret key for its REST API.\n                               Read from environment variable\n                               OTREE_REST_KEY if not provided. Only\n                               required if the oTree server is\n                               running in DEMO or STUDY mode.\n\n  -m, --model TEXT             Path to the LLM model to use for\n                               botex. Read from environment variable\n                               LLM_MODEL if not provided. If\n                               environment variable is not set, you\n                               will be prompted for the model.\n\n  -k, --api-key TEXT           API key for the LLM model. Read from\n                               environment variable API_KEY if not\n                               provided. If environment variable is\n                               not set, you will be prompted to\n                               enter the key.\n\n  -a, --api-base TEXT          Base URL for the LLM model. If not\n                               provided it will default to None for\n                               LiteLLM and http://localhost:8080 for\n                               llama.cpp.\n\n  --llamacpp-server TEXT       Path to the llama.cpp server\n                               executable. Required if the model is\n                               'llamacpp'. Read from environment\n                               variable LLAMACPP_SERVER_PATH if not\n                               provided.\n\n  --llamacpp-local-llm TEXT    Path to the local llama.cpp model.\n                               Required if the model is 'llamacpp'.\n                               Read from environment variable\n                               LLAMACPP_LOCAL_LLM_PATH if not\n                               provided.\n\n  -s, --session-config TEXT    oTree session config to run. If not\n                               provided, and also not set by the\n                               environment variable\n                               OTREE_SESSION_CONFIG, you will be\n                               prompted to select from the available\n                               session configurations.\n\n  -p, --nparticipants INTEGER  Number of participants to run the\n                               session with. Read from environment\n                               variable OTREE_NPARTICIPANTS if not\n                               provided.\n\n  -n, --nhumans INTEGER        Number of human participants to\n                               include in the session. Read from\n                               environment variable OTREE_NHUMANS if\n                               not provided.\n\n  -e, --export-csv-file TEXT   CSV file to export botex data to. If\n                               not provided, you will be prompted to\n                               enter a file name after the session\n                               is complete.\n\n  -x, --no-throttle            Disables throttling requests to deal\n                               with rate limiting. Defaults to\n                               False. If set to True, you might run\n                               into rate limiting resulting in\n                               failed bot runs.\n\n  -v, --verbose                Print out botex logs while running.\n                               Defaults to False.\n\n  --help                       Show this message and exit.\n</code></pre>"},{"location":"reference.html#python-api-setup","title":"Python API: Setup","text":"<p>The botex configuration can be provided via function parameters or by setting environment variables. The latter is useful for secrets like API keys and also makes handling the API easier if you run repeated experiments. This an be facilitated by calling the function <code>load_botex_env()</code> that reads an <code>.env</code> file (<code>botex.env</code> by default). For users that want to use local LLMs for inference, <code>botex</code>can also start and/or stop a local llama.cpp instance.</p>"},{"location":"reference.html#load_botex_env","title":"<code>load_botex_env</code>","text":"<p>Load botex environment variables from a file.</p> <p>Parameters:</p> Name Type Description Default <code>env_file</code> <code>str</code> <p>The path to the .env file containing the botex configuration. Defaults to \"botex.env\".</p> <code>'botex.env'</code> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>True if at least one botex environment variable was set.</p> Source code in <code>src/botex/env.py</code> <pre><code>def load_botex_env(env_file = \"botex.env\") -&gt; bool:\n    \"\"\"\n    Load botex environment variables from a file.\n\n    Args:\n        env_file (str, optional): The path to the .env file containing\n            the botex configuration. Defaults to \"botex.env\".\n\n    Returns:\n        Bool: True if at least one botex environment variable was set.\n    \"\"\"\n    if not os.path.exists(env_file):\n        logger.warning(\n            f\"Could not read any botex environment variables from '{env_file}' \"\n            \"as the file does not exist. \"\n            \"Please make sure that the file is in the right location and that \"\n            \"it sets the botex environment variables that you need.\"\n        )\n        return False\n    success = load_dotenv(env_file)\n    if success:\n        logger.info(f\"Loaded botex environment variables from '{env_file}'\")\n    else:\n        logger.info(\n            f\"Botex environment variables parsed from '{env_file}'. \"\n            \"No new environment variables were set.\"\n        )\n    return success\n</code></pre>"},{"location":"reference.html#start_llamacpp_server","title":"<code>start_llamacpp_server</code>","text":"<p>Starts a llama.cpp server instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict | None</code> <p>A dict containing to the least the keys <code>llamacpp_server_path</code> (the path to the llama.cpp  executable) and <code>local_llm_path</code> (the path to the LLM model that you want llama.cpp to use). If None (the default), then these parameters are read from the environment variables <code>LLAMACPP_SERVER_PATH</code> and  <code>LLAMACPP_LOCAL_LLM_PATH</code>. See notes below for additional configuration parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>Popen</code> <p>The process of the running llama.cpp sever if start was successful.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the server is already running or if starting the server  fails.            </p> Additional details <p>You can provide other configuration parameters for the  local model in the model configuration dictionary. These include:</p> <pre><code>-   `server_url` (str): The base URL for the llama.cpp \n    server, defaults to `\"http://localhost:8080\"`.\n\n-   `context_length` (int): The context length for the model. \n    If `None`, BotEx will try to get the context length from the \n    local model metadata; if that is not possible, it defaults \n    to `4096`.\n\n-   `number_of_layers_to_offload_to_gpu` (int): The number of \n    layers to offload to the GPU, defaults to `0`.\n\n- ` temperature` (float): The temperature for the model, \n    defaults to `0.5`.\n\n-   `maximum_tokens_to_predict` (int): The maximum number of \n    tokens to predict, defaults to `10000`.\n\n-   `top_p` (float): The top-p value for the model, \n    defaults to `0.9`.\n\n-   `top_k` (int): The top-k value for the model, \n    defaults to `40`.\n\n-   `num_slots` (int): The number of slots for the model, \n    defaults to `1`.\n</code></pre> <p>For all these keys, if not provided in the configuration dictionary,  botex will try to get the value from environment variables (in all  capital letters, prefixed by LLAMACPP_); if that is not possible, it  will use the default value.</p> Example <pre><code>from botex.llamacpp import start_llamacpp_server\n\nconfig = {\n    \"server_path\": \"/path/to/llama.cpp\",\n    \"local_llm_path\": \"/path/to/local/model\",\n    \"server_url\": \"http://localhost:8080\",\n    \"context_length\": 4096,\n    \"number_of_layers_to_offload_to_gpu\": 0,\n    \"temperature\": 0.8,\n    \"maximum_tokens_to_predict\": 10000,\n    \"top_p\": 0.9,\n    \"top_k\": 40,\n    \"num_slots\": 1\n}\n</code></pre> Source code in <code>src/botex/llamacpp.py</code> <pre><code>def start_llamacpp_server(config: dict | None = None) -&gt; subprocess.Popen:\n    \"\"\"\n    Starts a llama.cpp server instance.\n\n    Args:\n        config (dict | None, optional): A dict containing to the least\n            the keys `llamacpp_server_path` (the path to the llama.cpp \n            executable) and `local_llm_path` (the path to the LLM model that you\n            want llama.cpp to use). If None (the default), then these parameters\n            are read from the environment variables `LLAMACPP_SERVER_PATH` and \n            `LLAMACPP_LOCAL_LLM_PATH`. See notes below for additional\n            configuration parameters\n\n    Returns:\n        The process of the running llama.cpp sever if start was\n            successful.\n\n    Raises:\n        Exception: If the server is already running or if starting the server \n            fails.            \n\n    ??? tip \"Additional details\"\n        You can provide other configuration parameters for the \n        local model in the model configuration dictionary. These include:\n\n            -   `server_url` (str): The base URL for the llama.cpp \n                server, defaults to `\"http://localhost:8080\"`.\n\n            -   `context_length` (int): The context length for the model. \n                If `None`, BotEx will try to get the context length from the \n                local model metadata; if that is not possible, it defaults \n                to `4096`.\n\n            -   `number_of_layers_to_offload_to_gpu` (int): The number of \n                layers to offload to the GPU, defaults to `0`.\n\n            - ` temperature` (float): The temperature for the model, \n                defaults to `0.5`.\n\n            -   `maximum_tokens_to_predict` (int): The maximum number of \n                tokens to predict, defaults to `10000`.\n\n            -   `top_p` (float): The top-p value for the model, \n                defaults to `0.9`.\n\n            -   `top_k` (int): The top-k value for the model, \n                defaults to `40`.\n\n            -   `num_slots` (int): The number of slots for the model, \n                defaults to `1`.\n\n\n        For all these keys, if not provided in the configuration dictionary, \n        botex will try to get the value from environment variables (in all \n        capital letters, prefixed by LLAMACPP_); if that is not possible, it \n        will use the default value.\n\n\n    ??? example \"Example\"\n        ```python\n        from botex.llamacpp import start_llamacpp_server\n\n        config = {\n            \"server_path\": \"/path/to/llama.cpp\",\n            \"local_llm_path\": \"/path/to/local/model\",\n            \"server_url\": \"http://localhost:8080\",\n            \"context_length\": 4096,\n            \"number_of_layers_to_offload_to_gpu\": 0,\n            \"temperature\": 0.8,\n            \"maximum_tokens_to_predict\": 10000,\n            \"top_p\": 0.9,\n            \"top_k\": 40,\n            \"num_slots\": 1\n        }\n        ```\n    \"\"\"\n    manager = LlamaCppServerManager(config)\n    return manager.start_server()\n</code></pre>"},{"location":"reference.html#stop_llamacpp_server","title":"<code>stop_llamacpp_server</code>","text":"<p>Stops a running llama.cpp server instance.</p> <p>Parameters:</p> Name Type Description Default <code>process</code> <code>Popen</code> <p>The process of the running llama.cpp server.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None (stops the running llama.cpp server)</p> Source code in <code>src/botex/llamacpp.py</code> <pre><code>def stop_llamacpp_server(process: subprocess.Popen) -&gt; None:\n    \"\"\"\n    Stops a running llama.cpp server instance.\n\n    Args:\n        process (subprocess.Popen): The process of the running llama.cpp server.\n\n    Returns:\n        None (stops the running llama.cpp server)\n    \"\"\"\n    LlamaCppServerManager.stop_server(process)\n</code></pre>"},{"location":"reference.html#python-api-otree-interface","title":"Python API: oTree Interface","text":"<p>Running experiments with botex requires an oTree server with an active session to be accessible. The following functions allow the user to interact with the oTree server, such as starting/stopping the server, reading session config data from it, and initializing sessions. Once a session is initialized, the core functions <code>run_bots_on_session()</code> and <code>run_single_bot()</code> can be used to run bots on the session.</p>"},{"location":"reference.html#start_otree_server","title":"<code>start_otree_server</code>","text":"<p>Start an oTree server in a subprocess.</p> <p>Parameters:</p> Name Type Description Default <code>project_path</code> <code>str</code> <p>Path to your oTree project folder. If None (the default), it will be obtained from the environment variable OTREE_PROJECT_PATH.</p> <code>None</code> <code>port</code> <code>int</code> <p>The port to run the server on. If None  (the default), it will first try to read from the  environment variable OTREE_PORT. It that is not set. it  will default to 8000.</p> <code>None</code> <code>log_file</code> <code>str</code> <p>Path to the log file. If None  (the default), it will first be tried to read from the environment variable OTREE_LOG_FILE. If that is not set, it will default to 'otree.log'.</p> <code>None</code> <code>auth_level</code> <code>str</code> <p>The authentication level for the oTree  server. It is set by environment variable OTREE_AUTH_LEVEL.  The default is None, which will leave this environment variable unchanged. if you use 'DEMO' or 'STUDY', the environment variable will be set accordingly and you need to provide a rest key in the argument 'rest_key' below.</p> <code>None</code> <code>rest_key</code> <code>str</code> <p>The API key for the oTree server. If None (the default), it will be obtained from the environment variable OTREE_REST_KEY.</p> <code>None</code> <code>admin_password</code> <code>str</code> <p>The admin password for the oTree server. For this to work, <code>settings.py</code> in the oTree project needs to read the password from the environment variable OTREE_ADMIN_PASSWORD (which is normally the case).</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds to wait for the  server. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>Popen</code> <p>A subprocess object.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the oTree server does not start within the timeout.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def start_otree_server(\n        project_path = None,\n        port = None, \n        log_file = None,\n        auth_level = None, \n        rest_key = None,\n        admin_password = None,\n        timeout = 5\n    ) -&gt; subprocess.Popen:\n    \"\"\"\n    Start an oTree server in a subprocess.\n\n    Args:\n        project_path (str, optional): Path to your oTree project folder.\n            If None (the default), it will be obtained from the environment\n            variable OTREE_PROJECT_PATH.\n        port (int, optional): The port to run the server on. If None \n            (the default), it will first try to read from the \n            environment variable OTREE_PORT. It that is not set. it \n            will default to 8000.\n        log_file (str, optional): Path to the log file. If None \n            (the default), it will first be tried to read from the\n            environment variable OTREE_LOG_FILE. If that is not set,\n            it will default to 'otree.log'.\n        auth_level (str, optional): The authentication level for the oTree \n            server. It is set by environment variable OTREE_AUTH_LEVEL. \n            The default is None, which will leave this environment variable\n            unchanged. if you use 'DEMO' or 'STUDY', the environment variable\n            will be set accordingly and you need to provide a rest key\n            in the argument 'rest_key' below.\n        rest_key (str, optional): The API key for the oTree server.\n            If None (the default), it will be obtained from the environment\n            variable OTREE_REST_KEY.\n        admin_password (str, optional): The admin password for the oTree server.\n            For this to work, `settings.py` in the oTree project needs to read\n            the password from the environment variable OTREE_ADMIN_PASSWORD\n            (which is normally the case).\n        timeout (int, optional): Timeout in seconds to wait for the \n            server. Defaults to 5.\n\n    Returns:\n        A subprocess object.\n\n    Raises:\n        Exception: If the oTree server does not start within the timeout.\n    \"\"\"\n    if project_path is None:\n        project_path = os.environ.get('OTREE_PROJECT_PATH')\n        if project_path is None:\n            raise Exception('No oTree project path provided.')\n    if port is None: port = os.environ.get('OTREE_PORT', 8000)\n    if log_file is None: log_file = os.environ.get('OTREE_LOG_FILE', 'otree.log')\n    otree_log = open(log_file, 'w')\n    if auth_level is not None: \n        os.environ['OTREE_AUTH_LEVEL'] = auth_level\n        if rest_key is not None:\n            os.environ['OTREE_REST_KEY'] = rest_key\n        if admin_password is not None:\n            os.environ['OTREE_ADMIN_PASSWORD'] = admin_password \n\n\n    if platform.system() == \"Windows\":\n        otree_server = subprocess.Popen(\n            [\"otree\", \"devserver\", str(port)], cwd=project_path,\n            stderr=otree_log, stdout=otree_log,\n            creationflags=subprocess.CREATE_NEW_PROCESS_GROUP\n        )\n    else:\n        otree_server = subprocess.Popen(\n            [\"otree\", \"devserver\", str(port)], cwd=project_path,\n            stderr=otree_log, stdout=otree_log\n        )\n    otree_server_url = f'http://localhost:{port}'\n    os.environ['OTREE_SERVER_URL'] = otree_server_url\n\n    # Access oTree API to check if server is running\n    time_out = time.time() + timeout\n    while True:\n        if otree_server_is_running(rest_key = rest_key):\n            logger.info(\n                \"oTree server started successfully \"\n                f\"with endpoint '{otree_server_url}'\"\n            )\n            break\n        else:\n            if time.time() &gt; time_out:\n                logger.error(\n                    f\"oTree endpoint '{otree_server_url}' did not respond \"\n                    f\"within {timeout} seconds. Exiting.\"\n                )\n                raise Exception('oTree server did not start.')\n            time.sleep(1)\n    return otree_server\n</code></pre>"},{"location":"reference.html#stop_otree_server","title":"<code>stop_otree_server</code>","text":"<p>Stop an oTree server subprocess.</p> <p>Parameters:</p> Name Type Description Default <code>otree_server</code> <code>subprocess</code> <p>The subprocess object to be terminated.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The return code of the oTree subprocess</p> Source code in <code>src/botex/otree.py</code> <pre><code>def stop_otree_server(otree_server: subprocess.Popen) -&gt; int:\n    \"\"\"\n    Stop an oTree server subprocess.\n\n    Args:\n        otree_server (subprocess): The subprocess object to be terminated.\n\n    Returns:\n        The return code of the oTree subprocess\n    \"\"\"\n    otree_running = otree_server.poll() is None\n    if otree_running: \n        proc = psutil.Process(otree_server.pid)\n        if platform.system() == \"Windows\":\n            proc.send_signal(signal.CTRL_BREAK_EVENT)\n            proc.wait()\n        else: \n            proc.children()[0].send_signal(signal.SIGKILL)\n            otree_server.kill()\n            otree_server.wait()\n        logger.info(\"oTree server stopped.\")\n    else:\n        logger.warning('oTree server already stopped.')\n    return otree_server.poll()\n</code></pre>"},{"location":"reference.html#otree_server_is_running","title":"<code>otree_server_is_running</code>","text":"<p>Check if an oTree server is running.</p> <p>Parameters:</p> Name Type Description Default <code>server_url</code> <code>str</code> <p>The URL of the oTree server. Read from environment variable OTREE_SERVER_URL if None (the default).</p> <code>None</code> <code>rest_key</code> <code>str</code> <p>The API key for the oTree server. Read from environment variable OTREE_REST_KEY if None (the default).</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the server is running, False otherwise.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def otree_server_is_running(server_url = None, rest_key = None) -&gt; bool:\n    \"\"\"\n    Check if an oTree server is running.\n\n    Args:\n        server_url (str): The URL of the oTree server. Read from environment\n            variable OTREE_SERVER_URL if None (the default).\n        rest_key (str): The API key for the oTree server. Read from environment\n            variable OTREE_REST_KEY if None (the default).\n\n    Returns:\n        True if the server is running, False otherwise.\n    \"\"\"\n    try: \n        data = call_otree_api(\n            requests.get, 'otree_version', \n            otree_server_url=server_url, otree_rest_key=rest_key\n        )\n    except:\n        data = {'error': \"No API response\"}\n    return 'version' in data\n</code></pre>"},{"location":"reference.html#get_session_configs","title":"<code>get_session_configs</code>","text":"<p>Get the session configurations from an oTree server.</p> <p>Parameters:</p> Name Type Description Default <code>otree_server_url</code> <code>str</code> <p>The URL of the oTree server. Read from  environment variable OTREE_SERVER_URL if None (the default).</p> <code>None</code> <code>otree_rest_key</code> <code>str</code> <p>The API key for the oTree server. Read from  environment variable OTREE_REST_KEY if None (the default).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The session configurations.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def get_session_configs(\n        otree_server_url: str | None = None,\n        otree_rest_key: str | None = None\n    ) -&gt; dict:\n    \"\"\"\n    Get the session configurations from an oTree server.\n\n    Args:\n        otree_server_url (str): The URL of the oTree server. Read from \n            environment variable OTREE_SERVER_URL if None (the default).\n        otree_rest_key (str): The API key for the oTree server. Read from \n            environment variable OTREE_REST_KEY if None (the default).\n\n    Returns:\n        The session configurations.\n    \"\"\"\n\n    return call_otree_api(\n        requests.get, 'session_configs', \n        otree_server_url=otree_server_url, otree_rest_key=otree_rest_key\n    )\n</code></pre>"},{"location":"reference.html#init_otree_session","title":"<code>init_otree_session</code>","text":"<p>Initialize an oTree session with a given number of participants.</p> <p>Parameters:</p> Name Type Description Default <code>config_name</code> <code>str</code> <p>The name of the oTree session configuration.</p> required <code>npart</code> <code>int</code> <p>The total number of participants.</p> required <code>nhumans</code> <code>int</code> <p>The number of human participants (defaults to zero. Provide either nhumans or is_human, but not both.</p> <code>0</code> <code>is_human</code> <code>list</code> <p>A list of booleans indicating whether each participant  is human. Needs to be the same length as npart. If None (the  default), humans (if present) will be randomly assigned.</p> <code>None</code> <code>room_name</code> <code>str</code> <p>The name of the oTree room for the session. If None  (the default), no room will be used.</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file to store BotEx    data. If None (the default), it will be obtained from the  environment variable BOTEX_DB. If the database does not exist, it  will be created.</p> <code>None</code> <code>otree_server_url</code> <code>str</code> <p>The URL of the oTree server. If None (the  default), it will be obtained from the environment variable  OTREE_SERVER_URL.</p> <code>None</code> <code>otree_rest_key</code> <code>str</code> <p>The API key for the oTree server. If None (the  default), it will be obtained from the environment variable  OTREE_REST_KEY.</p> <code>None</code> <code>modified_session_config_fields</code> <code>dict</code> <p>A dictionary of fields to modify  in the the oTree session config. Default is None. </p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict with the keys 'session_id', 'participant_code', 'is_human',  'bot_urls', and 'human_urls' containing the session ID, participant  codes, human indicators, and the URLs for the human and bot  participants.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def init_otree_session(\n        config_name: str,\n        npart: int,\n        nhumans: int = 0, \n        is_human: List[bool] | None = None,\n        room_name: str | None = None,\n        botex_db: str | None = None,\n        otree_server_url: str | None = None,\n        otree_rest_key: str | None = None,\n        modified_session_config_fields: dict | None = None,\n    ) -&gt; dict:\n    \"\"\"\n    Initialize an oTree session with a given number of participants.\n\n    Args:\n        config_name (str): The name of the oTree session configuration.\n        npart (int): The total number of participants.\n        nhumans (int): The number of human participants (defaults to zero.\n            Provide either nhumans or is_human, but not both.\n        is_human (list): A list of booleans indicating whether each participant \n            is human. Needs to be the same length as npart. If None (the \n            default), humans (if present) will be randomly assigned.\n        room_name (str): The name of the oTree room for the session. If None \n            (the default), no room will be used.\n        botex_db (str): The name of the SQLite database file to store BotEx     \n            data. If None (the default), it will be obtained from the \n            environment variable BOTEX_DB. If the database does not exist, it \n            will be created.\n        otree_server_url (str): The URL of the oTree server. If None (the \n            default), it will be obtained from the environment variable \n            OTREE_SERVER_URL.\n        otree_rest_key (str): The API key for the oTree server. If None (the \n            default), it will be obtained from the environment variable \n            OTREE_REST_KEY.\n        modified_session_config_fields (dict): A dictionary of fields to modify \n            in the the oTree session config. Default is None. \n\n    Returns:\n        dict with the keys 'session_id', 'participant_code', 'is_human', \n            'bot_urls', and 'human_urls' containing the session ID, participant \n            codes, human indicators, and the URLs for the human and bot \n            participants.\n    \"\"\"\n\n    if nhumans &gt; 0 and is_human is not None: raise(Exception(\n        \"Provide either nhumans or is_human, but not both.\"\n    ))\n\n    if is_human is not None:\n        if len(is_human) != npart: raise(Exception(\n            \"Length of is_human must be the same as npart.\"\n        ))\n\n    if is_human is None and nhumans &gt; 0:\n        is_human = [True]*nhumans + [False]*(npart - nhumans)\n        shuffle(is_human)\n\n    if is_human is None and nhumans == 0: is_human = [False]*npart\n\n    if botex_db is None: botex_db = os.environ.get('BOTEX_DB')\n\n    if otree_server_url is None:\n        otree_server_url = os.environ.get('OTREE_SERVER_URL')\n\n    session_id = call_otree_api(\n        requests.post, 'sessions', \n        otree_server_url=otree_server_url, otree_rest_key=otree_rest_key, \n        session_config_name=config_name, \n        num_participants=npart, room_name=room_name,\n        modified_session_config_fields=modified_session_config_fields\n    )['code']\n    part_data = sorted(\n        call_otree_api(\n            requests.get, 'sessions', session_id,\n            otree_server_url=otree_server_url, otree_rest_key=otree_rest_key \n        )['participants'],\n        key=lambda d: d['id_in_session']\n    )\n    part_codes = [pd['code'] for pd in part_data]\n\n    base_url = otree_server_url + '/InitializeParticipant/'\n    urls = [base_url + pc for pc in part_codes]\n\n    rows = zip(\n        [config_name]*npart, [session_id]*npart, \n        part_codes, is_human, urls\n    )\n\n    conn = setup_botex_db(botex_db)\n    cursor = conn.cursor()\n    cursor.executemany(\n        \"\"\"\n        INSERT INTO participants (\n            session_name, session_id, participant_id, is_human, url) \n            VALUES (?, ?, ?, ?, ?) \n        \"\"\", rows\n    )\n    conn.commit()\n    cursor.close()\n    return {\n        'session_id': session_id, \n        'participant_code': part_codes,\n        'is_human': is_human,\n        'bot_urls': list(compress(urls, [not x for x in is_human])), \n        'human_urls': list(compress(urls, is_human))\n    }\n</code></pre>"},{"location":"reference.html#get_bot_urls","title":"<code>get_bot_urls</code>","text":"<p>Get the URLs for the bot participants in an oTree session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The ID of the oTree session.</p> required <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file to store BotEx  data. If None (the default), it will be obtained from the  environment variable BOTEX_DB.</p> <code>None</code> <code>already_started</code> <code>bool</code> <p>If True, the function will also run bots that  have already started but not yet finished. This is useful if bots  did not startup properly because of network issues. Default is  False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of URLs for the bot participants</p> Source code in <code>src/botex/otree.py</code> <pre><code>def get_bot_urls(\n        session_id: str,\n        botex_db: str | None = None,\n        already_started: bool = False\n    ) -&gt; List[str]:\n    \"\"\"\n    Get the URLs for the bot participants in an oTree session.\n\n    Args:\n        session_id (str): The ID of the oTree session.\n        botex_db (str): The name of the SQLite database file to store BotEx \n            data. If None (the default), it will be obtained from the \n            environment variable BOTEX_DB.\n        already_started (bool): If True, the function will also run bots that \n            have already started but not yet finished. This is useful if bots \n            did not startup properly because of network issues. Default is \n            False.\n\n    Returns:\n        List of URLs for the bot participants\n    \"\"\"\n\n    if botex_db is None: botex_db = os.environ.get('BOTEX_DB')\n    conn = sqlite3.connect(botex_db)\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT url,time_in,time_out FROM participants \n        WHERE session_id = ? AND is_human = 0\n        \"\"\", (session_id,)\n    )\n    if already_started:\n        urls = [row[0] for row in cursor.fetchall() if row[2] is None]\n    else:\n        urls = [row[0] for row in cursor.fetchall() if row[1] is None]\n    cursor.close()\n    conn.close()\n    return urls\n</code></pre>"},{"location":"reference.html#run_bots_on_session","title":"<code>run_bots_on_session</code>","text":"<p>Run botex bots on an oTree session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The ID of the oTree session.</p> required <code>bot_urls</code> <code>list</code> <p>A list of URLs for the bot participants. Will be retrieved from the database if None (the default).</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file for BotEx data. If None (the default), it will be obtained from the environment  variable BOTEX_DB.</p> <code>None</code> <code>model</code> <code>str</code> <p>The model to use for the bot. Default is  <code>gpt-4o-2024-08-06</code> from OpenAI vie LiteLLM. It needs to be a model  that supports structured outputs. For OpenAI, these are  gpt-4o-mini-2024-07-18 and later or gpt-4o-2024-08-06 and later. If  you use a commercial model, You need to provide an API key in the  parameter <code>api_key</code> and be prepared to pay to use this model.</p> <p>If you want to use local models, we suggest that you use llama.cpp,  In this case, set this string to <code>lamacpp</code> and set the URL of your  llama.cpp server in <code>api_base</code>. If you want botex to start the  llama.cpp server for you, run <code>start_llamacpp_sever()</code> prior to  running run_bots_on_session().</p> <code>'gpt-4o-2024-08-06'</code> <code>api_key</code> <code>str</code> <p>The API key for the model that you use. If None (the  default), it will be obtained from environment variables by liteLLM  (e.g., OPENAI_API_KEY or GEMINI_API_KEY). </p> <code>None</code> <code>api_base</code> <code>str</code> <p>The base URL for the llm server. Default is None not to interfere with the default LiteLLM behavior. If you want to use a  local model with llama.cpp and if you have not explicitly set this  parameter, it will default to <code>http://localhost:8080</code>, the default  url for the llama.cpp server.</p> <code>None</code> <code>throttle</code> <code>bool</code> <p>Whether to slow down the bot's requests. Slowing done  the requests can help to avoid rate limiting. Default is False. The  bot will switch to <code>throttle=True</code> when LiteLLM is used and  completion requests raise exceptions.</p> <code>False</code> <code>full_conv_history</code> <code>bool</code> <p>Whether to keep the full conversation history. This will increase token use and only work with very short  experiments. Default is False.</p> <code>False</code> <code>user_prompts</code> <code>dict</code> <p>A dictionary of user prompts to override the  default prompts that the bot uses. The keys should be one or more  of the following:</p> <p>[<code>start</code>, <code>analyze_first_page_no_q</code>, <code>analyze_first_page_q</code>,  <code>analyze_page_no_q</code>, <code>analyze_page_q</code>, <code>analyze_page_no_q_full_hist</code>, <code>analyze_page_q_full_hist</code>,  <code>page_not_changed</code>, <code>system</code>, <code>system_full_hist</code>, <code>resp_too_long</code>,  <code>json_error</code>, <code>end</code>, <code>end_full_hist</code>].</p> <p>If a key is not present in  the dictionary, the default prompt will  be used. If a key that is not in the default prompts is present in  the dictionary, then the bot will exit with a warning and not run  to make sure that the user is aware of the issue.</p> <code>None</code> <code>already_started</code> <code>bool</code> <p>If True, the function will also run bots that  have already started but not yet finished. This is useful if bots  did not startup properly because of network issues. Default is  False.</p> <code>False</code> <code>wait</code> <code>bool</code> <p>If True (the default), the function will wait for the bots  to finish.</p> <code>True</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass on to <code>litellm.completion()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None | List[Thread]</code> <p>None (bot conversation logs are stored in database) if wait is True. A list of Threads running the bots if wait is False.</p> Additional details <p>When running local models via llama.cpp, if you would like      botex to start the llama.cpp server for you,      run <code>start_llamacpp_server()</code> to start up the server prior to     running <code>run_bots_on_session()</code>.</p> Example Usage <ul> <li>Running botex with the default model (<code>gpt-4o-2024-08-06</code>)</li> </ul> <pre><code>run_bots_on_session(\n    session_id=\"your_session_id\",\n    botex_db=\"path/to/botex.sqlite3\",\n    api_key=\"your_openai_api_key\",\n    # Other parameters if and as needed\n)\n</code></pre> <ul> <li>Using a specific model supported by LiteLLM</li> </ul> <pre><code>run_bots_on_session(\n    session_id=\"your_session_id\",\n    botex_db=\"path/to/botex.sqlite3\",\n    model=\"gemini/gemini-1.5-flash\",\n    api_key=\"your_gemini_api_key\",\n    # Other parameters if and as needed\n)\n</code></pre> <ul> <li>Using a local model with BotEx starting the llama.cpp server</li> </ul> <pre><code>llamacpp_config = {\n    \"server_path\": \"/path/to/llama/server\",\n    \"local_llm_path\": \"/path/to/local/model\",\n    # Additional configuration parameters if and as needed\n}\nprocess_id = start_llamacpp_server(llamacpp_config)\nrun_bots_on_session(\n    session_id=\"your_session_id\",\n    botex_db=\"path/to/botex.sqlite3\",\n    model=\"llamacpp\",\n    # Other parameters if and as needed\n)\nstop_llamacpp_server(process_id)\n</code></pre> <ul> <li>Using a local model with an already running llama.cpp server that      uses a URL different from the default (if you are using the      default \"http://localhost:8080\", you can simply omit the <code>api_base</code>      parameter)</li> </ul> <pre><code>run_bots_on_session(\n    session_id=\"your_session_id\",\n    botex_db=\"path/to/botex.sqlite3\",\n    model = \"llamacpp\",\n    api_base = \"http://yourserver:port\"},\n    # Other parameters if and as needed\n)\n</code></pre> Source code in <code>src/botex/otree.py</code> <pre><code>def run_bots_on_session(\n        session_id: str,\n        bot_urls: List[str] | None = None, \n        botex_db: str | None = None, \n        model: str = \"gpt-4o-2024-08-06\",\n        api_key: str | None = None,\n        api_base: str | None = None,\n        throttle: bool = False,\n        full_conv_history: bool = False,\n        user_prompts: dict | None = None,\n        already_started: bool = False,\n        wait: bool = True,\n        **kwargs\n    ) -&gt; None | List[Thread]:\n    \"\"\"\n    Run botex bots on an oTree session.\n\n    Args:\n        session_id (str): The ID of the oTree session.\n        bot_urls (list): A list of URLs for the bot participants.\n            Will be retrieved from the database if None (the default).\n        botex_db (str): The name of the SQLite database file for BotEx data.\n            If None (the default), it will be obtained from the environment \n            variable BOTEX_DB.\n        model (str): The model to use for the bot. Default is \n            `gpt-4o-2024-08-06` from OpenAI vie LiteLLM. It needs to be a model \n            that supports structured outputs. For OpenAI, these are \n            gpt-4o-mini-2024-07-18 and later or gpt-4o-2024-08-06 and later. If \n            you use a commercial model, You need to provide an API key in the \n            parameter `api_key` and be prepared to pay to use this model.\n\n            If you want to use local models, we suggest that you use llama.cpp, \n            In this case, set this string to `lamacpp` and set the URL of your \n            llama.cpp server in `api_base`. If you want botex to start the  llama.cpp server for you, run `start_llamacpp_sever()` prior to \n            running run_bots_on_session().\n        api_key (str): The API key for the model that you use. If None (the \n            default), it will be obtained from environment variables by liteLLM \n            (e.g., OPENAI_API_KEY or GEMINI_API_KEY). \n        api_base (str): The base URL for the llm server. Default is None not to\n            interfere with the default LiteLLM behavior. If you want to use a \n            local model with llama.cpp and if you have not explicitly set this \n            parameter, it will default to `http://localhost:8080`, the default \n            url for the llama.cpp server.\n        throttle (bool): Whether to slow down the bot's requests. Slowing done \n            the requests can help to avoid rate limiting. Default is False. The \n            bot will switch to `throttle=True` when LiteLLM is used and \n            completion requests raise exceptions.\n        full_conv_history (bool): Whether to keep the full conversation history.\n            This will increase token use and only work with very short \n            experiments. Default is False.\n        user_prompts (dict): A dictionary of user prompts to override the \n            default prompts that the bot uses. The keys should be one or more \n            of the following:\n\n            [`start`, `analyze_first_page_no_q`, `analyze_first_page_q`, \n            `analyze_page_no_q`, `analyze_page_q`,\n            `analyze_page_no_q_full_hist`, `analyze_page_q_full_hist`, \n            `page_not_changed`, `system`, `system_full_hist`, `resp_too_long`, \n            `json_error`, `end`, `end_full_hist`].\n\n            If a key is not present in  the dictionary, the default prompt will \n            be used. If a key that is not in the default prompts is present in \n            the dictionary, then the bot will exit with a warning and not run \n            to make sure that the user is aware of the issue.\n        already_started (bool): If True, the function will also run bots that \n            have already started but not yet finished. This is useful if bots \n            did not startup properly because of network issues. Default is \n            False.\n        wait (bool): If True (the default), the function will wait for the bots \n            to finish.\n        kwargs (dict): Additional keyword arguments to pass on to\n            `litellm.completion()`.\n\n    Returns:\n        None (bot conversation logs are stored in database) if wait is True. A list of Threads running the bots if wait is False.\n\n    ??? tip \"Additional details\"\n\n        When running local models via llama.cpp, if you would like \n            botex to start the llama.cpp server for you, \n            run `start_llamacpp_server()` to start up the server prior to\n            running `run_bots_on_session()`.\n\n    ??? example \"Example Usage\"\n\n        - Running botex with the default model (`gpt-4o-2024-08-06`)\n\n        ```python\n        run_bots_on_session(\n            session_id=\"your_session_id\",\n            botex_db=\"path/to/botex.sqlite3\",\n            api_key=\"your_openai_api_key\",\n            # Other parameters if and as needed\n        )\n        ```\n\n        - Using a specific model supported by LiteLLM\n\n        ```python    \n        run_bots_on_session(\n            session_id=\"your_session_id\",\n            botex_db=\"path/to/botex.sqlite3\",\n            model=\"gemini/gemini-1.5-flash\",\n            api_key=\"your_gemini_api_key\",\n            # Other parameters if and as needed\n        )\n        ```\n\n        - Using a local model with BotEx starting the llama.cpp server\n\n        ```python\n        llamacpp_config = {\n            \"server_path\": \"/path/to/llama/server\",\n            \"local_llm_path\": \"/path/to/local/model\",\n            # Additional configuration parameters if and as needed\n        }\n        process_id = start_llamacpp_server(llamacpp_config)\n        run_bots_on_session(\n            session_id=\"your_session_id\",\n            botex_db=\"path/to/botex.sqlite3\",\n            model=\"llamacpp\",\n            # Other parameters if and as needed\n        )\n        stop_llamacpp_server(process_id)\n        ```\n\n        - Using a local model with an already running llama.cpp server that \n            uses a URL different from the default (if you are using the \n            default \"http://localhost:8080\", you can simply omit the `api_base` \n            parameter)\n\n        ```python\n        run_bots_on_session(\n            session_id=\"your_session_id\",\n            botex_db=\"path/to/botex.sqlite3\",\n            model = \"llamacpp\",\n            api_base = \"http://yourserver:port\"},\n            # Other parameters if and as needed\n        )\n        ```\n    \"\"\"\n    if botex_db is None: botex_db = os.environ.get('BOTEX_DB')\n    if api_key is None and 'openai_api_key' in kwargs: \n        api_key = kwargs.pop('openai_api_key')\n    if bot_urls is None: \n        bot_urls = get_bot_urls(session_id, botex_db, already_started)\n\n    otree_url = bot_urls[0].split('/InitializeParticipant/')[0]\n    logger.info(\n        f\"Running bots on session {session_id}. \"\n        f\"You can monitor the session at {otree_url}/SessionMonitor/{session_id}\"\n    )\n    thread_kwargs = {\n        'botex_db': botex_db, 'session_id': session_id, \n        'full_conv_history': full_conv_history, \n        'model': model, 'api_key': api_key,\n        'api_base': api_base,\n        'user_prompts': user_prompts,\n        'throttle': throttle\n    }\n    thread_kwargs.update(kwargs)\n    threads = [\n        Thread(\n            target = run_bot, \n            kwargs = dict(thread_kwargs, **{'url': url})\n        ) for url in bot_urls \n    ]\n    for t in threads: t.start()\n    if wait: \n        for t in threads: t.join()\n    else:\n        return threads\n</code></pre>"},{"location":"reference.html#run_single_bot","title":"<code>run_single_bot</code>","text":"<p>Runs a single botex bot manually.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The participant URL to start the bot on.</p> required <code>session_name</code> <code>str</code> <p>The name of the oTree session. Defaults to \"unknown\"</p> <code>'unknown'</code> <code>session_id</code> <code>str</code> <p>The oTree ID of the oTree session. Defaults to  \"unknown\".</p> <code>'unknown'</code> <code>participant_id</code> <code>str</code> <p>The oTree ID of the participant. Defaults to  \"unknown\".</p> <code>'unknown'</code> <code>botex_db</code> <code>str</code> <p>The name of the SQLite database file to store botex  data.</p> <code>None</code> <code>full_conv_history</code> <code>bool</code> <p>Whether to keep the full conversation history. This will increase token use and only work with very short  experiments. Default is False.</p> <code>False</code> <code>model</code> <code>str</code> <p>The model to use for the bot. Default is  <code>gpt-4o-2024-08-06</code> from OpenAI vie LiteLLM. It needs to be a model  that supports structured outputs. For OpenAI, these are  gpt-4o-mini-2024-07-18 and later or gpt-4o-2024-08-06 and later. If  you use a commercial model, You need to provide an API key in the  parameter <code>api_key</code> and be prepared to pay to use this model.</p> <p>If you want to use local models, we suggest that you use llama.cpp,  In this case, set this string to <code>lamacpp</code> and set the URL of your  llama.cpp server in <code>api_base</code>. If you want botex to start the  llama.cpp server for you, run <code>start_llamacpp_sever()</code> prior to  running run_bots_on_session().</p> <code>'gpt-4o-2024-08-06'</code> <code>api_key</code> <code>str</code> <p>The API key for the model that you use. If None (the  default), it will be obtained from environment variables by liteLLM  (e.g., OPENAI_API_KEY or GEMINI_API_KEY). </p> <code>None</code> <code>api_base</code> <code>str</code> <p>The base URL for the llm server. Default is None not to interfere with the default LiteLLM behavior. If you want to use a  local model with llama.cpp and if you have not explicitly set this  parameter, it will default to <code>http://localhost:8080</code>, the default  url for the llama.cpp server.</p> <code>None</code> <code>throttle</code> <code>bool</code> <p>Whether to slow down the bot's requests. Slowing done  the requests can help to avoid rate limiting. Default is False. The  bot will switch to <code>throttle=True</code> when LiteLLM is used and  completion requests raise exceptions.</p> <code>False</code> <code>full_conv_history</code> <code>bool</code> <p>Whether to keep the full conversation history. This will increase token use and only work with very short  experiments. Default is False.</p> <code>False</code> <code>user_prompts</code> <code>dict</code> <p>A dictionary of user prompts to override the  default prompts that the bot uses. The keys should be one or more  of the following:</p> <p>[<code>start</code>, <code>analyze_first_page_no_q</code>, <code>analyze_first_page_q</code>,  <code>analyze_page_no_q</code>, <code>analyze_page_q</code>, <code>analyze_page_no_q_full_hist</code>, <code>analyze_page_q_full_hist</code>,  <code>page_not_changed</code>, <code>system</code>, <code>system_full_hist</code>, <code>resp_too_long</code>,  <code>json_error</code>, <code>end</code>, <code>end_full_hist</code>].</p> <p>If a key is not present in  the dictionary, the default prompt will  be used. If a key that is not in the default prompts is present in  the dictionary, then the bot will exit with a warning and not run  to make sure that the user is aware of the issue.</p> <code>None</code> <code>wait</code> <code>bool</code> <p>If True (the default), the function will wait for the bots  to finish.</p> <code>True</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass on to <code>litellm.completion()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None | Thread</code> <p>None (conversation is stored in the botex database) if wait is True. The Thread running the bot if wait is False.</p> <p>Notes:</p> <ul> <li>When running local models via llama.cpp, if you would like      botex to start the llama.cpp server for you,      run <code>start_llamacpp_server()</code> to start up the server prior to     running <code>run_bots_on_session()</code>.</li> </ul> Example Usage <ul> <li>Using a model via LiteLLM</li> </ul> <pre><code>run_single_bot(\n    url=\"your_participant_url\",\n    session_name=\"your_session_name\",\n    session_id=\"your_session_id\",\n    participant_id=\"your_participant_id\",\n    botex_db=\"path/to/botex.sqlite3\",\n    model=\"a LiteLLM model string, e.g. 'gemini/gemini-1.5-flash'\",\n    api_key=\"the API key for your model provide\",\n    # Other parameters if and as needed\n)\n</code></pre> <ul> <li>Using a local model with an already running llama.cpp server</li> </ul> <pre><code>run_single_bot(\n    url=\"your_participant_url\",\n    session_name=\"your_session_name\",\n    session_id=\"your_session_id\",\n    participant_id=\"your_participant_id\",\n    botex_db=\"path/to/botex.sqlite3\",\n    model=\"llamacpp\",\n    api_base=\"http://yourhost:port\" # defaults to http://localhost:8080\n    # Other parameters if and as needed\n)\n</code></pre> <ul> <li>Using a local model with BotEx starting the llama.cpp server</li> </ul> <pre><code>llamacpp_config = {\n    \"server_path\": \"/path/to/llama/server\",\n    \"local_llm_path\": \"/path/to/local/model\",\n    # Additional configuration parameters if and as needed\n}\nprocess_id = start_llamacpp_server(llamacpp_config)\nrun_single_bot(\n    url=\"your_participant_url\",\n    session_name=\"your_session_name\",\n    session_id=\"your_session_id\",\n    participant_id=\"your_participant_id\",\n    botex_db=\"path/to/botex.sqlite3\",\n    model=\"llamacpp\",\n    # Other parameters if and as needed\n)\nstop_llamacpp_server(process_id)\n</code></pre> Source code in <code>src/botex/otree.py</code> <pre><code>def run_single_bot(\n    url: str,\n    session_name: str = \"unknown\",\n    session_id: str = \"unknown\", \n    participant_id: str = \"unknown\",\n    botex_db: str | None = None,\n    model: str = \"gpt-4o-2024-08-06\",\n    api_key: str | None = None,\n    api_base: str | None = None,\n    throttle: bool = False, \n    full_conv_history: bool = False,\n    user_prompts: dict | None = None,\n    wait: bool = True,\n    **kwargs\n) -&gt; None | Thread:\n    \"\"\"\n    Runs a single botex bot manually.\n\n    Args:\n        url (str): The participant URL to start the bot on.\n        session_name (str): The name of the oTree session. Defaults to \"unknown\"\n        session_id (str): The oTree ID of the oTree session. Defaults to \n            \"unknown\".\n        participant_id (str): The oTree ID of the participant. Defaults to \n            \"unknown\".\n        botex_db (str): The name of the SQLite database file to store botex \n            data.\n        full_conv_history (bool): Whether to keep the full conversation history.\n            This will increase token use and only work with very short \n            experiments. Default is False.\n        model (str): The model to use for the bot. Default is \n            `gpt-4o-2024-08-06` from OpenAI vie LiteLLM. It needs to be a model \n            that supports structured outputs. For OpenAI, these are \n            gpt-4o-mini-2024-07-18 and later or gpt-4o-2024-08-06 and later. If \n            you use a commercial model, You need to provide an API key in the \n            parameter `api_key` and be prepared to pay to use this model.\n\n            If you want to use local models, we suggest that you use llama.cpp, \n            In this case, set this string to `lamacpp` and set the URL of your \n            llama.cpp server in `api_base`. If you want botex to start the  llama.cpp server for you, run `start_llamacpp_sever()` prior to \n            running run_bots_on_session().\n        api_key (str): The API key for the model that you use. If None (the \n            default), it will be obtained from environment variables by liteLLM \n            (e.g., OPENAI_API_KEY or GEMINI_API_KEY). \n        api_base (str): The base URL for the llm server. Default is None not to\n            interfere with the default LiteLLM behavior. If you want to use a \n            local model with llama.cpp and if you have not explicitly set this \n            parameter, it will default to `http://localhost:8080`, the default \n            url for the llama.cpp server.\n        throttle (bool): Whether to slow down the bot's requests. Slowing done \n            the requests can help to avoid rate limiting. Default is False. The \n            bot will switch to `throttle=True` when LiteLLM is used and \n            completion requests raise exceptions.\n        full_conv_history (bool): Whether to keep the full conversation history.\n            This will increase token use and only work with very short \n            experiments. Default is False.\n        user_prompts (dict): A dictionary of user prompts to override the \n            default prompts that the bot uses. The keys should be one or more \n            of the following:\n\n            [`start`, `analyze_first_page_no_q`, `analyze_first_page_q`, \n            `analyze_page_no_q`, `analyze_page_q`,\n            `analyze_page_no_q_full_hist`, `analyze_page_q_full_hist`, \n            `page_not_changed`, `system`, `system_full_hist`, `resp_too_long`, \n            `json_error`, `end`, `end_full_hist`].\n\n            If a key is not present in  the dictionary, the default prompt will \n            be used. If a key that is not in the default prompts is present in \n            the dictionary, then the bot will exit with a warning and not run \n            to make sure that the user is aware of the issue.\n        wait (bool): If True (the default), the function will wait for the bots \n            to finish.\n        kwargs (dict): Additional keyword arguments to pass on to\n            `litellm.completion()`.\n\n    Returns:\n        None (conversation is stored in the botex database) if wait is True.\n            The Thread running the bot if wait is False.\n\n    Notes:\n\n    -   When running local models via llama.cpp, if you would like \n        botex to start the llama.cpp server for you, \n        run `start_llamacpp_server()` to start up the server prior to\n        running `run_bots_on_session()`.\n\n    ??? example \"Example Usage\"\n\n        - Using a model via LiteLLM\n\n        ```python\n        run_single_bot(\n            url=\"your_participant_url\",\n            session_name=\"your_session_name\",\n            session_id=\"your_session_id\",\n            participant_id=\"your_participant_id\",\n            botex_db=\"path/to/botex.sqlite3\",\n            model=\"a LiteLLM model string, e.g. 'gemini/gemini-1.5-flash'\",\n            api_key=\"the API key for your model provide\",\n            # Other parameters if and as needed\n        )\n        ```\n\n\n        - Using a local model with an already running llama.cpp server\n\n        ```python\n        run_single_bot(\n            url=\"your_participant_url\",\n            session_name=\"your_session_name\",\n            session_id=\"your_session_id\",\n            participant_id=\"your_participant_id\",\n            botex_db=\"path/to/botex.sqlite3\",\n            model=\"llamacpp\",\n            api_base=\"http://yourhost:port\" # defaults to http://localhost:8080\n            # Other parameters if and as needed\n        )\n        ```\n\n        - Using a local model with BotEx starting the llama.cpp server\n\n        ```python\n        llamacpp_config = {\n            \"server_path\": \"/path/to/llama/server\",\n            \"local_llm_path\": \"/path/to/local/model\",\n            # Additional configuration parameters if and as needed\n        }\n        process_id = start_llamacpp_server(llamacpp_config)\n        run_single_bot(\n            url=\"your_participant_url\",\n            session_name=\"your_session_name\",\n            session_id=\"your_session_id\",\n            participant_id=\"your_participant_id\",\n            botex_db=\"path/to/botex.sqlite3\",\n            model=\"llamacpp\",\n            # Other parameters if and as needed\n        )\n        stop_llamacpp_server(process_id)\n        ```\n    \"\"\"\n    if api_base is not None:\n        kwargs['api_base'] = api_base\n\n    if botex_db is None: botex_db = os.environ.get('BOTEX_DB')\n    if api_key is None and 'openai_api_key' in kwargs: \n        api_key = kwargs.pop('openai_api_key')\n\n    kwargs['api_key'] = api_key\n    is_human = 0\n\n    conn = setup_botex_db(botex_db)\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        INSERT INTO participants (\n            session_name, session_id, participant_id, is_human, url) \n            VALUES (?, ?, ?, ?, ?) \n        \"\"\", (session_name, session_id, participant_id, is_human, url,)\n    )\n    conn.commit()\n    cursor.close()\n    if wait:\n        run_bot(\n            botex_db = botex_db, \n            session_id = session_id, \n            url = url, \n            model = model, \n            throttle = throttle, \n            full_conv_history = full_conv_history,\n            user_prompts = user_prompts,\n            **kwargs\n        )\n    else:\n        return Thread(\n            target = run_bot, \n            kwargs = dict(\n                botex_db = botex_db, \n                session_id = session_id, \n                url = url, \n                model = model, \n                throttle = throttle, \n                full_conv_history = full_conv_history,\n                user_prompts = user_prompts,\n                **kwargs\n            )\n        )\n</code></pre>"},{"location":"reference.html#python-api-export-data","title":"Python API: Export data","text":"<p>Running oTree experiments with botex generates two data sources:</p> <ol> <li>The 'normal' experiment data that oTree collects.</li> <li>Additional data that botex collects, such as the prompting sequence between botex and the bots, as well as the answers and the reasoning behind the answers that the LLM bots provide.</li> </ol> <p>botex provides functions to export these data from the botex and oTree databases. In addition, the function <code>normalize_otree_data()</code> can be used to re-organize the wide multi-app oTree data into a normalized format that is more convenient for downstream use.</p>"},{"location":"reference.html#read_participants_from_botex_db","title":"<code>read_participants_from_botex_db</code>","text":"<p>Read the participants table from the botex database and return it as a list  of dicts.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of a SQLite database file. If not  provided, it will try to read the file name from the environment  variable BOTEX_DB.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of dictionaries with participant data.</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def read_participants_from_botex_db(session_id = None, botex_db = None) -&gt; List[Dict]:\n    \"\"\"\n    Read the participants table from the botex database and return it as a list \n    of dicts.\n\n    Args:\n        session_id (str, optional): A session ID to filter the results.\n        botex_db (str, optional): The name of a SQLite database file. If not \n            provided, it will try to read the file name from the environment \n            variable BOTEX_DB.\n\n    Returns:\n        A list of dictionaries with participant data.\n    \"\"\"\n\n    if botex_db is None: botex_db = environ.get('BOTEX_DB')\n    conn = sqlite3.connect(botex_db)\n    conn.row_factory = sqlite3.Row \n    cursor = conn.cursor()\n    if session_id:\n        cursor.execute(\n            \"SELECT * FROM participants WHERE session_id = ?\", (session_id,)\n        )\n    else:\n        cursor.execute(\"SELECT * FROM participants\")\n    sessions = [dict(row) for row in cursor.fetchall()]\n    cursor.close()\n    conn.close()\n    return sessions\n</code></pre>"},{"location":"reference.html#read_conversations_from_botex_db","title":"<code>read_conversations_from_botex_db</code>","text":"<p>Reads the conversations table from the botex database.  The conversation table contains the messages exchanged  with the LLM underlying the bot. Each conversation is returned as a dictionary containing a JSON string with the  message sequence.</p> <p>Parameters:</p> Name Type Description Default <code>participant_id</code> <code>str</code> <p>A Participant ID to filter the results.</p> <code>None</code> <code>botex_db</code> <code>str</code> <p>The name of a SQLite database file. If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of dictionaries with the conversation data.</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def read_conversations_from_botex_db(\n        participant_id = None, botex_db = None, session_id = None\n    ) -&gt; List[Dict]:\n    \"\"\"\n    Reads the conversations table from the botex database. \n    The conversation table contains the messages exchanged \n    with the LLM underlying the bot. Each conversation is\n    returned as a dictionary containing a JSON string with the \n    message sequence.\n\n    Args:\n        participant_id (str, optional): A Participant ID to filter the results.\n        botex_db (str, optional): The name of a SQLite database file.\n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n        session_id (str, optional): A session ID to filter the results.\n\n    Returns:\n        A list of dictionaries with the conversation data.\n    \"\"\"\n    if botex_db is None: botex_db = environ.get('BOTEX_DB')\n    conn = sqlite3.connect(botex_db)\n    conn.row_factory = sqlite3.Row \n    cursor = conn.cursor()\n    if participant_id:\n        cursor.execute(\n            \"SELECT * FROM conversations WHERE id = ?\", (participant_id,)\n        )\n    else:\n        cursor.execute(\"SELECT * FROM conversations\")\n    conversations = [dict(row) for row in cursor.fetchall()]\n    if session_id:\n        conversations = [\n            c for c in conversations \n            if json.loads(c['bot_parms'])['session_id'] == session_id\n        ]\n    cursor.close()\n    conn.close()\n    return conversations\n</code></pre>"},{"location":"reference.html#read_responses_from_botex_db","title":"<code>read_responses_from_botex_db</code>","text":"<p>Extracts the responses and their rationales from the botex conversation data and returns them as a list of dicts. </p> <p>Parameters:</p> Name Type Description Default <code>botex_db</code> <code>str</code> <p>The name of a SQLite database file. If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of dictionaries with the rationale data.</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def read_responses_from_botex_db(botex_db = None, session_id = None) -&gt; List[Dict]:\n    \"\"\"\n    Extracts the responses and their rationales from the botex conversation data\n    and returns them as a list of dicts. \n\n    Parameters:\n        botex_db (str, optional): The name of a SQLite database file.\n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n        session_id (str, optional): A session ID to filter the results.\n\n    Returns:\n        A list of dictionaries with the rationale data.\n    \"\"\"\n\n    cs = read_conversations_from_botex_db(botex_db = botex_db, session_id = session_id)\n    resp = [parse_conversation(c) for c in cs]\n    rt = []\n    for r in resp:\n        for answer_dict in r['answers']:\n            for id_, a in answer_dict.items():\n                if id_ == 'round': continue\n                rt.append({\n                    'session_id': r['session_id'], \n                    'participant_id': r['participant_id'], \n                    'round': answer_dict['round'], \n                    'question_id': id_, \n                    'answer': a['answer'], \n                    'reason': a['reason']\n                })\n    return rt\n</code></pre>"},{"location":"reference.html#export_participant_data","title":"<code>export_participant_data</code>","text":"<p>Export the participants table from the botex database, retrieved by calling <code>read_participants_from_botex_db()</code>, to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>The file path to save the CSV file.</p> required <code>botex_db</code> <code>str</code> <p>The file path to the botex sqlite3 file.  If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None (saves the CSV to the specified file path)</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def export_participant_data(csv_file, botex_db = None, session_id = None) -&gt; None:\n    \"\"\"\n    Export the participants table from the botex database, retrieved by calling\n    `read_participants_from_botex_db()`, to a CSV file.\n\n    Parameters:\n        csv_file (str): The file path to save the CSV file.\n        botex_db (str, optional): The file path to the botex sqlite3 file. \n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n        session_id (str, optional): A session ID to filter the results.\n\n    Returns:\n        None (saves the CSV to the specified file path)\n    \"\"\"\n    p = read_participants_from_botex_db(\n        session_id = session_id, botex_db = botex_db\n    )\n    with open(csv_file, 'w', newline='') as f:\n        w = csv.DictWriter(f, p[0].keys())\n        w.writeheader()\n        w.writerows(p)\n</code></pre>"},{"location":"reference.html#export_response_data","title":"<code>export_response_data</code>","text":"<p>Export the responses parsed from the bot conversations in the botex database, retrieved by calling <code>read_responses_from_botex_db()</code>,  to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>The file path to save the CSV file.</p> required <code>botex_db</code> <code>str</code> <p>The file path to the botex sqlite3 file.  If not provided, it will try to read the file name from the environment variable BOTEX_DB.</p> <code>None</code> <code>session_id</code> <code>str</code> <p>A session ID to filter the results.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None (saves the CSV to the specified file path)</p> Source code in <code>src/botex/botex_db.py</code> <pre><code>def export_response_data(csv_file, botex_db = None, session_id = None) -&gt; None:\n    \"\"\"\n    Export the responses parsed from the bot conversations in the botex\n    database, retrieved by calling `read_responses_from_botex_db()`, \n    to a CSV file.\n\n    Parameters:\n        csv_file (str): The file path to save the CSV file.\n        botex_db (str, optional): The file path to the botex sqlite3 file. \n            If not provided, it will try to read the file name from\n            the environment variable BOTEX_DB.\n        session_id (str, optional): A session ID to filter the results.\n\n    Returns:\n        None (saves the CSV to the specified file path)\n    \"\"\"\n\n    r = read_responses_from_botex_db(botex_db = botex_db, session_id = session_id)\n    with open(csv_file, 'w', newline='') as f:\n        w = csv.DictWriter(f, r[0].keys())\n        w.writeheader()\n        w.writerows(r)\n</code></pre>"},{"location":"reference.html#export_otree_data","title":"<code>export_otree_data</code>","text":"<p>Export wide data from an oTree server.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>Path to the CSV file where the data should be stored.</p> required <code>server_url</code> <code>str</code> <p>URL of the oTree server. If None  (the default), the function will try to use the oTree server URL  from the environment variable OTREE_SERVER_URL.</p> <code>None</code> <code>admin_name</code> <code>str</code> <p>Admin username. Defaults to \"admin\".</p> <code>'admin'</code> <code>admin_password</code> <code>str</code> <p>Admin password. If None (the default), the function will try to use the oTree admin password from the  environment variable OTREE_ADMIN_PASSWORD.</p> <code>None</code> <code>time_out</code> <code>int</code> <p>Timeout in seconds to wait for the download.  Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the download does not succeed within the timeout.</p> <p>Returns     None (data is stored in the CSV file).</p> Detail <p>The function uses Selenium and a headless Chrome browser to download  the CSV file. Ideally, it would use an oTree API endpoint instead.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def export_otree_data(\n        csv_file: str,\n        server_url: str | None = None, \n        admin_name: str | None = \"admin\", \n        admin_password: str | None = None,\n        time_out: int | None = 10\n    ) -&gt; None:\n    \"\"\"\n    Export wide data from an oTree server.\n\n    Args:\n        csv_file (str): Path to the CSV file where the data should be stored.\n        server_url (str, optional): URL of the oTree server. If None \n            (the default), the function will try to use the oTree server URL \n            from the environment variable OTREE_SERVER_URL.\n        admin_name (str, optional): Admin username. Defaults to \"admin\".\n        admin_password (str, optional): Admin password. If None (the default),\n            the function will try to use the oTree admin password from the \n            environment variable OTREE_ADMIN_PASSWORD.\n        time_out (int, optional): Timeout in seconds to wait for the download. \n            Defaults to 10.\n\n    Raises:\n        Exception: If the download does not succeed within the timeout.\n\n    Returns\n        None (data is stored in the CSV file).\n\n    Detail:\n        The function uses Selenium and a headless Chrome browser to download \n        the CSV file. Ideally, it would use an oTree API endpoint instead.\n    \"\"\"\n\n    chrome_options = Options()\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--disable-gpu\")\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    chrome_options.add_argument(\"--log-level=3\")\n    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        prefs = {\"download.default_directory\": tmp_dir}\n        chrome_options.add_experimental_option(\"prefs\", prefs)\n        driver = webdriver.Chrome(options=chrome_options)\n        driver.set_window_size(1920, 1400)\n        if server_url is None:\n            server_url = os.getenv(\"OTREE_SERVER_URL\")\n        if admin_password is None:\n            admin_password = os.getenv(\"OTREE_ADMIN_PASSWORD\")\n\n        export_url = f\"{server_url}/export\"\n        driver.get(export_url)\n        current_url = driver.current_url\n        if \"login\" in current_url:\n            driver.find_element(By.ID, \"id_username\").send_keys(admin_name)\n            driver.find_element(By.ID, \"id_password\").send_keys(admin_password)\n            submit_button = WebDriverWait(driver, 10).until(\n                EC.element_to_be_clickable((By.ID, \"btn-login\"))\n            )\n            submit_button.click()\n            WebDriverWait(driver, 10).until(EC.url_changes(current_url))\n            driver.get(export_url)\n\n        download_link = WebDriverWait(driver, 10).until(\n            EC.element_to_be_clickable((By.ID, \"wide-csv\"))\n        )\n        driver.execute_script(\"arguments[0].scrollIntoView(true)\", download_link)\n        download_link.click()\n\n        time_out = time.time() + time_out\n        while True:            \n            time.sleep(1)\n            csv_files = [f for f in os.listdir(tmp_dir) if f.endswith(\".csv\")]\n            if len(csv_files) == 1:\n                shutil.move(f\"{tmp_dir}/{csv_files[0]}\", csv_file)\n                logger.info(\"oTree CSV file downloaded.\")\n                break\n            else:\n                if time.time() &gt; time_out:\n                    logger.error(\"oTree CSV file download failed.\")\n                    break\n        driver.quit()\n</code></pre>"},{"location":"reference.html#normalize_otree_data","title":"<code>normalize_otree_data</code>","text":"<p>Normalize oTree data from wide to long format, then reshape it into a set  of list-of-dicts structures. Optionally save it to a set of CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>otree_csv_file</code> <code>str</code> <p>Path to a wide multi app oTree CSV file.</p> required <code>var_dict</code> <code>dict</code> <p>A dict to customize the exported data. See detail section.</p> <code>{'participant': {'code': 'participant_code', 'time_started_utc': 'time_started_utc', '_current_app_name': 'current_app', '_current_page_name': 'current_page'}, 'session': {'code': 'session_code'}}</code> <code>store_as_csv</code> <code>bool</code> <p>Whether to store the normalized data as CSV files. Defaults to False.</p> <code>False</code> <code>data_exp_path</code> <code>str</code> <p>Path to the folder where the normalized CSV files should be stored. Defaults to '.' (current folder).</p> <code>'.'</code> <code>exp_prefix</code> <code>str</code> <p>Prefix to be used for the CSV file names.  Defaults to '' (no prefix).</p> <code>''</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict whose keys are table names (e.g. 'session', 'participant',  'myapp_group', 'myapp_player', etc.) and whose values are lists of  dictionaries (i.e., row data).</p> Additional details <p>The var_dict parameter is a dictionary that allows to customize the exported data. The keys of the dictionary are the names of the oTree apps. The values are dictionaries that map the original column names to the desired column names. The keys of these inner dictionaries are the original column names and the values are the desired column names. All variables that are not included in the dict are omitted from the output. The 'participant' and 'session' keys are reserved for the participant  and session data, respectively.</p> Source code in <code>src/botex/otree.py</code> <pre><code>def normalize_otree_data(\n    otree_csv_file: str, \n    var_dict: dict | None = {\n        'participant': {\n            'code': 'participant_code', \n            'time_started_utc': 'time_started_utc',\n            '_current_app_name': 'current_app', \n            '_current_page_name': 'current_page',\n        },\n        'session': {\n            'code': 'session_code'\n        }\n    },\n    store_as_csv: bool = False,\n    data_exp_path: str | None = '.', \n    exp_prefix: str | None = '',\n) -&gt; dict:\n    \"\"\"\n    Normalize oTree data from wide to long format, then reshape it into a set \n    of list-of-dicts structures. Optionally save it to a set of CSV files.\n\n    Args:\n        otree_csv_file (str): Path to a wide multi app oTree CSV file.\n        var_dict (dict, optional): A dict to customize the exported data. See\n            detail section.\n        store_as_csv (bool, optional): Whether to store the normalized data as\n            CSV files. Defaults to False.\n        data_exp_path (str, optional): Path to the folder where the normalized\n            CSV files should be stored. Defaults to '.' (current folder).\n        exp_prefix (str, optional): Prefix to be used for the CSV file names. \n            Defaults to '' (no prefix).\n\n    Returns:\n        A dict whose keys are table names (e.g. 'session', 'participant', \n            'myapp_group', 'myapp_player', etc.) and whose values are lists of \n            dictionaries (i.e., row data).\n\n    ??? tip \"Additional details\"\n        The var_dict parameter is a dictionary that allows to customize the\n        exported data. The keys of the dictionary are the names of the oTree\n        apps. The values are dictionaries that map the original column names to\n        the desired column names. The keys of these inner dictionaries are the\n        original column names and the values are the desired column names. All\n        variables that are not included in the dict are omitted from the output.\n        The 'participant' and 'session' keys are reserved for the participant \n        and session data, respectively.\n    \"\"\"\n\n    # The function is based on the naming conventions of oTree CSV files.\n    # oTree uses multi-level headers in the CSV file, where each level is\n    # separated by a dot. \n\n    # The general flow of the function is as follows:\n    # 1) Read the CSV file and extract the multi-level headers.\n    # 2) Pivot the data to long format by flattening wide columns into rows of   \n    #     [observation, level_1..4, value].\n    # 3) Extract participant and session data.\n    # 4) Separate the remaining long data by app and sub-level, pivoting each\n    #    resulting data set back into wide format and add the appropriate\n    #    keys from participant and session data.\n    #    - subsession (should be empty since subsessions seem to be equal to \n    #      rounds, tbc)\n    #    - group (merge with session data on participant_code to create key, \n    #      only keep if it contains data)\n    #    - player (rename id_in_group to player_id)  \n    # 5) Optionally store the data as CSV files.\n\n    # --------------------------------------------------------------------------\n    # Helper functions\n    # --------------------------------------------------------------------------\n\n    def extract_data(var, stacked_data, var_dict):\n        relevant = [\n            d for d in stacked_data if d['level_1'] == var \n        ]\n        var_dict = var_dict.get(var)\n        if var in ['participant', 'session']:\n            # exclude not requested rows\n            relevant = [\n                d for d in relevant if d['level_2'] in var_dict.keys()\n            ]\n            # Remap level_2 column names\n            for item in relevant:\n                item['level_2'] = var_dict[item['level_2']]\n\n        # Build pivot result as dict {observation -&gt; one row dict}\n        pivoted = {}\n        for item in relevant:\n            obs = item['observation']\n            col = item['level_2']\n            val = item['value']\n            if obs not in pivoted:\n                pivoted[obs] = {'observation': obs}\n            # Only keep first occurrence if duplicates exist\n            if col not in pivoted[obs]:\n                pivoted[obs][col] = val\n\n        return list(pivoted.values())\n\n    def index_to_participant_code(data_list, obs_to_pcode):\n        out = []\n        for row in data_list:\n            obs = row['observation']\n            new_row = dict(row)\n            new_row['participant_code'] = obs_to_pcode.get(obs, None)\n            del new_row['observation']\n            out.append(new_row)\n        return out\n\n    def try_convert_number(x):\n        if x is None:\n            return None\n        # Already a number? (in case we call this multiple times)\n        if isinstance(x, (int, float)):\n            return x\n        # Attempt int -&gt; float -&gt; fallback str\n        try:\n            return int(x)\n        except (ValueError, TypeError):\n            pass\n        try:\n            return float(x)\n        except (ValueError, TypeError):\n            pass\n        return str(x)\n\n    def convert_columns(data_list, keys=None):\n        missing_keys = set()\n        if keys:\n            missing_keys = set(keys) - set(data_list[0].keys())\n        for row in data_list:\n            for k, v in row.items():\n                if k not in (\"observation\",):  # do not convert observation index\n                    row[k] = try_convert_number(v)\n            for k in missing_keys:\n                row[k] = '' \n        return data_list\n\n    def unify_dict_keys(rows):\n        seen_keys = []\n        for d in rows:\n            for k in d.keys():\n                if k not in seen_keys:\n                    seen_keys.append(k)\n        new_rows = []\n        for d in rows:\n            new_d = {}\n            for k in seen_keys:\n                new_d[k] = d.get(k, '') \n            new_rows.append(new_d)\n        return new_rows\n\n    def reorder_columns(data_list, first_cols):\n        out = []\n        for row in data_list:\n            new_row = {}\n            # keep track of which keys got placed\n            placed = set()\n            # place the \"first_cols\" in order\n            for c in first_cols:\n                if c in row:\n                    new_row[c] = row[c]\n                    placed.add(c)\n            # place remaining\n            for c in row:\n                if c not in placed:\n                    new_row[c] = row[c]\n            out.append(new_row)\n        return out\n\n\n    def write_dicts_to_csv(dict_rows, file_path):\n        if not dict_rows:\n            # If empty, write just an empty file or possibly only headers\n            with open(file_path, 'w', newline='', encoding='utf-8') as f:\n                pass\n            return\n        fieldnames = list(dict_rows[0].keys())\n        with open(file_path, 'w', newline='', encoding='utf-8') as f:\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n            writer.writeheader()\n            writer.writerows(dict_rows)\n\n\n    # --------------------------------------------------------------------------\n    # Main code\n    # --------------------------------------------------------------------------\n\n    # --- 1) Read the CSV file and extract the multi-level headers -------------\n\n    with open(otree_csv_file, 'r', encoding='utf-8-sig') as f:\n        reader = csv.reader(f)\n        all_rows = list(reader)\n    if not all_rows:\n        raise ValueError(f\"CSV file {otree_csv_file} is empty or invalid.\")\n\n    headers = all_rows[0]\n    data_rows = all_rows[1:]\n    multi_headers = [tuple(h.split('.')) for h in headers]\n\n\n    # --- 2) Pivot the data to long format -------------------------------------\n\n    processed_rows = []\n    for row_idx, row in enumerate(data_rows):\n        row_dict = {}\n        for col_idx, val in enumerate(row):\n            key_tuple = list(multi_headers[col_idx])\n            while len(key_tuple) &lt; 4:\n                key_tuple.append(None)\n            row_dict[tuple(key_tuple)] = val\n        processed_rows.append(row_dict)\n\n    stacked_data = []\n    for obs_idx, row_dict in enumerate(processed_rows):\n        for (lvl1, lvl2, lvl3, lvl4), val in row_dict.items():\n            stacked_data.append({\n                'observation': obs_idx,\n                'level_1': lvl1,\n                'level_2': lvl2,\n                'level_3': lvl3,\n                'level_4': lvl4,\n                'value': val\n            })\n\n    all_level1 = set(d['level_1'] for d in stacked_data if d['level_1'])\n    all_level1 = all_level1 - {'participant', 'session'}\n    apps = sorted(list(all_level1))\n\n    # Ensure var_dict has entries for each discovered app (even if empty).\n    for app in apps:\n        if app not in var_dict:\n            var_dict[app] = {}\n\n\n    # --- 3) Extract participant and session data ------------------------------\n\n    participant_data = extract_data('participant', stacked_data, var_dict)\n    participant_data = convert_columns(participant_data)\n\n    # Build a map: obs -&gt; participant_code\n    obs_to_participant_code = {}\n    for row in participant_data:\n        # We expect 'participant_code' in these rows\n        if 'participant_code' in row:\n            obs_to_participant_code[row['observation']] = row['participant_code']\n\n    session_data = extract_data('session', stacked_data, var_dict)\n    session_data = convert_columns(session_data)\n    session_data = index_to_participant_code(session_data, obs_to_participant_code)\n\n    for row in participant_data: row.pop('observation')\n    participant_data = reorder_columns(participant_data, ['participant_code'])\n\n    final_tables = {\n        'participant': participant_data,\n        'session': session_data,\n    }\n\n    # --- 4) Separate the remaining long data by app and sub-level -------------\n\n    # For convenience, gather all unique level_3 for each (level_1 == app).\n    # Typically level_3 is 'subsession', 'group', 'player', etc.\n    for app in apps:\n        logger.info(f\"Normalize data for oTree app: '{app}'\")\n        app_level_3 = sorted(set(\n            d['level_3'] for d in stacked_data if d['level_1'] == app and d['level_3']\n        ))\n\n        for group_name in app_level_3:\n            # Filter data for this app &amp; group\n            relevant = [\n                d for d in stacked_data\n                if d['level_1'] == app and d['level_3'] == group_name\n            ]\n            if not relevant:\n                continue\n\n            # Check whether the user provided a custom variable dictionary\n            # for this app and group \n\n            group_dict = var_dict[app].get(group_name, None)\n            if group_dict:\n                # exclude not requested rows\n                struct_vars = [\n                    'id_in_group', 'id_in_subsession', 'round_number'\n                ]\n                relevant = [\n                    d for d in relevant if (\n                        d['level_4'] in group_dict.keys() or\n                        d['level_4'] in struct_vars\n                    )\n                ]\n                # Remap level_4 column names\n                for item in relevant:\n                    if item['level_4'] not in struct_vars:\n                        item['level_4'] = group_dict[item['level_4']]\n\n            pivoted = {}\n            cols = set()\n            for item in relevant:\n                obs = item['observation']\n                try:\n                    rnd = int(item['level_2'])  # round number\n                except (ValueError, TypeError):\n                    rnd = None\n\n                cols.add(item['level_4'])\n                if item['value'] not in (None, ''):\n                    col = item['level_4']\n                    val = item['value']\n                    key = (obs, rnd)\n\n                    if key not in pivoted:\n                        pivoted[key] = {\n                            'observation': obs,\n                            'round': rnd\n                        }\n                    if col and col not in pivoted[key]:\n                        pivoted[key][col] = val\n\n            pivoted_list = unify_dict_keys(list(pivoted.values()))\n            out_df_rows = convert_columns(pivoted_list, cols)\n            out_df_rows = index_to_participant_code(\n                out_df_rows, obs_to_participant_code\n            )\n            table_name = f\"{app}_{group_name}\"  \n\n            if group_name == 'subsession':\n                # This code assumes that subsesions are equal to rounds.\n                # This implies that round_number' and 'round' columns should \n                # match and that the resulting data should have 3 columns.\n                col_names = out_df_rows[0].keys() if out_df_rows else []\n                if len(col_names) != 3:\n                    logger.error(\n                        f\"Error {app} data extraction: app seems to contain \"\n                        \"more subsessions than rounds or subsession level data.\"\n                    )\n                    raise ValueError(\n                        f\"Error in {app} data extraction: app seems to contain \"\n                        \"more subsessions than rounds.\"\n                    )\n                for row in out_df_rows:\n                    if 'round_number' in row and row['round_number'] != row['round']:\n                        logger.error(\n                            f\"Error {app} data extraction: subsession round_number \"\n                            \"does not match inferred round.\"\n                        )\n                        raise ValueError(\n                            f\"Error in {app} data extraction: subsession round_number \"\n                            \"does not match inferred round.\"\n                        )\n                # No data in subsession, so skip storing\n                continue\n\n            elif group_name == 'group':\n                # group data might or might not have data\n                # (only if there's more than one group or if there are \n                # group-level variables)\n\n                sess_index = {}\n                for srow in session_data:\n                    pcode = srow.get('participant_code', None)\n                    sess_index[pcode] = srow\n\n                merged_group = []\n                cols = set()\n                for row in out_df_rows:\n                    pcode = row.get('participant_code', None)\n                    newrow = dict(row)\n                    if pcode in sess_index:\n                        # merge session data into newrow\n                        for k, v in sess_index[pcode].items():\n                            # do not overwrite if we already have something\n                            if k not in ('participant_code', 'observation', 'round'):\n                                cols.add(k)\n                                newrow[k] = v\n                    merged_group.append(newrow)\n                out_df_rows = merged_group\n\n                # Check if 'id_in_subsession' is all 1 - meaning there is only \n                # one group per session\n                all_id1 = True\n                for row in out_df_rows:\n                    if row.get('id_in_subsession') != 1:\n                        all_id1 = False\n                        break\n\n                if all_id1:\n                    # drop ['id_in_subsession', 'participant_code']\n                    cleaned = []\n                    for row in out_df_rows:\n                        newrow = dict(row)\n                        newrow.pop('id_in_subsession', None)\n                        newrow.pop('participant_code', None)\n                        cleaned.append(newrow)\n                    out_df_rows = cleaned\n\n                    # If after dropping columns we are left with only 2 columns \n                    # (session_code, round), there is no group level data\n                    if out_df_rows:\n                        col_count = len(out_df_rows[0])\n                        if col_count == 2:\n                            # skip storing\n                            continue\n                else:\n                    # rename id_in_subsession -&gt; group_id\n                    # build a map of group_id to participant_code\n                    cleaned = []\n                    group_participant_map = []\n                    for row in out_df_rows:\n                        newrow = dict(row)\n                        newmap = dict()\n                        newrow['group_id'] = newrow.pop('id_in_subsession')\n                        newmap['participant_code'] = newrow.pop('participant_code')\n                        newmap['round'] = newrow['round']\n                        newmap['group_id'] = newrow['group_id']\n                        cleaned.append(newrow)\n                        group_participant_map.append(newmap)\n                    out_df_rows = cleaned\n\n                # reorder columns\n                col_order = ['session_code', 'round']\n                if not all_id1: col_order.append('group_id')\n                out_df_rows = convert_columns(\n                    reorder_columns(out_df_rows, col_order),\n                    cols\n                )\n\n                # Deduplicate\n                unique_rows = []\n                seen = set()\n                for row in out_df_rows:\n                    # convert to a tuple of items\n                    row_tup = tuple(sorted(row.items()))\n                    if row_tup not in seen:\n                        seen.add(row_tup)\n                        unique_rows.append(row)\n                out_df_rows = unique_rows\n\n                final_tables[table_name] = out_df_rows\n\n            elif group_name == 'player':\n                # rename id_in_group -&gt; player_id\n                # and merge with group_participant_map if it exists\n                cleaned = []\n                for row in out_df_rows:\n                    newrow = dict(row)\n                    newrow['player_id'] = newrow.pop('id_in_group')\n                    if not all_id1:\n                        for m in group_participant_map:\n                            if  m['round'] == newrow['round'] and \\\n                                m['participant_code'] == newrow['participant_code']:\n                                newrow['group_id'] = m['group_id']\n                                break\n                    cleaned.append(newrow)\n                out_df_rows = cleaned\n                # reorder\n                if not all_id1:\n                    col_order = ['participant_code', 'round', 'group_id', 'player_id']\n                else:\n                    col_order = ['participant_code', 'round', 'player_id']\n                out_df_rows = reorder_columns(\n                    out_df_rows, col_order\n                )\n                final_tables[table_name] = out_df_rows\n\n            else:\n                # This should not happen - but if it does, store the data as is\n                logger.warning(\n                    f\"Unrecognized level name '{group_name}' in app '{app}'.\"\n                )\n                final_tables[table_name] = out_df_rows\n\n    # delete app names from table names if there is only one app\n    if len(apps) == 1:\n        for table_name in final_tables.keys():\n                if table_name.startswith(apps[0] + '_'):\n                    final_tables[table_name.split('_')[-1]] = \\\n                        final_tables.pop(table_name)\n\n\n    # --- 5) Optionally store as CSV -------------------------------------------\n\n    if store_as_csv:\n        if exp_prefix: exp_prefix += '_'\n        for table_name, rows in final_tables.items():\n            out_csv = os.path.join(\n                data_exp_path, f\"{exp_prefix}{table_name}.csv\"\n            )\n            write_dicts_to_csv(rows, out_csv)\n\n    logger.info(f\"Normalizing data completed\")\n    return final_tables\n</code></pre>"},{"location":"tutorials/exp_3llms.html","title":"Using botex to Benchmark LLM Performance in oTree Experiments","text":"<p>This tutorial walks you through a toy study that lets three different LLMs compete against each other in oTree's example Two Thirds game. It features the use of LiteLLM as well as llama.cpp for communicating with these models. It also uses the <code>run_single_bot()</code>function of the botex API and shows how you can evaluate the oTree data from the resulting experiment.</p>"},{"location":"tutorials/exp_3llms.html#running-the-experiment","title":"Running the Experiment","text":"<p>Like the Run an oTree experiment with botex tutorial, the code below starts with starting an oTree server. In addition, it now also starts a llama.cpp server, meaning that you need to install llama.cpp and download the required LLM model file if you want to run the code 1:1 to reproduce the analysis presented below.</p> <p>We compare the results of the following LLMs:</p> <ul> <li>OpenAI's GPT-4p</li> <li>Google's Gemini-1.5 Flash</li> <li>LLama 3.1-8B Instruct</li> </ul> <p>Different from the first tutorial, we will now use environment variables to configure the setup.</p> <p>In your project folder, create a file called <code>botex.env</code> and add the following content:</p> <pre><code># Path to your botex SQLite database file\nBOTEX_DB=\"botex.sqlite3\"\n\n# Your otree project path\nOTREE_PROJECT_PATH=\"otree\" \n\n# Path to your llama.cpp server executable\nLLAMACPP_SERVER_PATH=\"llama.cpp/llama-server\"\n\n# Path to your llama.cpp model GGUF file\nLLAMACPP_LOCAL_LLM_PATH=\"models/meta-llama-3.1-8B-instruct-Q8_0.gguf\"\n\n# Configure the use of GPU for llama.cpp\nLLAMACPP_NUMBER_OF_LAYERS_TO_OFFLOAD_TO_GPU=32\n\n# Your Google AI Studio API key\nGEMINI_API_KEY=\"YOUR_API_KEY\"\n\n# Your OpenAI API key\nOPENAI_API_KEY=\"YOUR_API_KEY\"\n</code></pre> <p>Then, create a Python script called <code>run_exp_3llms.py</code> in the same folder and add the following code:</p> <pre><code>import os\nfrom datetime import date\nimport botex\n\nimport logging\nlogging.basicConfig(level=logging.WARNING)\n\nimport botex \n\nNPART = 3\nNSESSIONS = 1\n\n# Read the botex.env file to configure the botex setup\nbotex.load_botex_env()\n\n# Start the oTree server\notree_server = botex.start_otree_server()\notree_server_url = os.environ['OTREE_SERVER_URL']\n\nllamacpp = botex.start_llamacpp_server()\n\n# Starts a single bot in a separate thread and returns the thread\ndef start_bot_thread(session, part_pos, model):\n    thread = botex.run_single_bot(\n        session_name = \"guess_two_thirds\",\n        session_id = session[\"session_id\"],\n        participant_id = session['participant_code'][part_pos],\n        url = session['bot_urls'][part_pos],\n        model=model,\n        throttle=True,\n        wait = False\n    )\n    thread.start()\n    return thread\n\nfor r in range(NSESSIONS):\n    print(f\"Running Session #{r+1}\")\n    session = botex.init_otree_session(\"guess_two_thirds\", NPART)\n    print(f\"Session Monitor URL: {otree_server_url}/SessionMonitor/{session['session_id']}\")\n    threads = []\n    threads.append(start_bot_thread(session, 0, \"gemini/gemini-1.5-flash\"))\n    threads.append(start_bot_thread(session, 1, \"gpt-4o-2024-08-06\"))\n    threads.append(start_bot_thread(session, 2, \"llamacpp\"))\n    for t in threads:\n        t.join()\n    print(f\"Session #{r + 1} complete.\")\n\n\nbotex.stop_llamacpp_server(llamacpp)\nbotex.export_otree_data(\"exp_3llms_otree_wide.csv\")\nbotex.stop_otree_server(otree_server)\n</code></pre> <p>The code above starts an oTree server, a llama.cpp server, and runs three bots in parallel. By default, <code>run_single_bot()</code> will wait for its bot's thread to finish before returning. But if you set <code>wait=False</code> it will instead return the thread after starting it. The subsequent joining of the threads ensures that the code will continue after all three bot threads have completed.</p> <p>After <code>NSESSIONS</code> have been completed, the code exports the oTree data to a CSV file and stops both the oTree and the llama.cpp servers. </p> <p>If you want to reproduce the analysis presented below, you would need increase <code>NSESSIONS</code> so that multiple sessions are beeing played. The analysis below is based on 50 sessions and running them has incurred OpenAI API costs of around US-$ 3.</p>"},{"location":"tutorials/exp_3llms.html#analyze-the-data","title":"Analyze the Data","text":"<p>The code below reads the CSV file created by the script above, normalizes the data and plots the results. If you want to reproduce the analysis, you need to install the required Python packages first. You can do this by running the following command:</p> <pre><code>pip install pandas seaborn matplotlib statannotations statsmodels\n</code></pre> <p>Then, create a Python script called <code>analyze_exp_3llms.py</code> in the same folder and add the following code:</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom statannotations.Annotator import Annotator  \nimport pandas as pd\nimport botex\n\ndef convert_normalized_otree_data_to_pandas(normalized_data):\n    \"\"\"\n    Convert the dict of list-of-dicts (returned by botex.normalize_otree_data())\n    into a dict of pandas DataFrames.\n\n    Args:\n        normalized_data (dict): \n            List of dicts, each representing a table of data from an oTree experiment.\n            As returned by `botex.normalize_otree_data()`.\n            Keys are table names (e.g. 'participant', 'session', 'myapp_group', etc.)\n            Values are lists of dicts, each dict representing a row of data.\n\n    Returns:\n        A dict with the same keys, where each value is a pandas DataFrame.\n    \"\"\"\n    dfs = {}\n    for table_name, lod in normalized_data.items():\n        df = pd.DataFrame(lod)\n        dfs[table_name] = df\n\n    return dfs\n\ndta = botex.normalize_otree_data(\"exp_3llms_otree_wide.csv\")\ndfs = convert_normalized_otree_data_to_pandas(dta)\n\nparticipants = [\n    'gemini-1.5-flash', 'gpt-4o-2024-08-06', 'llama-3.1-8B-instruct'\n]\n\ndf = dfs['guess_two_thirds_player'].merge(\n    dfs['session'], on = 'participant_code', how = 'left'\n).merge(\n    dfs['guess_two_thirds_group'], \n    on = ['session_code', 'round'], \n    how = 'left'\n)\n\ndf['round'] = df['round'].astype('category')\ndf['player_id'] = df['player_id'].astype('category')\ndf['player_id'] = df['player_id'].cat.rename_categories({\n    1: participants[0],\n    2: participants[1],\n    3: participants[2]\n})\n\nplt.figure(figsize=(8, 5))\n\nsns.lineplot(\n    data=df, x='round', y='guess', hue='player_id',\n    marker='o', errorbar='ci', n_boot=1000, err_style='bars',  \n    legend='full'\n)\n\ncorrect_avg_summary = df.groupby(\n    'round', as_index=False\n)['two_thirds_avg'].mean()\n\nsns.lineplot(\n    data=correct_avg_summary,\n    x='round', y='two_thirds_avg', color='black', \n    marker='s', label='Correct 2/3 Average'\n)\n\nplt.title(\n    'Average Guesses by Round and Player\\n(with 95% CI) vs. Correct 2/3 Average'\n)\nplt.xlabel('Round')\nplt.ylabel('Guess')\nplt.xticks([1, 2, 3])\nplt.legend()\nplt.tight_layout()\nplt.show()\n\ndf_sums = (\n    df.groupby(['session_code', 'player_id'], as_index=False)\n      .agg({'payoff': 'sum'})\n)\n\nplt.figure(figsize=(8, 8))\n\nax = sns.barplot(data=df_sums, x='player_id', y='payoff', hue='player_id')\npairs = [\n    (participants[0], participants[1]),\n    (participants[0], participants[2]),\n    (participants[1], participants[2])\n]\nannotator = Annotator(\n    ax, pairs, data=df_sums, x=\"player_id\", y=\"payoff\"\n)\nannotator.configure(\n    test=\"Mann-Whitney\", text_format=\"star\",\n    comparisons_correction=\"bonferroni\"\n)\nannotator.apply_and_annotate()\n\nplt.title(\n    'Mean Summed Payoff by Player\\n'\n    '(with 95% CI and Bonferroni-corrected Mann-Whitney tests)'\n)\nplt.xlabel(\"\") \nplt.ylabel('Summed Payoff')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>The first plot shows the average guesses by round and player, with 95% confidence intervals, and the correct 2/3 average.</p> <p></p> <p>As you can see, OpenAI's 4o model seems to make the best guesses in the first round, indicating that it might have a little bit of an edge in higher order beliefs. However, Google Gemini quickly picks up in later rounds and tend to be closer towards the correct 2/3 average towards the end. In comparison, Llama 3.1-8B Instruct guesses too high throughout.</p> <p></p> <p>The second plot shows the mean summed payoff by player, with 95% confidence intervals and Bonferroni-corrected Mann-Whitney tests for cross-participant differences. It seems that OpenAI's 4o model and Google Gemini are roughly at par (the difference between the two is no significant at conventional levels) while both clearly dominate Llama 3.1-8B Instruct.</p>"},{"location":"tutorials/running_an_experiment.html","title":"Run an oTree Experiment with botex","text":""},{"location":"tutorials/running_an_experiment.html#starting-otree-via-python","title":"Starting oTree via Python","text":"<p>When you want to run an oTree experiment with botex bots, you need to have access to a running oTree instance that is able to host the experiment that you want to run.</p> <p>in the getting started section, we have seen how to start an oTree server via the command line. Now, we will use botex to start an oTree server instance via Python. This is useful if you want to automate the process of running an experiment and collecting its data.</p> <p>The following code snippet shows how to start an oTree server via Python:</p> <pre><code>import botex\n\n# botex is silent by default. If you want to understand\n# what it is doing, it is useful to enable logging \nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Start the oTree server\nbotex.start_otree_server(project_path=\"otree\")\n</code></pre> <p>Adjust the <code>project_path</code> parameter to where you installed your otree project. The above starts a development oTree server without authentication. If you want to require authentication (recommended for publicly accessible servers), you can set the <code>auth_level</code> parameter of <code>start_otree_server</code> to <code>'STUDY'</code>. In this case, you should also set the parameters <code>rest_key</code> and <code>admin_password</code>.</p> <p>Running the snippet from above, you should see the following output:</p> <pre><code>INFO:botex:oTree server started successfully with endpoint 'http://localhost:8000'\n</code></pre> <p>Please note that the oTree sever instance will terminate when your script terminates. </p>"},{"location":"tutorials/running_an_experiment.html#retrieve-the-available-session-configurations","title":"Retrieve the Available Session Configurations","text":"<p>Before you can run an experiment, you might want to know which session configurations are available. The extension of the code shows how to retrieve the available session configurations from a running oTree server:</p> <pre><code>import botex\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Start the oTree server\notree_process = botex.start_otree_server(project_path=\"otree\")\n\n# Retrieve the session configurations\nsession_configs = botex.get_session_configs(\n    otree_server_url=\"http://localhost:8000\"\n)\nprint(session_configs)\n\n# Stop the oTree server\nbotex.stop_otree_server(otree_process)\n</code></pre> <p>If you are using a locally running oTree server, you can omit the <code>otree_server_url</code> parameter. When you are accessing a remotely running oTree server, you need to provide its URL (including the correct port if required). If you try to access an oTree server with authentication, you also need to provide the <code>rest_key</code> parameter. Besides querying the session config from the server, we also added a call to <code>stop_otree_server</code> to gracefully terminate the oTree server instance before ending the script.</p> <p>The output of the above script should look like this:</p> <pre><code>INFO:botex:oTree server started successfully with endpoint 'http://localhost:8000'\n[{'real_world_currency_per_point': 1.0, 'participation_fee': 0.0, 'doc': '', \n'name': 'guess_two_thirds', 'display_name': 'Guess 2/3 of the Average', \n'app_sequence': ['guess_two_thirds', 'payment_info'], \n'num_demo_participants': 3}, \n{'real_world_currency_per_point': 1.0, 'participation_fee': 0.0, 'doc': '', \n'name': 'survey', 'app_sequence': ['survey', 'payment_info'], \n'num_demo_participants': 1, 'display_name': 'survey'}]\nINFO:botex:oTree server stopped.\n</code></pre>"},{"location":"tutorials/running_an_experiment.html#initialize-a-session","title":"Initialize a Session","text":"<p>Once you know which session configurations are available, you can initialize a session. The extension of the code shows how to do this:</p> <pre><code>import botex\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Start the oTree server\notree_process = botex.start_otree_server(project_path=\"otree\")\n\n# Retrieve the session configurations\nsession_configs = botex.get_session_configs(\n    otree_server_url=\"http://localhost:8000\"\n)\n\n# Initialize a session\nsession = botex.init_otree_session(\n    config_name=session_configs[0]['name'], # \"guess_two_thirds\"\n    npart = 3,\n    otree_server_url=\"http://localhost:8000\",\n    botex_db = 'botex.sqlite3'\n)\n\nprint(session)\n\n# Stop the oTree server\nbotex.stop_otree_server(otree_process)\n</code></pre> <p>Again, adjust the <code>otree_server_url</code> parameter and add <code>rest_key</code> as required. <code>init_otree_session()</code> requires the session config name and the number of participants to include. When changing this parameter, make sure that the experiment supports it. E.g., the Guess two Thirds game provided as an example with the oTree installation requires participants to be multiples of three. In addition, we need to provide a file for the SQLite database where botex stores its data. If the file does not exist, it will be created. If it exists, the data will be appended.</p> <p><code>init_otree_session()</code> returns a dict with session data, including the session ID and the participant URLs. The output of the above script should look like this:</p> <pre><code>INFO:botex:oTree server started successfully with endpoint 'http://localhost:8000'\n{'session_id': 'lry96cc8', \n'participant_code': ['dlg5vdbq', 'j8y24ubc', 'pbewmoh2'], \n'is_human': [False, False, False], \n'bot_urls': ['http://localhost:8000/InitializeParticipant/dlg5vdbq', \n'http://localhost:8000/InitializeParticipant/j8y24ubc', \n'http://localhost:8000/InitializeParticipant/pbewmoh2'], \n'human_urls': []}\nINFO:botex:oTree server stopped.\n</code></pre> <p>You see that we initialized a session with three participants, all of which are bots. The bot URLs are provided in the <code>bot_urls</code> list.</p>"},{"location":"tutorials/running_an_experiment.html#running-botex-bots-on-a-session","title":"Running botex Bots on a Session","text":"<p>Once you have initialized a session, you can run the botex bots on it. Let's extend our code:</p> <pre><code>import botex\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Start the oTree server\notree_process = botex.start_otree_server(project_path=\"otree\")\n\n# Retrieve the session configurations\nsession_configs = botex.get_session_configs(\n    otree_server_url=\"http://localhost:8000\"\n)\n\n# Initialize a session\nsession = botex.init_otree_session(\n    config_name=session_configs[0]['name'], # \"guess_two_thirds\"\n    npart = 3,\n    otree_server_url=\"http://localhost:8000\",\n    botex_db = 'botex.sqlite3'\n)\n\n# Run the bots on the session\nbotex.run_bots_on_session(\n    session_id=session['session_id'],\n    otree_server_url=\"http://localhost:8000\",\n    botex_db = 'botex.sqlite3',\n    model=\"gemini/gemini-1.5-flash\",\n    api_key=\"***\"\n)\n\n# Stop the oTree server\nbotex.stop_otree_server(otree_process)\n</code></pre> <p>If it works, you will be greeted with a very long log ouput, detailing the botex bots' interactions with the oTree server. If you want to see less of this, you can adjust the logging level to <code>logging.WARNING</code> or disable logging. However, we suggest that you take a good look at the log output to familiarize yourself with the workflow of the botex bots.</p> <p>The output should start and end like this:</p> <pre><code>INFO:botex:oTree server started successfully with endpoint 'http://localhost:8000'\nINFO:botex:Running bots on session q4ntmcdt. You can monitor the session at http://localhost:8000/SessionMonitor/q4ntmcdt\nINFO:botex:Running bot with parameters: {\"botex_db\": \"botex.sqlite\", \"session_id\": \"q4ntmcdt\", \"full_conv_history\": false, \"model\": \"gemini/gemini-1.5-flash\", \"api_key\": \"******\", \"api_base\": null, \"user_prompts\": null, \"throttle\": false, \"otree_server_url\": \"http://localhost:8000\", \"url\": \"http://localhost:8000/InitializeParticipant/372wu8py\"}\nINFO:botex:Running bot with parameters: {\"botex_db\": \"botex.sqlite\", \"session_id\": \"q4ntmcdt\", \"full_conv_history\": false, \"model\": \"gemini/gemini-1.5-flash\", \"api_key\": \"******\", \"api_base\": null, \"user_prompts\": null, \"throttle\": false, \"otree_server_url\": \"http://localhost:8000\", \"url\": \"http://localhost:8000/InitializeParticipant/ay4nos1w\"}\nINFO:botex:Running bot with parameters: {\"botex_db\": \"botex.sqlite\", \"session_id\": \"q4ntmcdt\", \"full_conv_history\": false, \"model\": \"gemini/gemini-1.5-flash\", \"api_key\": \"******\", \"api_base\": null, \"user_prompts\": null, \"throttle\": false, \"otree_server_url\": \"http://localhost:8000\", \"url\": \"http://localhost:8000/InitializeParticipant/nr4th0it\"}\nINFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=****** \"HTTP/1.1 200 OK\"\nINFO:botex:Bot's response to start message:\n{\n    \"task\": \"I am participating in an online survey or experiment.  My task is to provide JSON formatted responses based on prompts that will include a summary of the survey/experiment so far and any new information to be added to the summary. This summary will function as my memory throughout the experiment.  Each prompt also includes scraped text from a webpage related to the survey/experiment.  I will analyze the text, answer any questions or complete tasks within it, and incorporate any relevant information into the updated summary.  If compensation is mentioned for participants, I will consider that compensation to apply to me as well. I will provide responses only in JSON format, adhering to the specified schema.\",\n    \"understood\": true\n}\n\n[... maaaaany more lines ...]\n\nINFO:botex:Bot's final remarks about experiment:\n{\n    \"confused\": false,\n    \"remarks\": \"The experiment was well-structured and engaging. My strategy of iteratively reducing my guess in round 3 was based on observing that lower numbers were generally more successful in the previous rounds. However, it didn't prove to be as effective as hoped. The instructions were clear and the payoff system was straightforward. The information about the experiment administrator's instructions and the oTree implementation details were interesting to learn, although as a participant, they were not directly relevant to my participation in the game. Overall, this was a successful experiment design and the implementation in this conversation worked effectively.  The JSON format for responses worked well and was easy to use.\"\n}\nINFO:botex:Bot finished.\nINFO:botex:Data stored in botex database.\nINFO:botex:oTree server stopped.\n</code></pre> <p>When inspecting the log output, you will likely notice a warning like this:</p> <pre><code>INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=****** \"HTTP/1.1 429 Too Many Requests\"\nWARNING:botex:Litellm completion failed, error: 'litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n    \"status\": \"RESOURCE_EXHAUSTED\"\n  }\n}\n'\nINFO:botex:Retrying with throttling.\n</code></pre> <p>This is because the free tier of the Google Gemini model has per minute rate limits for requests. On encountering this error, botex automatically apply an exponential backoff strategy, meaning that it keeps retrying with increasing delays until the request is successful. If you want to avoid the warning, you can set the <code>throttle</code> parameter of <code>run_bots_on_session</code> to <code>True</code>. This will cause botex to throttle the requests to the model by default.</p>"},{"location":"tutorials/running_an_experiment.html#exporting-the-data","title":"Exporting the Data","text":"<p>After running the experiment and before shutting down the oTree server, you might want to export the data, both from oTree and from botex. So here is our final extension of the code:</p> <pre><code>import botex\n\n# botex is relatively silent by default. If you want to understand\n# what it is doing, it is useful to enable logging. \n# We set it to WARNING here so that we will be only informed if \n# something goes wrong.\nimport logging\nlogging.basicConfig(level=logging.WARNING)\n\n# Will be created in the current directory if it does not exist\nBOTEX_DB = \"botex.sqlite3\"\n\n# Path to your oTree project folder if you want the code to start the server\nOTREE_PROJECT_PATH = \"otree\"\n\n# Change the oTree URL if you are using a remote server\nOTREE_URL = \"http://localhost:8000\"\n\n# If you use a higher oTree authentication level, \n# you need to set the following\nOTREE_REST_KEY = None\nOTREE_ADMIN_NAME = None\nOTREE_ADMIN_PASSWORD = None\n\n# LLM model vars\nLLM_MODEL = \"gemini/gemini-1.5-flash\"\nLLM_API_KEY = \"******\"\n\n# Start the oTree server - if not using an already running server\notree_process = botex.start_otree_server(project_path=OTREE_PROJECT_PATH)\n\n# Get the available session configurations from the oTree server\nsession_configs = botex.get_session_configs(otree_server_url=OTREE_URL)\n\n# Initialize a session\nsession = botex.init_otree_session(\n    config_name=session_configs[0]['name'], # \"guess_two_thirds\"\n    npart = 3,\n    otree_server_url=OTREE_URL,\n    otree_rest_key=OTREE_REST_KEY,\n    botex_db = BOTEX_DB\n)\n\n# Run the bots on the session\nprint(\n    f\"Starting bots. You can monitor their progress at \"\n    f\"http://localhost:8000/SessionMonitor/{session['session_id']}\"\n)\nbotex.run_bots_on_session(\n    session_id=session['session_id'],\n    otree_server_url=OTREE_URL,\n    botex_db=BOTEX_DB,\n    model=LLM_MODEL,\n    api_key=LLM_API_KEY,\n    throttle=True\n)\n\n# Export oTree data - you only need to set admin name and password if you\n# have set a higher authentication level ('DEMO' or 'STUDY') in oTree\nbotex.export_otree_data(\n    \"two_thirds_otree_wide.csv\",\n    admin_name = OTREE_ADMIN_NAME,\n    admin_password = OTREE_ADMIN_PASSWORD\n)\nbotex.normalize_otree_data(\n    \"two_thirds_otree_wide.csv\", \n    store_as_csv=True,\n    exp_prefix=\"two_thirds_otree\"\n)\n\n# Export botex data\nbotex.export_participant_data(\n    \"two_thirds_botex_participants.csv\",\n    botex_db=BOTEX_DB\n)\nbotex.export_response_data(\n    \"two_thirds_botex_responses.csv\",\n    botex_db='botex.sqlite',\n    session_id=session['session_id']\n)\n\n# Stop the oTree server\nbotex.stop_otree_server(otree_process)\n</code></pre> <p>To silence botex, we now set the logging level to <code>WARNING</code>. Also, we set <code>throttle</code> to <code>True</code> to avoid that botex nags us about the rate limit rejects. We also refactor the code to use variables for the paths and URLs. This makes it easier to adjust the code to your setup.</p> <p>The code now starts an oTree server, initializes a session, runs the bots on the session, and exports the data. The data is exported in CSV format. The botex data is exported in two files: one containing the participant data and one containing the responses. The oTree data is exported in wide format and then normalized. The normalized data is stored in a set of CSV files with the prefix <code>two_thirds_otree</code>. You should see all files in your project directory after the code has been run.</p> <p>This concludes this tutorial. If you want to learn how to run single botex bots using different LLM models to benchmark their performance with oTree experiments and how to evaluate the results, please refer to the next tutorial.</p>"}]}