# Botex can be configured by 
#
#  - using the dialog of the command line interface `botex`,
#  - passing the required function arguments, and/or
#  - setting common required arguments as environment variables.
#  
# You can use the file below to set the environment variables.
# For this, copy it to `botex.env` and edit the values.

# ___ DO NOT COMMIT 'botex.env' ___
# See https://pypi.org/project/python-dotenv/
# for more information on the file format.

# When can botex reach your oTree instance? The below is the default if
# you start oTree locally.
OTREE_SERVER_URL="http://localhost:8000"

# You can set the next ones to any value you like.
# They are used to authenticate botex with the oTree server in case 
# it is run in production mode.
OTREE_ADMIN_PASSWORD=****
OTREE_REST_KEY=****

# For commercial LLM apis like OpenAI or Google Gemini, you need API keys.
# Botex uses the environment variable system of LiteLLM to 
# send the API keys to the LLM servers.
# See https://docs.litellm.ai/docs/set_keys for more detail.
OPENAI_API_KEY=******
GEMINI_API_KEY=******
# Add and edit API keys as needed.

# This sets the location for the data output of botex. The respective
# file will be created if it does not exist. 
# The default is used when testing the package.
BOTEX_DB="tests/botex.db"

# ------------------------------------------------------------------------------
# The following are additional arguments used by botex's command line interface

# The session config that you want to use when starting an oTree session.
# OTREE_SESSION_CONFIG="botex_test"

# How many participants do you want your oTree session to have?
# OTREE_NPARTICIPANTS=3

# How many of these participants should be humans?
# OTREE_NHUMANS=0

# The LiteLLM model name that want to use for botex bots.
# See https://docs.litellm.ai/docs/providers for more detail.
# if you use llama.cpp instead of LiteLLM, set it to "llamacpp".
# LLM_MODEL="gpt-4o-2024-08-06"

# The required API key of the respective model set above
# API_KEY="sk-proj-******"

# If your model's endpoint is not the standard one, you can set it here.
# This is also required when accessing a running llama.cpp server.
# API_BASE="http://localhost:8080"


# ------------------------------------------------------------------------------
# Everything below is only needed if you want to use llama.cpp for local LLMs 

# If you are accessing a running llama.cpp sever, you only need to set its URL 
# The below is the default if you start llama.cpp locally.
LLAMA_SERVER_URL="http://localhost:8080"

# Instead, You can call botex.start_llamacpp_server() to start a llama.cpp 
# instance locally. For this you need to set at least the two following 
# arguments
LLAMACPP_SERVER_PATH="llama.cpp/llama-server"
LLAMACPP_LOCAL_LLM_PATH="models/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf"

# In this case, you should also configure the following if you have a GPU
# that you want llama.cpp to use.
LLAMACPP_NUMBER_OF_LAYERS_TO_OFFLOAD_TO_GPU=0

# You can also increase the number of slots for more parallelism if 
# you have plenty of GPU memory.
LLAMACPP_NUM_SLOTS=1
